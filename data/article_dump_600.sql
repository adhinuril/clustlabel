-- MySQL dump 10.13  Distrib 5.7.17, for Win64 (x86_64)
--
-- Host: localhost    Database: article600
-- ------------------------------------------------------
-- Server version	5.7.19-log

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `cluster_label`
--

DROP TABLE IF EXISTS `cluster_label`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `cluster_label` (
  `article_id` int(11) NOT NULL,
  `cluster_id` int(11) DEFAULT NULL,
  `silh_score` float DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `cluster_label`
--

LOCK TABLES `cluster_label` WRITE;
/*!40000 ALTER TABLE `cluster_label` DISABLE KEYS */;
INSERT INTO `cluster_label` VALUES (19452,4,0.4687),(19458,4,0.4153),(27210,4,0.5639),(32199,4,0.481),(40420,4,0.543),(70146,4,0.5039),(109792,4,0.4726),(172445,4,0.3825),(208831,4,0.5279),(220176,4,0.5279),(244141,4,0.5182),(247950,4,0.5451),(248343,4,0.507),(257317,4,0.5177),(269426,4,0.5297),(274462,4,0.5534),(290247,4,0.562),(308701,4,0.5167),(322098,4,0.4406),(330191,4,0.5752),(336337,4,0.5343),(427553,4,0.5781),(438973,4,0.5634),(438999,4,0.432),(442169,4,0.4723),(442515,4,0.4475),(553248,4,0.5093),(583010,4,0.4428),(620106,4,0.4835),(620124,4,0.5179),(620275,4,0.4973),(620330,4,0.4122),(620466,4,0.4648),(621624,4,0.5056),(621690,4,0.4313),(621909,4,0.5044),(631432,4,0.3905),(665183,4,0.5116),(732499,4,0.4761),(852646,4,0.4146),(1011007,4,0.4204),(1011655,4,0.4587),(1011674,4,0.4597),(1011700,4,0.4206),(1011787,4,0.5249),(1011864,4,0.4891),(1013148,4,0.5653),(1148507,4,0.5079),(1308367,4,0.5017),(1512888,4,0.4381),(2840,5,0.3462),(3566,5,0.3963),(3885,5,0.4078),(12270,5,0.4309),(12638,5,0.3496),(23615,5,0.3854),(23649,5,0.3249),(27218,5,0.4097),(35909,5,0.4051),(54214,5,0.4644),(60045,5,0.4162),(60096,5,0.3759),(90241,5,0.2998),(102176,5,0.4404),(112226,5,0.3083),(115508,5,0.4553),(116876,5,0.4573),(124491,5,0.4526),(124522,5,0.4017),(132251,5,0.334),(144424,5,0.4068),(145492,5,0.374),(183373,5,0.3507),(218982,5,0.3419),(248344,5,0.451),(274469,5,0.4378),(288986,5,0.3059),(289005,5,0.441),(290178,5,0.4159),(320221,5,0.3818),(442014,5,0.4252),(442024,5,0.3351),(442119,5,0.4202),(442391,5,0.4114),(442511,5,0.3571),(593763,5,0.4191),(604730,5,0.4235),(653812,5,0.4821),(656784,5,0.4307),(672335,5,0.3969),(766645,5,0.3828),(791376,5,0.332),(1010515,5,0.4192),(1010903,5,0.3577),(1012707,5,0.318),(1013233,5,0.4185),(1080852,5,0.4225),(1373608,5,0.3581),(1373609,5,0.353),(1845754,5,0.3401),(8194,11,0.3419),(54093,11,0.3513),(87099,11,0.3937),(91230,11,0.3535),(285988,11,0.3168),(319121,11,0.3664),(319987,11,0.3772),(353073,11,0.3667),(369504,11,0.3454),(370028,11,0.3746),(379868,11,0.3755),(383409,11,0.4181),(394043,11,0.4588),(395143,11,0.4338),(405130,11,0.4215),(440012,11,0.4409),(442031,11,0.3575),(578163,11,0.4703),(597698,11,0.396),(598026,11,0.4202),(599224,11,0.4106),(599986,11,0.3726),(600466,11,0.3947),(602117,11,0.4271),(602634,11,0.3958),(603740,11,0.4591),(603748,11,0.4367),(605118,11,0.3978),(647869,11,0.4215),(664670,11,0.3451),(672745,11,0.3986),(674545,11,0.3541),(680789,11,0.3202),(719116,11,0.4058),(719433,11,0.4074),(801319,11,0.3822),(937513,11,0.4309),(1012212,11,0.4569),(1012330,11,0.3174),(1013158,11,0.3563),(1133613,11,0.4319),(1147064,11,0.3368),(1147219,11,0.3894),(1148814,11,0.5014),(1149013,11,0.4966),(1167962,11,0.4314),(1274106,11,0.3322),(1274299,11,0.366),(1307017,11,0.4178),(1308231,11,0.4536),(171446,2,0.4901),(196090,2,0.4788),(217951,2,0.4949),(227884,2,0.5225),(248893,2,0.4791),(249958,2,0.5449),(303693,2,0.5163),(303696,2,0.5086),(309440,2,0.5597),(331644,2,0.4074),(334436,2,0.5372),(339219,2,0.4836),(340165,2,0.4879),(342338,2,0.4723),(345540,2,0.5027),(433317,2,0.4987),(434375,2,0.5001),(449083,2,0.4842),(449868,2,0.4822),(545753,2,0.4372),(555268,2,0.4551),(571858,2,0.4978),(571864,2,0.4638),(591419,2,0.4493),(596365,2,0.4396),(599480,2,0.4814),(610746,2,0.4555),(613938,2,0.4228),(617451,2,0.4887),(625939,2,0.4889),(664061,2,0.4247),(664661,2,0.512),(674297,2,0.5135),(718326,2,0.4566),(735978,2,0.4856),(737950,2,0.4828),(762649,2,0.4267),(787176,2,0.4825),(788989,2,0.4815),(799549,2,0.4502),(802330,2,0.4749),(803751,2,0.4777),(824944,2,0.493),(884455,2,0.4714),(889047,2,0.5259),(963463,2,0.4669),(963651,2,0.4914),(1720760,2,0.456),(1862987,2,0.4505),(1863073,2,0.5047),(11330,9,0.1848),(11594,9,0.1112),(20372,9,0.1151),(27192,9,0.2733),(27802,9,0.2074),(44805,9,0.1867),(55268,9,0.1161),(67216,9,0.1062),(70075,9,0.0595),(82263,9,0.2179),(112175,9,0.1475),(112217,9,0.0747),(121716,9,0.1075),(121881,9,0.1873),(126420,9,0.2075),(126421,9,0.2231),(132313,9,0.2636),(212162,9,0.2257),(220230,9,0.0973),(221178,9,0.181),(224702,9,0.2876),(224847,9,0.0373),(267952,9,0.1626),(299229,9,0.1144),(442057,9,0.225),(442101,9,0.1454),(442117,9,0.1078),(442167,9,0.2134),(442332,9,0.0866),(442418,9,0.2257),(442520,9,0.2587),(442537,9,0.1308),(585398,9,0.2604),(585484,9,0.2615),(586410,9,0.1158),(638820,9,0.1204),(654203,9,0.0756),(768823,9,0.2423),(784470,9,0.2391),(791379,9,0.1825),(820001,9,0.2137),(824917,9,0.1112),(837592,9,0.2752),(843866,9,0.1446),(889025,9,0.1879),(889035,9,0.0976),(974952,9,0.2117),(982717,9,0.137),(1010502,9,0.1732),(1010676,9,0.0561),(1010704,9,0.2106),(1010819,9,0.1168),(1010979,9,0.2137),(1011000,9,0.1132),(1011083,9,-0.0131),(1011085,9,0.1628),(1011223,9,0.1597),(1011232,9,0.2033),(1011260,9,0.12),(1011327,9,0.249),(1011374,9,0.0968),(1011576,9,0.1812),(1011584,9,-0.0614),(1011586,9,0.1316),(1011596,9,-0.0542),(1011610,9,0.0449),(1011673,9,0.1318),(1011772,9,0.1456),(1011814,9,0.1762),(1011882,9,0.1587),(1011920,9,0.1982),(1011934,9,0.0438),(1011949,9,0.1628),(1011977,9,0.1281),(1012075,9,0.1835),(1012314,9,0.1943),(1012375,9,0.1732),(1012411,9,-0.0267),(1012425,9,0.046),(1012436,9,0.0874),(1012543,9,0.1344),(1012606,9,-0.0394),(1012617,9,0.1134),(1012678,9,0.2132),(1012905,9,0.0992),(1013119,9,0.1976),(1013223,9,0.2348),(1013291,9,0.0907),(1013432,9,0.075),(1115241,9,0.1784),(1115242,9,0.1216),(1120937,9,0.0866),(1286547,9,0.1941),(1286598,9,0.077),(1286607,9,0.0989),(1286641,9,0.2399),(1305794,9,0.0042),(1316973,9,0.0756),(1316999,9,0.0448),(1784280,9,0.0705),(11318,6,0.363),(11321,6,0.3108),(19459,6,0.3079),(38732,6,0.315),(55278,6,0.3366),(60027,6,0.3442),(70072,6,0.3375),(212551,6,0.3249),(212567,6,0.3727),(224603,6,0.419),(235665,6,0.3677),(248339,6,0.4081),(290622,6,0.3481),(299230,6,0.2986),(316837,6,0.3049),(437354,6,0.4101),(439861,6,0.4619),(442052,6,0.3605),(442159,6,0.2916),(444252,6,0.2862),(444391,6,0.3352),(545487,6,0.3503),(571850,6,0.2677),(588249,6,0.3602),(613826,6,0.4502),(617304,6,0.3206),(639717,6,0.4533),(655805,6,0.357),(721119,6,0.3678),(838884,6,0.3171),(849800,6,0.286),(1011097,6,0.2954),(1011423,6,0.3916),(1011425,6,0.3352),(1011433,6,0.2696),(1011667,6,0.3028),(1011680,6,0.2983),(1011702,6,0.3718),(1011720,6,0.3571),(1011894,6,0.3402),(1011917,6,0.2522),(1012211,6,0.3392),(1012317,6,0.3086),(1012347,6,0.4118),(1012687,6,0.3242),(1013114,6,0.2939),(1013145,6,0.3955),(1013211,6,0.2764),(1148817,6,0.3837),(1308275,6,0.3725),(35448,8,0.341),(55277,8,0.4822),(99030,8,0.5161),(121876,8,0.547),(183383,8,0.5493),(184952,8,0.5212),(206423,8,0.4493),(236195,8,0.3888),(290237,8,0.4742),(290676,8,0.5341),(300930,8,0.4778),(334438,8,0.5166),(430945,8,0.4509),(442533,8,0.4418),(442534,8,0.5501),(442598,8,0.3842),(445820,8,0.5112),(591106,8,0.422),(615572,8,0.5465),(615577,8,0.4728),(620934,8,0.5349),(623193,8,0.4672),(652981,8,0.5054),(657730,8,0.5011),(660918,8,0.4344),(664159,8,0.4773),(664161,8,0.4769),(669858,8,0.464),(719210,8,0.5409),(721123,8,0.5011),(729780,8,0.4902),(729808,8,0.4574),(729810,8,0.4489),(732511,8,0.5262),(733612,8,0.4795),(737953,8,0.4709),(761665,8,0.5519),(770701,8,0.5197),(796464,8,0.438),(832532,8,0.4994),(835561,8,0.5043),(837581,8,0.4952),(844602,8,0.5233),(863356,8,0.5324),(884341,8,0.5979),(949795,8,0.467),(974861,8,0.5732),(1012726,8,0.4353),(1034658,8,0.6002),(1546453,8,0.5055),(24421,7,0.5278),(65578,7,0.5508),(97718,7,0.5597),(124788,7,0.6108),(202499,7,0.5621),(202500,7,0.5263),(206422,7,0.4578),(218540,7,0.5136),(219615,7,0.5852),(251619,7,0.5555),(251621,7,0.588),(252751,7,0.5646),(267987,7,0.4848),(268128,7,0.5567),(269429,7,0.4804),(287325,7,0.5185),(290630,7,0.5307),(295869,7,0.5427),(304007,7,0.5765),(342296,7,0.5063),(345575,7,0.5063),(439873,7,0.4392),(442192,7,0.561),(442357,7,0.5416),(442394,7,0.5051),(442543,7,0.5267),(444452,7,0.537),(482594,7,0.5813),(482755,7,0.5141),(508237,7,0.5752),(577044,7,0.5424),(577050,7,0.4995),(582204,7,0.4838),(610141,7,0.5319),(631322,7,0.5422),(631465,7,0.4868),(631703,7,0.4423),(636905,7,0.5403),(653520,7,0.5685),(674686,7,0.5816),(729748,7,0.4257),(764826,7,0.4248),(784471,7,0.5683),(801349,7,0.5158),(813914,7,0.575),(839314,7,0.5764),(884952,7,0.5448),(910751,7,0.5833),(972698,7,0.5151),(1012725,7,0.5335),(27184,3,0.349),(27789,3,0.4156),(58188,3,0.3856),(64812,3,0.3603),(64816,3,0.4324),(101855,3,0.458),(102163,3,0.4211),(109502,3,0.2857),(109828,3,0.3988),(109835,3,0.3413),(207058,3,0.4007),(209610,3,0.402),(217348,3,0.3968),(224886,3,0.3683),(256688,3,0.3981),(290624,3,0.3093),(296486,3,0.3611),(296742,3,0.3984),(299831,3,0.3141),(442015,3,0.3441),(442050,3,0.4559),(442137,3,0.4294),(442183,3,0.3803),(442401,3,0.4008),(442496,3,0.369),(442503,3,0.3683),(442510,3,0.4104),(444173,3,0.345),(444235,3,0.3753),(444365,3,0.4087),(444475,3,0.3904),(444480,3,0.3693),(444495,3,0.3009),(464972,3,0.4475),(469732,3,0.3119),(470075,3,0.4184),(545199,3,0.3744),(571869,3,0.2821),(588945,3,0.3089),(617124,3,0.4069),(636650,3,0.3486),(727642,3,0.3691),(784483,3,0.4032),(1011494,3,0.368),(1011520,3,0.4711),(1012098,3,0.4194),(1012445,3,0.4267),(1013295,3,0.3405),(1502033,3,0.3144),(1512895,3,0.3933),(11599,0,0.1872),(390489,0,0.0058),(584342,0,0.0878),(631357,0,-0.0173),(660162,0,-0.0015),(1010655,0,0.0746),(1010855,0,-0.0156),(1010917,0,0.173),(1012083,0,0.1569),(1012140,0,0.2231),(1012352,0,0.1136),(1012807,0,0.1229),(1135857,0,-0.0151),(1148977,0,0.0291),(1307882,0,0.018),(116739,1,0.1073),(121856,1,0.0886),(267949,1,0.2325),(311544,1,0.1107),(320845,1,0.1427),(336915,1,0.0727),(513702,1,0.124),(565412,1,0.1567),(569822,1,0.2293),(579054,1,0.2131),(584212,1,0.1905),(584349,1,0.1944),(585555,1,0.2507),(596852,1,0.0857),(653452,1,0.2183),(653462,1,0.1415),(656981,1,0.1825),(660120,1,0.1412),(718765,1,0.1118),(721126,1,0.0899),(721463,1,0.2431),(722702,1,0.2056),(737553,1,0.1945),(738143,1,0.065),(764883,1,0.0647),(777175,1,0.1561),(784478,1,0.2235),(835765,1,0.1552),(1011330,1,0.113),(1012577,1,0.08),(1130785,1,0.1348),(1135852,1,0.0255),(1136999,1,0.1156),(1306114,1,0.1705),(1512875,1,0.0825),(7511,10,0.515),(142174,10,0.4637),(160106,10,0.5),(320810,10,0.4695),(320887,10,0.5395),(321651,10,0.4966),(322334,10,0.4998),(408272,10,0.515),(442512,10,0.4884),(452191,10,0.5199),(572499,10,0.5175),(578746,10,0.4873),(602128,10,0.5297),(604290,10,0.4835),(668721,10,0.5746),(1010686,10,0.5189),(1010953,10,0.4885),(1010954,10,0.4772),(1011174,10,0.466),(1011243,10,0.5357),(1011323,10,0.4487),(1011560,10,0.493),(1011600,10,0.5487),(1011931,10,0.457),(1012004,10,0.5249),(1012304,10,0.5274),(1012362,10,0.4929),(1012644,10,0.4413),(1012699,10,0.477),(1012703,10,0.4528),(1013035,10,0.4877),(1013198,10,0.4981),(1013222,10,0.5638),(1135847,10,0.4652),(1272407,10,0.5065),(1272414,10,0.5099),(1272415,10,0.4968),(1272798,10,0.4681),(1274341,10,0.5573),(1286618,10,0.5066),(1286636,10,0.4787),(1286637,10,0.4953),(1289415,10,0.4907),(1305147,10,0.5213),(1305736,10,0.5235),(1306424,10,0.466),(1307144,10,0.4755),(1808917,10,0.4727),(1843320,10,0.5125),(1862520,10,0.4267);
/*!40000 ALTER TABLE `cluster_label` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `fg_m_article`
--

DROP TABLE IF EXISTS `fg_m_article`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `fg_m_article` (
  `article_id` bigint(20) NOT NULL,
  `article_publicationcategory` int(11) DEFAULT NULL,
  `article_year` char(4) DEFAULT NULL,
  `article_impact` float DEFAULT NULL,
  `article_title` text,
  `article_abstract` text,
  `article_publicationvenue` int(11) DEFAULT NULL,
  `article_publicationvenuetext` varchar(512) DEFAULT NULL,
  `article_cluster_id` int(11) DEFAULT NULL,
  PRIMARY KEY (`article_id`),
  KEY `article_publicationvenue` (`article_publicationvenue`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `fg_m_article`
--

LOCK TABLES `fg_m_article` WRITE;
/*!40000 ALTER TABLE `fg_m_article` DISABLE KEYS */;
INSERT INTO `fg_m_article` VALUES (2840,NULL,'1986',NULL,'A Heuristic for Suffix Solutions','The suffix problem has appeared in solutions of recurrence systems for parallel and pipelined machines and more recently in the design of gate and silicon compilers. In this paper we present two algorithms. The first algorithm generates parallel suffix solutions with minimum cost for a given length, time delay, availability of initial values, and fanout. This algorithm generates a minimal solution for any length n and depth range from log2 n to n. The second algorithm reduces the size of the solutions generated by the first algorithm.',117351,'IEEE Transactions on Computers',9),(3566,NULL,'1986',NULL,'Efficient Computation of the Maximum of the Sum of Two Sequences and Applications','Computing max{a1+ b1, a2+ b2, ... ,an+ bn} trivially takes n additions. We show that if we are given the ranking for the a\'s and the b\'s separately, then an algorithm exists which will compute the maximum in ?2n additions on the average. This can be generalized to yield an efficient algorithm to compute max{h(a1,b1',117351,'IEEE Transactions on Computers',9),(3885,NULL,'1986',NULL,'Finding Lowest Common Ancestors in Parallel','Two parallel algorithms for finding the lowest common ancestors of a set of vertex pairs Q (the query set) in a directed tree are presented. With all the overheads taken into account, these algorithms take O((n + QI) P log2 n) and O(n2/p + log2n) time, respectively, with p( 0) processors (n is the size of the tree). These results are better than the best known result in that the first achieves th',117351,'IEEE Transactions on Computers',9),(7511,NULL,'1986',NULL,'Implementing Watson\'s algorithm in three dimensions','Computer generated solid models must be decomposed into finite element meshes for analysis by the Finite Element Method. To enable decompositions of complex solid models, tetrahedra are employed and to avoid badly skewed tetrahedra for finite element analysis, a Delaunay triangulation is created by Watson\'s Algorithm [6]. Certain two-dimensional properties of Delaunay triangulations do not extend to the three-dimensional implementation of Watson\'s Algorithm. Furthermore, serious numerical difficulties can occur due to the nonrandomness of triangulation points, Nonrandomness imposed by the geometry can be ameliorated by using tetrahedral decompositions of icosahedra to fill space. A measure of the quality of tetrahedra is proposed and used to identify undesirable tetrahedra created due to point distributions and geometric constraints of solid models. Postprocessing Delaunay triangulations to rectify undesirable tetrahedra is briefly discussed.',211170,'SCG \'86 Proceedings of the second annual symposium on Computational geometry',2),(8194,NULL,'1987',NULL,'Distrbution and Abstract Types in Emerald','Emerald is an object-based language for programming distributed subsystems and applications. Its novel features include 1) a single object model that is used both for programming in the small and in the large, 2) support for abstract types, and 3) an explicit notion of object location and mobility. This paper outlines the goals of Em-erald, relates Emerald to previous work, and describes its type system and distribution support. We are currently constructing a prototype implementation of Emerald.',117430,'IEEE Transactions on Software Engineering - Special issue on distributed systems',5),(11318,NULL,'1986',NULL,'On the Diagnosis of System Faults with Propagation','This correspondence proposes a new fault model for system diagnosis, wherein the interaction among faulty subsystems and fault propagation is considered. Criteria for one-step diagnosability are given. Optimal design of diagnosable systems is also developed. As to sequential diagnosis, single-loop systems are studied in depth.',117351,'IEEE Transactions on Computers',6),(11321,NULL,'1986',NULL,'Roving Emulation as a Fault Detection Mechanism','In this paper we present a new built-in test methodology for detecting and locating faults in digital systems. The technique is called roving emulation and consists of an off-line snap shot type emulation or simulation of operating components in a system. Its primary application is in testing systems in the field where real-time fault detection is not required. The primary performance measure of this test schema is taken to be the expected value of the error latency, i.e., the time required to detect a fault once it first occurs. The primary results of this paper deal with deriving equations for the error latency. We present both a probabilistic and service-waiting model to analyze the expected error latency in a system tested via roving emulation. The effects of various controllable and uncontrollable system parameters on error latency are studied. Finally, the technique is applied to a system consisting of combinational logic modules, and numerical results are presented.',117351,'IEEE Transactions on Computers',6),(11330,NULL,'1986',NULL,'Techniques for Computing the Discrete Fourier Transform Using the Quadratic Residue Fermat Number Systems','In this correspondence, the complex integer multiplier and adder over the direct sum of two copies of finite field developed in [1] is specialized to the direct sum of the rings of integers modulo Fermat numbers. Such multiplication over the rings of integers modulo Fermat numbers can be performed by means of two integer multiplications, whereas the complex integer multiplication requires three integer multiplications. Such multiplications and additions can be used in the implementation of a discrete Fourier transform (DFT) of a sequence of complex numbers. The advantage of the present approach is that the number of multiplications needed to compute a systolic array of the DFT can be reduced substantially. The architectural designs using this approach are regular, simple, expandable and, therefore, naturally suitable for VLSI implementation.',117351,'IEEE Transactions on Computers',8),(11594,NULL,'1986',NULL,'Complexity Based on Partitioning of Boolean Circuits and their Relation to Multivalued Circuits','We present a new complexity measure for Boolean functions based on partitions of combinatorial circuits into subcircuits, and give upper and lower bounds on the complexity for Boolean functions. Roughly speaking, for a function g whose range is the set of positive integers, g(n)-partition of a circuit is a partition of a circuit into subcircuits such that. 1) each subcircuit has at most g(n) output gates, through which gates in the subcircuit are connected to gates in another succeeding subcircuits and 2) each subcircuit has at most two preceding subcircuits where subcircuit N1 (N2) is said to precede (succeed) subcircuit N2 (N1) when there is a line directed from a gate in N1 to a gate in N2. Our main result, which is stated in terms of a lower bound theorem and an upper bound theorem, is a precise version of the following statement: For \"almost all\" n-argument Boolean functions fn, the minimum nu mber of subcircuits over g n)-partition of a circuit to compute fn is given as (2n/g(n)22g(n)) unless g(n) grows much more slowly than n as n increases. To prove the theorems, we regard a Boolean circuit, together with g(n)-partition of it, as the multivalued circuit composed of multivalued gates corresponding to the subcircuits obtained from the g(n)-partition.',117351,'IEEE Transactions on Computers',1),(11599,NULL,'1986',NULL,'The Current Mode Fuzzy Logic Integrated Circuits Fabricated by the Standard CMOS Process','Nine basic fuzzy logic circuits employing p-ch and n-ch current mirrors are presented, and the fuzzy information processing hardware system design at a low cost with only one kind of master slice (semicustom fuzzy logic IC) is described. The fuzzy logic circuits presented here will be indispensable for a \"fuzzy computer\" in the near future.',117351,'IEEE Transactions on Computers',10),(12270,NULL,'1986',NULL,'A VLSI Solution to the Vertical Segment Visibility Problem','We present a parallel algorithm to solve the visibility problem among n vertical segments in a plane, which can be implemented on a VLSI chip arranged as a mesh of trees. Our algorithm determines all the pairs of segments that \"see\" each other in time O(log n); while the fastest sequential algorithm requires O(n log n). A lower bound to the area-time complexity of this problem of O(n2 log2 n) is also derived.',117351,'IEEE Transactions on Computers',9),(12638,NULL,'1986',NULL,'An O(20.304n) Algorithm for Solving Maximum Independent Set Problem','A faster algorithm for finding a maximum independent set in a graph is presented. The algorithm is an improved version of the one by Tarjan and Trojanowski [7]. A technique to further accelerate this algorithm is also described.',117351,'IEEE Transactions on Computers',9),(19452,NULL,'1987',NULL,'Pseudorandom Testing','Algorithmic test generation for high fault coverage is an expensive and time-consuming process. As an alternative, circuits can be tested by applying pseudorandom patterns generated by a linear feedback shift register (LFSR). Although no fault simulation is needed, analysis of pseudorandom testing requires the circuit detectability profile.',117351,'IEEE Transactions on Computers',11),(19458,NULL,'1987',NULL,'A New Built-In Self-Test Design for PLA\'s with Hligh Fault Coverage and Low Overhead','This correspondence presents a new built-in self-test design for PLA\'s, that has a lower area overhead and higher multiple fault coverage (of three types of faults: crosspoint, stuck, and bridging) than any existing design. This new design uses function independent test input patterns (which are generated on chip), compresses the output responses into a function independent string of parity bits (whose fault-free expected values are generated on-line with a simple circuit), and detects all siqgle faults and more than ( 1 --2(m+2n) of all multiple faults where m and n represent the number of product terms and input variables, respectively.',117351,'IEEE Transactions on Computers',11),(19459,NULL,'1987',NULL,'The Comparison Approach to Multiprocessor Fault Diagnosis','In this correspondence a system-level, comparison-based strategy for identifying faulty processors in a multiprocessor system is described. Unlike other strategies which have been proposed in the literature, the comparison approach is more efficient and relies on more realistic assumptions about the system under consideration. The new strategy is shown to correctly identify the set of faulty processors with a remarkably high probability, making it an attractive and viable addition or alternative to present fault diagnosis techniques.',117351,'IEEE Transactions on Computers',6),(20372,NULL,'1987',NULL,'Design of a high-speed square root multiply and divide unit','In this paper radix-4 algorithms for square root and division are developed. The division algorithm evaluates the more useful function xz/y. These algorithms are shown to be suitable for implementing as a unified hardware unit which evaluates square root, division, and multiplication. Cost reductions in the hardware are obtained by use of gate arrays. A design based on the Motorola MCA2500 series of Macrocell gate array (MCA) is presented. At a cost of 9 MCA\'s and 16 commercial ECL 100 K parts a 64-bit square root can be evaluated in 750 us using worst case delays. Division takes 710 ns and multiplication 325 ns. Redundancy in the digit set together with carry-save adders are used to achieve this high performance.',117351,'IEEE Transactions on Computers',8),(23615,NULL,'1987',NULL,'A new approach to all pairs shortest paths in planar graphs','An algorithm is presented for generating a succinct encoding of all pairs shortest path information in a directed planar graph G with real-valued edge costs but no negative cycles. The algorithm runs in &Ogr;(pn) time, where n is the number of vertices in G, and p is the minimum cardinality of a subset of the faces that cover all vertices, taken over all planar embeddings of G. Linear-time algorithms are presented for various subproblems including that of finding an appropriate embedding of G and a corresponding face-on-vertex covering of cardinality &Ogr;(p), and of generating all pairs shortest path information in a directed outerplanar graph.',225134,'STOC \'87 Proceedings of the nineteenth annual ACM symposium on Theory of computing',9),(23649,NULL,'1987',NULL,'Fast parallel algorithms for chordal graphs','We present an NC algorithm for recognizing chordal graphs, and we present NC algorithms for finding the following objects on chordal graphs: all maximal cliques, an intersection graph representation, an optimal coloring, a perfect elimination scheme, a maximum independent set, a minimum clique cover, and the chromatic polynomial. The well known polynomial algorithms for these problems seem highly sequential, and therefore a different approach is needed to find parallel algorithms.',225134,'STOC \'87 Proceedings of the nineteenth annual ACM symposium on Theory of computing',9),(24421,NULL,'1987',NULL,'Line (block) size choice for CPU cache memories','The line (block) size of a cache memory is one of the parameters that most strongly affects cache performance. In this paper, we study the factors that relate to the selection of a cache line size. Our primary focus is on the cache miss ratio, but we also consider influences such as logic complexity, address tags, line crossers, I/O overruns, etc. The behavior of the cache miss ratio as a function of line size is examined carefully through the use of trace driven simulation, using 27 traces from five different machine architectures. The change in cache miss ratio as the line size varies is found to be relatively stable across workloads, and tables of this function are presented for instruction caches, data caches, and unified caches. An empirical mathematical fit is obtained. This function is used to extend previously published design target miss ratios to cover line sizes from 4 to 128 bytes and cache sizes from 32 bytes to 32K bytes; design target miss ratios are to be used to guide new machine designs. Mean delays per memory reference and memory (bus) traffic rates are computed as a function of line and cache size, and memory access time parameters. We find that for high performance microprocessor designs, line sizes in the range 16-64 bytes seem best; shorter line sizes yield high delays due to memory latency, although they reduce memory traffic somewhat. Longer line sizes are suitable for mainframes because of the higher bandwidth to main memory.',117351,'IEEE Transactions on Computers',4),(27184,NULL,'1987',NULL,'On the permutation capability of multistage interconnection networks','We present analytic models for the blocking probability of both unique path and multiple path multistage interconnection networks under the assumption of either permutation or random memory request patterns. The blocking probability of an interconnection network under the assumption of permutation requests is a quantitative measure of the network\'s permutation capability. We compare the performance of networks with approximately equivalent hardware complexity. It is shown that variations of banyan networks can be designed with extremely low blocking probabilities under the assumption of permutation requests.',117351,'IEEE Transactions on Computers',7),(27192,NULL,'1987',NULL,'On-the-fly conversion of redundant into conventional representations','An algorithm to convert redundant number representations into conventional representations is presented. The algorithm is performed concurrently with the digit-by-digit generation of redundant forms by schemes such as SRT division. It has a step delay roughly equivalent to the delay of a carry-save adder and simple implementation. The conversion scheme is applicable in arithmetic algorithms such as nonrestoring division, square root, and on-line operations in which redundantly represented results are generated in a digit-by-digit manner, from most significant to least significant.',117351,'IEEE Transactions on Computers',8),(27210,NULL,'1987',NULL,'Test Length for Pseudorandom Testing','The process of determining the required test length for a desired level of confidence for pseudorandom testing using a random sampling without replacement model is examined. The differences between random and pseudorandom testing are discussed and developed. The strictly random testing model is shown to be inaccurate for high confidence testing of combinational circuits. A method of calculating the required test length for pseudorandom testing based upon fault detectabilities is described. The result provides a very accurate prediction of required test length applicable to self-test using pseudorandom inputs.',117351,'IEEE Transactions on Computers',11),(27218,NULL,'1987',NULL,'Optimal Graph Algorithms on a Fixed-Size Linear Array','Parallel algorithms for computing the minimum spanning tree of a weighted undirected graph, and the bridges and articulation points of an undirected graphs on a fixed-size linear array of processors are presented. For a graph of n vertices, the algorithms operate on a linear array of p processors and require O(n2/p) time for all p, 1 = p = n. In particular, using n processors the algorithms require O(n) time which is optimal on this model. The paper describes two approaches to limit the communication requirements for solving the problems. The first is a divide-and-conquer strategy applied to Sollin\'s algorithm for finding the minimum spanning tree of a graph. The second uses a novel data-reduction technique that constructs an auxiliary graph with no more than 2n - 2 edges, whose bridges and articulation points are the bridges and articulation points of the original graph.',117351,'IEEE Transactions on Computers',9),(27789,NULL,'1987',NULL,'Deadlock-Free Message Routing in Multiprocessor Interconnection Networks','A deadlock-free routing algorithm can be generated for arbitrary interconnection networks using the concept of virtual channels. A necessary and sufficient condition for deadlock-free routing is the absence of cycles in a channel dependency graph. Given an arbitrary network and a routing function, the cycles of the channel dependency graph can be removed by splitting physical channels into groups of virtual channels. This method is used to develop deadlock-free routing algorithms for k-ary n-cubes, for cube-connected cycles, and for shuffle-exchange networks.',117351,'IEEE Transactions on Computers',7),(27802,NULL,'1987',NULL,'Integer Division in Linear Time with Bounded Fan-In','A binary algorithm for division of an (M + N)-bit integer by an N-bit integer is presented. The algorithm produces the (M + 1)-bit quotient and the N-bit remainder in time O(M + N). Two hardware implementations, one using combinational logic in cellular arrays, and one employing systolic arrays, are given. These implementations are designed for modularity and regularity, and thus are suitable for VLSI systems. An important property of these implementations is that decisions are based on only one bit of the operands. Thus, fan-in and length of connecting wires are bounded independently of operand size. In addition, the systolic implementation has area O + N), which is the best possible.',117351,'IEEE Transactions on Computers',8),(32199,NULL,'1987',NULL,'Circular self-test path: a low-cost BIST technique','A new technique for designing self-testing VLSI circuits, referred to as Circular Self-Test Path, is presented The Circular Self-Test Path is a feedback shift register (output of the last flip-flop is supplied to the first flip-flop) with a data compaction capability. A distinguishing attribute of self-testing chips designed using this technique is a low silicon area overhead, slightly exceeding that of scan path designs. A theoretical analysis and comprehensive simulation experiments we performed to demonstrate that the effectiveness of test pattern generation for the circular self-test path is comparable to that of an ideal pseudorandom test generator.',67540,'DAC \'87 Proceedings of the 24th ACM/IEEE Design Automation Conference',11),(35448,NULL,'1987',NULL,'Task Allocation and Precedence Relations for Distributed Real-Time Systems','In a distributed processing system with the application software partitioned into a set of program modules, allocation of those modules to the processors is an important problem. This paper presents a method for optimal module allocation that satisfies certain performance constraints. An objective function that includes the intermodule communication (IMC) and accumulative execution time (AET) of each module is proposed. It minimizes the bottleneck-processor utilization a good principle for task allocation. Next, the effects of precedence relationship (PR) among program modules on response time are studied. Both simulation and analytical results reveal that the program-size ratio between two consecutive modules plays an important role in task response time. Finally, an algorithm based on PR, AET, and IMC and on the proposed objective function is presented. This algorithm generates better module assignments than those that do not consider the PR effects.',117351,'IEEE Transactions on Computers',0),(35909,NULL,'1987',NULL,'Fast algorithms for computing the largest empty rectangle','We provide two algorithms for solving the following problem: Given a rectangle containing n points, compute the largest-area and the largest-perimeter subrectangles with sides parallel to the given rectangle that lie within this rectangle and that do not contain any points in their interior. For finding the largest-area empty rectangle, the first algorithm takes &Ogr;(n log3 n) time and &Ogr;(n) memory space and it simplifies the algorithm given by Chazelle, Drysdale and Lee which takes &Ogr;(n log3 n) time but &Ogr;(n log n) storage. The second algorithm for computing the largest-area empty rectangle is more complicated but it only takes &Ogr;(n log2 n) time and &Ogr;(n) memory space. The two algorithms for computing the largest-area rectangle can be modified to compute the largest-perimeter rectangle in &Ogr;(n log2 n) and &Ogr;(n log n) time, respectively. Since &OHgr;(n log n) is a lower bound on time for computing the largest-perimeter empty rectangle, the second algorithm for computing such a rectangle is optimal within a multiplicative constant.',211171,'SCG \'87 Proceedings of the third annual symposium on Computational geometry',9),(38732,NULL,'1988',NULL,'Error Secure/Propagating Concept and its Application to the Design of Strongly Fault-Secure Processors','A concept of the error-secure and the error-propagating interfaces of the subsystems in a digital system is introduced, and shown to be useful for practical design and verification for a strongly fault-secure system which is known to achieve the totally self-checking (TSC) goal. A sufficient condition is shown for subsystem interfaces to meet for it to be possible to construct a strongly fault-secure system with no checkers used to monitor the embedded interfaces. On the basis of the error-secure/propagating concept, a design is presented for a strongly fault-secure microprocessor which implements the instruction set of Intel\'s i8080 8-b microprocessor. In the design, a complete set of building blocks is defined and all the partial interfaces are verified for the error secure/propagating property. Only four checkers are used at the embedded interfaces in the resulting strongly fault-secure processor.',117351,'IEEE Transactions on Computers',6),(40420,NULL,'1988',NULL,'Pseudoexhaustive Test Pattern Generator with Enhanced Fault Coverage','A method of pseudoexhaustive test pattern generation is proposed that is suitable above all for circuits using random access scan. Two linear feedback shift registers are used to generate scan addresses and test patterns to be scanned into these addresses. It is shown that the method gives better results than random testing.',117352,'IEEE Transactions on Computers - Fault-Tolerant Computing',11),(44805,NULL,'1988',NULL,'Relationship Between P-Valued Majority Functions and P-Valued Threshold Functions','In a previous paper (Trans. IECE Japan, vol.J63-D, p.493-500, 1980, vol.J64-D, p.172-3, 1981 and vol.E-67, p.47-8, 1984), the authors defined a new class of multiple-valued logic functions, called multiple-valued majority functions. The authors clarifies the distinction of multiple-valued majority functions from multiple-valued threshold functions through the difference between a number function and an inner product of an input vector and a weight vector.',117351,'IEEE Transactions on Computers',1),(54093,NULL,'1988',NULL,'An open enviornment for building parallel programming systems','PRESTO is a set of tools for building parallel programming systems on shared-memory multiprocessors. PRESTO\'s goal is to provide a framework within which one can easily build efficient support for any of a wide variety of “models” of parallel programming. PRESTO is designed for easy modification and extension, not only at the level of the primitives and structures made available for the application programmer\'s use, but also at the level of the run-time kernel that supports parallel applications. PRESTO is implemented in the object-oriented language C++ on a Sequent Balance 21000 and has been used in a number of applications that are described in this paper.',186312,'PPEALS \'88 Proceedings of the ACM/SIGPLAN conference on Parallel programming: experience with applications, languages and systems',5),(54214,NULL,'1988',NULL,'A faster strongly polynomial minimum cost flow algorithm','We present a new strongly polynomial algorithm for the minimum cost flow problem, based on a refinement of the Edmonds-Karp scaling technique. Our algorithm solves the uncapacitated minimum cost flow problem as a sequence of &Ogr;(n log n) shortest path problems on networks with n nodes and m arcs and runs in &Ogr;(n log n(m + n log n)) steps. Using a standard transformation, this approach yields an &Ogr;(m log n (m + n log n)) algorithm for the capacitated minimum cost flow problem. This algorithm improves the best previous strongly polynomial algorithm due to Galil and Tardos, by a factor of m/n. Our algorithm is even more efficient if the number of arcs with finite upper bounds, say m\', is much less than m. In this case, the number of shortest path problems solved is &Ogr;((m + n) log n).',225135,'STOC \'88 Proceedings of the twentieth annual ACM symposium on Theory of computing',9),(55268,NULL,'1989',NULL,'On the Optimal Design of Multiple-Valued PLAs','A description is given of the design and analysis of three types of multivalued PLAs (programmable logic arrays). Type 1 PLAs realize functions directly in the form of the max of min of literal functions and constants. In Type 2 PLAs, the body of the PLA is binary and the output is encoded as a multiple-valued logic value. Type 3 PLAs are the same as type 2 PLAs except for the use of 2-bit decoders and a permutation network on the input. Using the number of columns required to realize a given function as a measure to compare PLAs, it is shown that type 3 PLAs are superior to type 2, which in turn are superior to type 1.',117351,'IEEE Transactions on Computers',1),(55277,NULL,'1989',NULL,'Decentralized Decision-Making for Task Reallocation in a Hard Real-Time System','A decentralized task reallocation algorithm for hard real-time systems is developed and analyzed. The algorithm, which is fast and reliable, specifically considers deadlines of tasks, attempts to utilize all the nodes of a distributed system to achieve its objective, handles tasks in priority order, and separates policy and mechanism. An extensive performance analysis of the algorithm by means of simulation shows that it is quite effective in performing reallocations and that it is significantly better than a centralized approach.',117351,'IEEE Transactions on Computers',0),(55278,NULL,'1989',NULL,'Dynamic Testing Strategy for Distributed Systems','Fault diagnosis is treated as two distinct processes: fault discovery and dissemination of diagnostic information. Previous research determined what level of self-diagnosability a given set of test in a homogeneous system achieves, using a model in which only node failures occur and test coverage is complete. Adopting the same model, a new methodology is presented that minimizes the overhead associated with periodic testing, thus lowering testing overhead. The method diagnoses up to c-.1 faults (c is the connectivity of the system topology). The savings in testing is valid when processor failure rates are low. Environments are also examined with high processor failure rates. It is shown that adopting the proposed methodology for such systems results in greater reliability, while maintaining the same effective processing power.',117351,'IEEE Transactions on Computers',6),(58188,NULL,'1989',NULL,'The PM221 Interconnection Network','A scheme based on the standard multiplier recoding technique that enables the use of (5*5) switches for PM2I networks is presented. The connections between switching stages are based on the modified PM2I functions, and are called plus-minus-2/sup 2i/ (PM22I); hence, these networks are called the PM22I networks. Since the number of switching stages in the PM22I networks is one-half of those in the PM2I networks, with a moderate increase in the switch size from (3*3) to (5*5), these networks provide similar design tradeoffs availability by the (2*2) and (4*4) cube networks.',117351,'IEEE Transactions on Computers',7),(60027,NULL,'1989',NULL,'Coverage Modeling for Dependability Analysis of Fault-Tolerant Systems','Several different models for predicting coverage in a fault-tolerant system, including models for permanent, intermittent, and transient errors, are discussed. Markov, semi-Markov, nonhomogeneous Markov, and extended stochastic Petri net models for computing coverage are developed. Two types of events that interfere with recovery are examined; and methods for modeling such events, whether they are deterministic or random, are given. The sensitivity of system reliability/availability to the coverage parameter and the sensitivity of the coverage parameter to various error-handling strategies are investigated. It is found that a policy of attempting transient recovery upon detection of an error (as opposed to automatically reconfiguring the affected component out of the system) can actually increase the unreliability of the system.',117351,'IEEE Transactions on Computers',6),(60045,NULL,'1989',NULL,'A Fast Algorithm for Testing Isomorphism of Permutation Networks','The problem of deciding whether two given permutation sequences are conjugate is addressed. The author exhibits an algorithm that solves this problem in time O(N log N), where N is the sum of the sizes of the two sequences. The algorithm can be applied to the problem of deciding whether two permutation networks are equivalent. The time bound for the algorithm is a significant improvement over the O(N/sup 2/) bound of the algorithm due to Y.A. Oruc and M.Y. Oruc (1985). The author\'s method applies techniques used by the Hopcroft-Tarjan algorithm for deciding planar graph isomorphism.',117351,'IEEE Transactions on Computers',9),(60096,NULL,'1989',NULL,'Fair Edge Deletion Problems','The notation of fair edge-deletion problems is introduced. These arise when it is desirable to control the number of edges incident to any node that are either deleted or remain following edge deletion. Six such problems were formulated for the case where the resultant graph is known to be acyclic, and the complexity of four of these is easily determined from known results. The remaining two are the authors\' focus. It is shown that the problem of finding a minimum-degree deletion graph H such that G-H is acyclic is NP-hard when G is undirected, and is solvable in linear time when G is directed.',117351,'IEEE Transactions on Computers',9),(64812,NULL,'1989',NULL,'An Optimal Shortest-Path Routing Policy for Network Computers with Regular Mesh-Connected Topologies','A probabilistic routing policy, the Z/sup 2/ (zigzag) routing policy, is presented within the class of nonadaptive, shortest-path routing policies for regular mesh-connected topologies such as n-dimensional toroids and hypercubes. The focus of the research is routing in networks of computers in a distributed computing environment, where constituent subcomputers are organized in a mesh-connected topology and communication among individual computers takes places by some form of message exchange. The authors prove the optimality of this policy with respect to two criteria: (1) maximizing the probability of reaching the destination from a given source without delays at intermediate nodes; and (2) minimizing the expected lifetime of a message.',117351,'IEEE Transactions on Computers',7),(64816,NULL,'1989',NULL,'Merging and Sorting Networks with the Topology of the Omega Network','A class of comparator networks obtained from the omega permutation network by replacing each switch with a comparator exchanger of arbitrary direction is considered. These networks are all isomorphic to each other, have merging capabilities, and can be used as building blocks of sorting networks in ways different from the standard merge-sort scheme. It is shown that the bitonic and balanced mergers are members of the class. These two networks were not previously known to be isomorphic.',117351,'IEEE Transactions on Computers',7),(65578,NULL,'1989',NULL,'The impact of code density on instruction cache performance','The widespread use of reduced-instruction-set computers has generated a lot of interest in the tradeoff between the density of an instruction set and the size of the instruction cache. In this paper we present and justify a method that predicts the cache performance for a wide range of architectures, based on the miss rate for a single architecture. When we apply the method to a number of cache organizations we find that changes in code density can have a dramatic impact on memory traffic, but that modest improvements in code density do not reduce program execution time significantly in a well-balanced system.',131955,'ISCA \'89 Proceedings of the 16th annual international symposium on Computer architecture',4),(67216,NULL,'1989',NULL,'Incremental Computation of Squares and Sums of Squares','An incremental algorithm for computation of sums of squares is presented that is suitable for both most-significant-bit- (MSB-)first and least-significant-bit- (LSB-)first bit-sequential operation. By exploiting symmetry properties of numerical values and evaluation times in the bit-product matrix, it is shown how incremental multipliers can be converted to perform squaring at reduced hardware cost, and sum of squaring at a hardware cost to that of scalar multiplication. By the elimination of redundant computation, existing hardware modules are either reduced in size or assigned to the evaluation of a second squaring computation. The corresponding hardware architectures are derived from a simple conversion of existing incremental scalar multipliers. This conversion process is less practical on standard serial/parallel or serial-pipeline multipliers. A digit-on-line algorithm is outlined for magnitude extraction operations on plane vectors.',117351,'IEEE Transactions on Computers',8),(70072,NULL,'1990',NULL,'Dependability Modeling and Evaluation of Software Fault-Tolerant Systems','Dependability modeling and evaluation (encompassing reliability and safety issues) of the two major fault tolerance software approaches-recovery blocks (RBs) and N version programming (NVP)-are presented. The study is based on the detailed analysis of software fault-tolerance architectures able to tolerate a single fault (RB: two alternates and an acceptance test; NVP: three versions and a decider). For each approach a detailed model based on the software production process is established and then simplified by assuming that only a single fault type may manifest during execution of the fault-tolerant software and that no error compensation may take place within the software. The analytical results obtained make it possible to identify the improvement, compared to a non-fault-tolerant software, that could result from the use of RB (the acceptance test has to be more reliable from the alternates) and NVP (related faults among the versions and the decider have to be minimized) and to determine the most critical types of related faults. Nested RBs are studied, showing that the proposed analysis approach can be applied to such realistic software structures and that when an alternate is itself an RB, the results are analogous to the case of the addition of a third alternate. The reliability analysis shows that only a small improvement can be expected.',117351,'IEEE Transactions on Computers',6),(70075,NULL,'1990',NULL,'Redundant and On-Line CORDIC: Application to Matrix Triangularization and SVD','Several modifications to the CORDIC method of computing angles and performing rotations are presented: (1) the use of redundant (carry-free) addition instead of a conventional (carry-propagate) one; (2) a representation of angles in a decomposed form to reduce area and communication bandwidth; (3) the use of on-line addition (left-to-right, digit-serial addition) to replace shifters by delays; and (4) the use of online multiplication, square root, and division to compute scaling factors and perform the scaling operations. The modifications improve the speed and the area of CORDIC implementations. The proposed scheme uses efficiently floating-point representations. The application of the modified CORDIC method to matrix triangularization by Givens\' rotations and to the computation of the singular value decomposition (SVD) are discussed.',117351,'IEEE Transactions on Computers',8),(70146,NULL,'1990',NULL,'The Ballast Methodology for Structured Partial Scan Design','An efficient partial scan technique called Ballast (balanced structure scant test) is presented. Scan path storage elements (SPSEs) are selected such that the remainder of the circuit has certain desirable testability properties. A complete test set is obtained using combinatorial automatic test pattern generation (ATPG). Some SPSEs may need to be provided with a HOLD mode; their number is minimized by ordering the registers in the scan path and formatting the test patterns appropriately. This methodology leads to a low area overhead and allows 100% coverage of irredundant faults.',117351,'IEEE Transactions on Computers',11),(82263,NULL,'1990',NULL,'Fast Multiplication Without Carry-Propagate Addition','Conventional schemes for fast multiplication accumulate the partial products in redundant form (carry-save or signed-digit) and convert the result to conventional representation in the last step. This step requires a carry-propagate adder which is comparatively slow and occupies a significant area of the chip in a VLSI implementation. A report is presented on a multiplication scheme (left-to-right, carry-free, LRCF) that does not require this carry-propagate step. The LRCF scheme performs the multiplication most-significant bit first and produces a conventional sign-and-magnitude product (most significant n bits) by means of an on-the-fly conversion. The resulting implementation is fast and regular and is very well suited for VLSI. The LRCF scheme for general radix r and a radix-4 signed-digit implementation are presented.',117351,'IEEE Transactions on Computers',8),(87099,NULL,'1990',NULL,'Philosophies for Engineering Computer-Based Systems','A sound problem-relevant philosophy is the key to achieving successful implementation of complex computer-based systems and software engineering methods and tools naturally flow from this foundation. The importance of a philosophy is illustrated by considering the development of Simula. Several philosophy-relevant factors are discussed and experiences where philosophies have played a central role in realizing complex real-time applications are described. These experiences relate to telecommunication software, automatic train control and power dispatching.',56357,'Computer',5),(90241,NULL,'1990',NULL,'An Optimal Algorithm for Detecting Weak Visibility of a Polygon','Notation and a theorem are presented which, using a result of B. Chazelle and L.J. Guibas (1985), enable the authors to design an O(n log n) algorithm for reporting all visibility edges of a given n-vertex polygon. Improving on this bound to O(n) is presently focused upon. This problem is solved for polygons with at least one given visibility edge. It is assumed that both endpoints of this edge are convex vertices. Subsequently, it is shown how to drop this restriction. The general case of detecting weak edge visibility of an arbitrary simple polygon is dealt with.',117351,'IEEE Transactions on Computers',9),(91230,NULL,'1991',NULL,'PAWS: A Performance Evaluation Tool for Parallel Computing Systems','A description is given of PAWS (parallel assessment window system), a set of tools that provides an interactive user-friendly environment for analysis of existing, prototype, and conceptual machine architectures running a common application. PAWS consists of an application tool, an architectural characterization tool, a performance assessment tool, and an interactive graphical display tool. The application characterization tool provides a facility for evaluating the level and degree of an application\'s parallelism. The architecture characterization tool allows users to create, store, and retrieve descriptions of machines in a database. This approach permits users to evaluate conceptual machines before building any hardware. The performance assessment tool generates profile plots through the interactive graphical display tool. It shows both the ideal parallelism inherent in the machine-independent dataflow graph and the predicted parallelism of the partitioned dataflow graph on the target machine.',56366,'Computer - Special issue on experimental research in computer architecture',5),(97718,NULL,'1990',NULL,'Improving instruction cache behavior by reducing cache pollution','In this paper we describe compiler techniques for improving instruction cache performance. Through repositioning of the code in main memory, leaving memory locations unused, code duplication, and code propagation, the effectiveness of the cache can be improved due to reduced cache pollution and fewer cache misses. Results of experiments indicate that significant reduction in bus traffic results from the use of these techniques. Since memory bandwidth is a critical resource in shared memory multiprocessors, such systems can benefit from the techniques described. The notion of control dependence is used to decide when instructions belonging to different basic blocks can be allowed to share the same cache line without increasing cache pollution.',190556,'Proceedings of the 1990 ACM/IEEE conference on Supercomputing',4),(99030,NULL,'1991',NULL,'A Pre-Run-Time Scheduling Algorithm for Hard Real-Time Systems','Process scheduling, an important issue in the design and maintenance of hard real-time systems, is discussed. A pre-run-time scheduling algorithm that addresses the problem of process sequencing is presented. The algorithm is designed for multiprocessor applications with preemptable processes having release times, computation times, deadlines and arbitrary precedence and exclusion constraints. The algorithm uses a branch-and-bound implicit enumeration technique to generate a feasible schedule for each processor. The set of feasible schedules ensures that the timing specifications of the processes are observed and that all the precedence and exclusion constraints between pairs of processes are satisfied. the algorithm was tested using a model derived from the F-18 mission computer operational flight program.',117420,'IEEE Transactions on Software Engineering',0),(101855,NULL,'1991',NULL,'A Study of Odd Graphs as Fault-Tolerant Interconnection Networks','Odd graphs are analyzed to determine their suitable in designing interconnection networks. These networks are shown to possess many features that make them competitive with other architectures, such as ring, star, mesh, the binary n-cube and its generalized form, the chordal ring, and flip-trees. Among the features are small internode distances, a lighter density, simplicity in implementing various self-routing algorithms (both for faulty and nonfaulty networks), capability of maximal fault tolerance, strong resilience, and good persistence. The routing algorithms (both for the faulty and fault-free networks) do not require any table lookup mechanism, and intermediate nodes do not need to modify the message. These graphs are shown to have a partitioning property that is based on Hadamard matrices and can be effectively used for a system\'s expansion and self-diagnostics.',117351,'IEEE Transactions on Computers',7),(102163,NULL,'1991',NULL,'An Adaptive and Fault Tolerant Wormhole Routing Strategy for k-ary n-cubes','The concept of virtual channels is extended to multiple virtual communication systems that provide adaptability and fault tolerance in addition to being deadlock-free. A channel dependency graph is taken as the definition of what connections are possible, and any routing function must use only those connections defined by it. Virtual interconnection networks allowing adaptive, deadlock-free routing are examined for three k-ary n-cube topologies: unidirectional, torus-connected bidirectional, and mesh-connected bidirectional.',117351,'IEEE Transactions on Computers',7),(102176,NULL,'1991',NULL,'Minimum Spanning Trees of Moving Points in the Plane','Consideration is given to the following problem. Preprocess n moving points in a plane, such that the Euclidean minimum spanning tree of these points at a given time t can be reported efficiently. In the result, if the moving points are in k-motion, after an O(kn/sup 4/ log n) time preprocessing step and using O(m) space to store the preprocessing result, the Euclidean minimum spanning tree at t can be reported in O(n) time, where m denotes the number of changes of the Euclidean minimum spanning tree of these points from time t=0 to time t= infinity.',117351,'IEEE Transactions on Computers',9),(109502,NULL,'1990',NULL,'Minimal Order Loop-Free Routing Strategy','A multiorder routing strategy is developed which is loop-free even in the presence of link/node failures. Unlike most conventional methods in which the same routing strategy is applied indiscriminately to all nodes in the network, nodes under this proposal may adopt different routing strategies according to the network structure. Formulas are developed to determine the minimal order of routing strategy for each node to eliminate looping completely. A systematic procedure for striking a compromise between the operational overhead and network adaptability is proposed. Several illustrative examples are presented.',117351,'IEEE Transactions on Computers',7),(109792,NULL,'1991',NULL,'Polynomial Complexity Algorithms for Increasing the Testability of Digital Circuits by Testing-Module Insertion','The authors present a method for increasing the testability of combinational circuits for single stuck-at faults by partitioning the circuit and inserting testing-modules. A testing-module structure that allows lines in the circuit to be logically disconnected is shown. It allows the circuit to be partitioned into independent subcircuits. A test generation algorithm that is based on test set merging is presented. An optimal testing-module placement algorithm is described for fanout free circuits. A special type of circuit with fanout for which optimal testing-module placement can also be performed is defined, and a testing-module placement algorithm for them is outlined. For general fanout circuits, polynomial testing-module placement and test generation algorithms are described. Testing-modules are used for partitioning the circuit into fanout free subcircuits. Test set merging that yields a complete test set for the circuit is described.',117351,'IEEE Transactions on Computers',11),(109828,NULL,'1991',NULL,'Indirect Star-Type Networks for Large Multiprocessor Systems','The authors propose three indirect star-type networks, the indirect star networks I and II and the star-delta network, and investigate their properties. An indirect star-type network is obtained by unfolding the star graph. The star-delta network is obtained through an unfolding scheme based on the recursive property of the star graph, and has n-1 switching stages. The star-delta network has the advantage of being controlled by the destination tag routing scheme. The indirect star-type networks are to the star graph as the indirect cube-type networks are to the n-cube. The authors analyze the performance of the indirect star-type networks under uniform traffic to investigate their potential as an alternative to the indirect cube-type networks for the future high-performance large multiprocessor systems.',117351,'IEEE Transactions on Computers',7),(109835,NULL,'1991',NULL,'A Variation on the Hypercube with Lower Diameter','A new interconnection structure is proposed as a basis for distributed-memory parallel computer architectures. The network is a variation of the hypercube and preserves many of its desirable properties, including regularity and large vertex connectivity. It has the same node and link complexity, but has a diameter only about half of the hypercube\'s. Some of the basic properties of this topology are discussed. Efficient routing and broadcasting algorithms are presented.',117351,'IEEE Transactions on Computers',7),(112175,NULL,'1991',NULL,'A VLSI Modulo m Multiplier','A novel method to compute the exact digits of the modulo m product of integers is proposed, and a modulo m multiply structure is defined. Such a structure can be implemented by means of a few fast VLSI binary multipliers, and a response time of about 150-200 ns to perform modular multiplications with moduli up to 32767 can be reached. A comparison to ROM-based structures is also provided. The modular multiplier has been evaluated asymptotically, according to the VLSI complexity theory, and it turned out to be an optimal design. This structure can be used to implement a residue multiplier in arithmetic structures using residue number systems (RNSs). The complexity of this residue multiplier has been evaluated and lower complexity figures than for ROM-based multiply structures have been obtained under several hypotheses on RNS parameters.',117351,'IEEE Transactions on Computers',8),(112217,NULL,'1991',NULL,'Pseudorandom Rounding for Truncated Multipliers','An economical, unbiased, overflow-free rounding scheme for multiplication of multiple-precision floating-point numbers is proposed. The scheme, called pseudorandom rounding, saves multiplications of lower bits and makes use of statistical properties of bits around the least significant bit of products in order to compensate for truncated parts. The method is deterministic, and inputs are commutable. The validity of the rounding is verified by numerical simulation.',117351,'IEEE Transactions on Computers',8),(112226,NULL,'1991',NULL,'Coping with Erroneous Information While Sorting','The authors study the problem of sorting n distinct elements in ascending sequence according to a total order, using comparison queries which receive \'yes\' or \'no\' answers, but of which as many as e may be erroneous. In a half-lie version, all \'yes\' answers are guaranteed to be correct and the errors are confined to \'no\' answers. It is shown that the comparison query complexity of the sorting problem for this case is Omega (n log n+e), and an asymptotically optimal algorithm is demonstrated. In a full-lie version, both \'yes\' and \'no\' answers can be false. It is shown that the comparison query complexity of the sorting problem for this case is Omega (n log n+en).',117351,'IEEE Transactions on Computers',9),(115508,NULL,'1992',NULL,'Finding approximate separators and computing tree width quickly','We show that for any fixed k, there is a linear-time algorithm which given a graph G either: (i) finds a cutset X of G with |X| ≤ k such that no component of G–X contains more than 3/4|G–X| vertices, or (ii) determines that for any set X of vertices of G with |X| ≤ k, there is a component of G–X which contains more than 2/3|G–X| vertices.This approximate separator algorithm can be used to develop and O(n log n algorithm for determining if G has a tree decomposition of width at most k (for fixed k) and finding such a tree decomposition if it exists.',225139,'STOC \'92 Proceedings of the twenty-fourth annual ACM symposium on Theory of computing',9),(116739,NULL,'1992',NULL,'Four State Asynchronous Architectures','An approach is presented to high-performance asynchronous architectures offering significant advantages over conventional clocked systems, without some of the drawbacks normally associated with asynchronous techniques. As the level of integration increases, an asynchronous wavefront array designed using the techniques described will have three important advantages over the equivalent synchronous systolic array: faster throughput (rate at which data are clocked through a system), reduced design complexity, and greater reliability. The benefits and drawbacks of using the asynchronous technique are highlighted using three wavefront arrays: two one-dimensional multipliers, and a two-dimensional sorter. All three can be built using just one basic building block.',117351,'IEEE Transactions on Computers',10),(116876,NULL,'1992',NULL,'Bottleneck Steiner Trees in the Plane','A Steiner tree with maximum-weight edge minimized is called a bottleneck Steiner tree (BST). The authors propose a Theta ( mod rho mod log mod rho mod ) time algorithm for constructing a BST on a point set rho , with points labeled as Steiner or demand; a lower bound, in the linear decision tree model, is also established. It is shown that if it is desired to minimize further the number of used Steiner points, then the problem becomes NP-complete. It is shown that when locations of Steiner points are not fixed the problem remains NP-complete; however, if the topology of the final tree is given, then the problem can be solved in Theta ( mod rho mod log mod rho mod ) time. The BST problem can be used, for example, in VLSI layout, communication network design, and (facility) location problems.',117351,'IEEE Transactions on Computers',9),(121716,NULL,'1992',NULL,'Basis Sets for Synthesis of Switching Functions','The synthesis of switching function f(x/sub 1/, x/sub 2/, . . ., x/sub n/) from a given family of functions g/sub i/(x/sub 1/, x/sub 2/, . . ., x/sub n/), 1or=ior=k, using a complete set of logic primitives is considered. Necessary and sufficient conditions for the synthesis of f from the g/sub i/\'s are derived using the concept of a basis set. The independence between the basis property and the completeness of a set of logic primitives is shown, the conditions for extending a set (g/sub 1/, g/sub 2/, . . ., g/sub j/), jn, to a basis set are found. Thus, the selection of a basis set and the logic primitives can be treated as separate problems. Finally, it is shown that there is a unique generalized Reed-Muller expansion for any f in terms of the basis functions (g/sub i/).',117351,'IEEE Transactions on Computers',1),(121856,NULL,'1992',NULL,'Configuring a Wafer-Scale Two-Dimensional Array of Single-Bit Processors','An overview of the ELSA (European large SIMD array) project, which uses a two-level strategy to achieve defect tolerance for wafer-scale architectures implemented in silicon, is presented. The target architecture is a 2-D array of processing elements for low-level image processing. An array is divided into subarrays called chips. At the chip level, defect tolerance is proved by an extra column of PEs (processing element) and bypassing techniques. At the wafer level, a double-rail connection network is used to construct a target array of defect-free chips that is as large and as fast as possible. Its main advantage is being independent of chip defects, as it is controlled from the I/O pads. An algorithm for constructing an optimized two-dimensional array on a wafer containing a given number of defect-free PEs and connections, a method to program the switches for the target architecture found by the algorithm, and software for programming the switches using laser cuts are discussed.',56371,'Computer - Special issue on wafer-scale integration',10),(121876,NULL,'1992',NULL,'Dynamic Scheduling of Hard Real-Time Tasks and Real-Time Threads','The authors investigate the dynamic scheduling of tasks with well-defined timing constraints. They present a dynamic uniprocessor scheduling algorithm with an O(n log n) worst-case complexity. The preemptive scheduling performed by the algorithm is shown to be of higher efficiency than that of other known algorithms. Furthermore, tasks may be related by precedence constraints, and they may have arbitrary deadlines and start times (which need not equal their arrival times). An experimental evaluation of the algorithm compares its average case behavior to the worst case. An analytic model used for explanation of the experimental results is validated with actual system measurements. The dynamic scheduling algorithm is the basis of a real-time multiprocessor operating system kernel developed in conjunction with this research. Specifically, this algorithm is used at the lowest, threads-based layer of the kernel whenever threads are created.',117420,'IEEE Transactions on Software Engineering',0),(121881,NULL,'1992',NULL,'P-Functions-Ternary Logic Functions Capable of Correcting Input Failures and Suitable for Treating Ambiguities','The authors consider a new class of special ternary logic functions: the P-functions capable of correcting input failures, based on the regular ternary logic. There are three major results. First, the P-functions are a special subset of the regular functions with no information loss. Second a ternary logic function defined by the all-prime-implicant disjoint of any Boolean connective is a P-function. Third, a P-function may be identified from the irredundant disjunctive form. The relationship between P-functions and other meaningful ternary logic functions is discussed.',117351,'IEEE Transactions on Computers',1),(124491,NULL,'1992',NULL,'On the approximation of maximum satisfiability','We present a 3/4 polynomial time approximation algorithm for the Maximum Satisfiability problem: Given a set of clauses, find a truth assignment that satisfies the maximum number of clauses. The algorithm applies to the weighted case as well, and involves nontrival application of network flow techniques.',218928,'SODA \'92 Proceedings of the third annual ACM-SIAM symposium on Discrete algorithms',9),(124522,NULL,'1992',NULL,'A faster deterministic maximum flow algorithm','We describe a deterministic version of a 1990 Cheriyan, Hagerup, and Mehlhorn randomized algorithm for computing the maximum flow on a directed graph with n nodes and m edges which runs in time O(mn + n2+&egr;, for any constant &egr;. This improves upon Alon\'s 1989 bound of O(mn + n8/3log n) [A] and gives an O(mn) deterministic algorithm for all m n1+&egr;. Thus it extends the range of m/n for which an O(mn) algorithm is known, and matches the 1988 algorithm of Goldberg and Tarjan [GT] for smaller values of m/n.',218928,'SODA \'92 Proceedings of the third annual ACM-SIAM symposium on Discrete algorithms',9),(124788,NULL,'1992',NULL,'Cache replacement with dynamic exclusion','Most recent cache designs use direct-mapped caches to provide the fast access time required by modern high speed CPU\'s. Unfortunately, direct-mapped caches have higher miss rates than set-associative caches, largely because direct-mapped caches are more sensitive to conflicts between items needed frequently in the same phase of program execution.This paper presents a new technique for reducing direct-mapped cache misses caused by conflicts for a particular cache line. A small finite state machine recognizes the common instruction reference patterns where storing an instruction in the cache actually harms performance. Such instructions are dynamically excluded, that is they are passed directly through the cache without being stored. This reduces misses to the instructions that would have been replaced.The effectiveness of dynamic exclusion is dependent on the severity of cache conflicts and thus on the particular program and cache size of interest. However, across the SPEC benchmarks, simulation results show an average reduction in miss rate of 33% for a 32KB instruction cache with 16B lines. In addition, applying dynamic exclusion to one level of a cache hierarchy can improve the performance of the next level since instructions do not need to be stored on both levels. Finally, dynamic exclusion also improves combined instruction and data cache miss rates.',131958,'ISCA \'92 Proceedings of the 19th annual international symposium on Computer architecture',4),(126420,NULL,'1992',NULL,'A Radix-4 Modular Multiplication Hardware Algorithm for Modular Exponentiation','A fast radix-4 modular multiplication hardware algorithm is proposed. It is efficient for modular exponentiation with a large modulus, used in public-key cryptosystems such as the RSA cryptosystem. The operands and the result of multiplication which are intermediate results in modular exponentiation are represented in a redundant representation. The computation proceeds in serial-parallel fashion. Each subtraction for the division for residue calculation is embedded in the repeated multiply-add. Each intermediate result is represented in a more redundant representation than that for the operands and the result, so that the number of the required addition/subtractions is reduced. All addition/subtraction are carried out without carry propagation. A serial-parallel modular multiplier based on the algorithm has a regular cellular array structure with a bit slice feature and is suitable for VLSI implementation.',117357,'IEEE Transactions on Computers - Special issue on computer arithmetic',8),(126421,NULL,'1992',NULL,'New Multipliers Modulo 2/sup N/-1','Techniques for computing the product of two N-bit integers modulo 2/sup N/-1 from their k-bit byte decompositions are presented. A modulus 2/sup N/-1 is chosen, as multiplication performed in this modulus can be reconstructed from the cyclic convolution between the sequences of the k-bit bytes of the decomposed numbers. It is shown that cyclic convolutions can be computed using only additions and squaring operations but not two-operand multiplications. Since the squaring operation is a one-operand operation, significant savings in ROM bits can be obtained if look-up tables are used.',117357,'IEEE Transactions on Computers - Special issue on computer arithmetic',8),(132251,NULL,'1992',NULL,'On the Complexity of Two Circle Strongly Connecting Problems','Given n demand points in the plane, the circle strongly connecting problem (CSCP) is to locate n circles in the plane, each with its center in a different demand point, and determine the radius of each circle such that the corresponding digraph G=(V, E), in which a vertex nu /sub 1/ in V stands for the point p/sub i/, and a directed edge ( nu /sub i/, nu /sub j/) in E, if and only if p/sub j/ located within the circle of p/sub i/, is strongly connected, and the sum of the radii of these n circles is minimal. The constrained circle strongly connecting problem is similar to the CSCP except that the points are given in the plane with a set of obstacles and a directed edge ( nu /sub i/, nu /sub j/) in E, if and only if p/sub j/ is located within the circle of p/sub i/ and no obstacles exist between them. It is proven that both these geometric problems are NP-hard. An O(n log n) approximation algorithm that can produce a solution no greater than twice an optimal one is also proposed.',117351,'IEEE Transactions on Computers',9),(132313,NULL,'1992',NULL,'Decomposition of Complex Multipliers Using Polynomial Encoding','A method for complex multiplication that relies on encoding 2n-bit complex numbers as polynomials of degree 7 in the ring of polynomials modulo x/sup 8/-1 with n/4-bit coefficients is introduced. Complex multiplication can then be performed with an 8-point cyclic convolution plus some conversion overhead and, with care, this can be done without introducing any errors. The technique is suitable for designs using systolic arrays.',117351,'IEEE Transactions on Computers',8),(142174,NULL,'1993',NULL,'Efficient flow-sensitive interprocedural computation of pointer-induced aliases and side effects','We present practical approximation methods for computing interprocedural aliases and side effects for a program written in a language that includes pointers, reference parameters and recursion. We present the following results: 1) An algorithm for flow-sensitive interprocedural alias analysis which is more precise and efficient than the best interprocedural method known. 2) An extension of traditional flow-insensitive alias analysis which accommodates pointers and provides a framework for a family of algorithms which trade off precision for efficiency. 3) An algorithm which correctly computes side effects in the presence of pointers. Pointers cannot be correctly handled by conventional methods for side effect analysis. 4) An alias naming technique which handles dynamically allocated objects and guarantees the correctness of data-flow analysis. 5) A compact representation based on transitive reduction which does not result in a loss of precision and improves precision in some case. 6) A method for intraprocedural alias analysis which is based on a sparse representation.',185531,'POPL \'93 Proceedings of the 20th ACM SIGPLAN-SIGACT symposium on Principles of programming languages',2),(144424,NULL,'1993',NULL,'Shortest paths among obstacles in the plane','We give a subquadratic (O(n5/3+&egr;) time and space) algorithm for computing Euclidean shortest paths in the plane in the presence of polygonal obstacles; previous time bounds were at least quadratic in n, in the worst-case. The method avoids use of visibility graphs, relying instead on the continuous Dijkstra paradigm. The output is a shortest path map (of size O(n)) with respect to a given source point, which allows shortest path length queries to be answered in time O(log n). The algorithm extends to the case of multiple source points, yielding a geodesic Voronoi diagram within the same time bound.',211177,'SCG \'93 Proceedings of the ninth annual symposium on Computational geometry',9),(145492,NULL,'1993',NULL,'Parametric Module Allocation on Partial k-Trees','The problem of allocating modules to processors in a distributed system to minimize total costs when the underlying communication graph is a partial k-tree and all costs are linear functions of a real parameter t is considered. It is shown that if the number of processors is fixed, the sequence of optimum assignments that are obtained as t varies from zero to infinity can be constructed in polynomial time. As an auxiliary result, a linear time separator algorithm for k-trees is developed. The implications of the results for parametric versions of the weighted vertex cover, independent set, and 0-1 quadratic programming problems on partial k-trees are discussed.',117351,'IEEE Transactions on Computers',9),(160106,NULL,'1994',NULL,'Value dependence graphs: representation without taxation','The value dependence graph (VDG) is a sparse dataflow-like representation that simplifies program analysis and transformation. It is a functional representation that represents control flow as data flow and makes explicit all machine quantities, such as stores and I/O channels. We are developing a compiler that builds a VDG representing a program, analyzes and transforms the VDG, then produces a control flow graph (CFG) [ASU86] from the optimized VDG. This framework simplifies transformations and improves upon several published results. For example, it enables more powerful code motion than [CLZ86, FOW87], eliminates as many redundancies as [AWZ88, RWZ88] (except for redundant loops), and provides important information to the code scheduler [BR91]. We exhibit a fast, one-pass method for elimination of partial redundancies that never performs redundant code motion [KFS92, DS93] and is simpler than the classical [MR79, Dha91] or SSA [RWZ88] methods. These results accrue from eliminating the CFG from the analysis/transformation phases and using demand dependences in preference to control dependences.',185532,'POPL \'94 Proceedings of the 21st ACM SIGPLAN-SIGACT symposium on Principles of programming languages',2),(171446,NULL,'1994',NULL,'MACAW: a media access protocol for wireless LAN\'s','In recent years, a wide variety of mobile computing devices has emerged, including portables, palmtops, and personal digital assistants. Providing adequate network connectivity for these devices will require a new generation of wireless LAN technology. In this paper we study media access protocols for a single channel wireless LAN being developed at Xerox Corporation\'s Palo Alto Research Center. We start with the MACA media access protocol first proposed by Karn [9] and later refined by Biba [3] which uses an RTS-CTS-DATA packet exchange and binary exponential back-off. Using packet-level simulations, we examine various performance and design issues in such protocols. Our analysis leads to a new protocol, MACAW, which uses an RTS-CTS-DS-DATA-ACK message exchange and includes a significantly different backoff algorithm.',216045,'SIGCOMM \'94 Proceedings of the conference on Communications architectures, protocols and applications',3),(172445,NULL,'1994',NULL,'Random pattern testable logic synthesis','Previous procedures for synthesis of testable logic guarantee that all faults in the synthesized circuits are detectable. However, the detectability of many faults in these circuits can be very low leading to poor random pattern testability. A new procedure to perform logic synthesis that synthesizes random pattern testable multilevel circuits is proposed. Experimental results show that the circuits synthesized by the proposed procedure tstfx are significantly more random pattern testable and smaller than those synthesized using its counterpart fast_extract (fx) in SIS. The proposed synthesis procedure design circuits that require only simple random pattern generators in built-in self-test, thereby obviating the need for complex BIST circuitry.',115251,'ICCAD \'94 Proceedings of the 1994 IEEE/ACM international conference on Computer-aided design',11),(183373,NULL,'1995',NULL,'Efficient Geometric Algorithms on the EREW PRAM','We present a technique that can be used to obtain efficient parallel geometric algorithms in the EREW PRAM computational model. This technique enables us to solve optimally a number of geometric problems in O(log n) time using O(n/log n) EREW PRAM processors, where n is the input size of a problem. These problems include: computing the convex hull of a set of points in the plane that are given sorted, computing the convex hull of a simple polygon, computing the common intersection of half-planes whose slopes are given sorted, finding the kernel of a simple polygon, triangulating a set of points in the plane that are given sorted, triangulating monotone polygons and star-shaped polygons, and computing the all dominating neighbors of a sequence of values. PRAM algorithms for these problems were previously known to be optimal (i.e., in O(log n) time and using O(n/log n) processors) only on the CREW PRAM, which is a stronger model than the EREW PRAM',117407,'IEEE Transactions on Parallel and Distributed Systems',9),(183383,NULL,'1995',NULL,'The Deferrable Server Algorithm for Enhanced Aperiodic Responsiveness in Hard Real-Time Environments','Most existing scheduling algorithms for hard real-time systems apply either to periodic tasks or aperiodic tasks but not to both. In practice, real-time systems require an integrated, consistent approach to scheduling that is able to simultaneously meet the timing requirements of hard deadline periodic tasks, hard deadline aperiodic (alert-class) tasks, and soft deadline aperiodic tasks. This paper introduces the Deferrable Server (DS) algorithm which will be shown to provide improved aperiodic response time performance over traditional background and polling approaches. Taking advantage of the fact that, typically, there is no benefit in early completion of the periodic tasks, the Deferrable Server (DS) algorithm assigns higher priority to the aperiodic tasks up until the point where the periodic tasks would start to miss their deadlines. Guaranteed alert-class aperiodic service and greatly reduced response times for soft deadline aperiodic tasks are important features of the DS algorithm, and both are obtained with the hard deadlines of the periodic tasks still being guaranteed. The results of a simulation study performed to evaluate the response time performance of the new algorithm against traditional background and polling approaches are presented. In all cases, the response times of aperiodic tasks are significantly reduced (often by an order of magnitude) while still maintaining guaranteed periodic task deadlines',117351,'IEEE Transactions on Computers',0),(184952,NULL,'1995',NULL,'Allocation and Scheduling of Precedence-Related Periodic Tasks','This paper discusses a static algorithm for allocating and scheduling components of periodic tasks across sites in distributed systems. Besides dealing with the periodicity constraints, (which have been the sole concern of many previous algorithms), this algorithm handles precedence, communication, as well as replication requirements of subtasks of the tasks. The algorithm determines the allocation of subtasks of periodic tasks to sites, the scheduled start times of subtasks allocated to a site, and the schedule for communication along the communication channel(s). Simulation results show that the heuristics and search techniques incorporated in the algorithm are very effective.Index Terms驴Real-time systems, task allocation, scheduling, periodic tasks, precedence constraints, distributed systems.',117407,'IEEE Transactions on Parallel and Distributed Systems',0),(196090,NULL,'1995',NULL,'Log-based receiver-reliable multicast for distributed interactive simulation','Reliable multicast communication is important in large-scale distributed applications. For example, reliable multicast is used to transmit terrain and environmental updates in distributed simulations. To date, proposed protocols have not supported these applications\' requirements, which include wide-area data distribution, low-latency packet loss detection and recovery, and minimal data and management over-head within fine-grained multicast groups, each containing a single data source.In this paper, we introduce the notion of Log-Based Receiver-reliable Multicast (LBRM) communication, and we describe and evaluate a collection of log-based receiver reliable multicast optimizations that provide an efficient, scalable protocol for high-performance simulation applications. We argue that these techniques provide value to a broader range of applications and that the receiver-reliable model is an appropriate one for communication in general.',216046,'SIGCOMM \'95 Proceedings of the conference on Applications, technologies, architectures, and protocols for computer communication',3),(202499,NULL,'1995',NULL,'Instruction cache fetch policies for speculative execution','Current trends in processor design are pointing to deeper and wider pipelines and superscalar architectures. The efficient use of these resources requires speculative execution, a technique whereby the processor continues executing the predicted path of a branch before the branch condition is resolved.In this paper, we investigate the implications of speculative execution on instruction cache performance. We explore policies for managing instruction cache misses ranging from aggressive policies (always fetch on the speculative path) to conservative ones (wait until branches are resolved). We test these policies and their interaction with next-line prefetching by simulating the effects on instruction caches with varying architectural parameters. Our results suggest that an aggressive policy combined with next-line prefetching is best for small latencies while more conservative policies are preferable for large latencies.',131962,'ISCA \'95 Proceedings of the 22nd annual international symposium on Computer architecture',4),(202500,NULL,'1995',NULL,'Streamlining data cache access with fast address calculation','For many programs, especially integer codes, untolerated load instruction latencies account for a significant portion of total execution time. In this paper, we present the design and evaluation of a fast address generation mechanism capable of eliminating the delays caused by effective address calculation for many loads and stores.Our approach works by predicting early in the pipeline (part of) the effective address of a memory access and using this predicted address to speculatively access the data cache. If the prediction is correct, the cache access is overlapped with non-speculative effective address calculation. Otherwise, the cache is accessed again in the following cycle, this time using the correct effective address. The impact on the cache access critical path is minimal; the prediction circuitry adds only a single OR operation before cache access can commence. In addition, verification of the predicted effective address is completely decoupled from the cache access critical path.Analyses of program reference behavior and subsequent performance analysis of this approach shows that this design is a good one, servicing enough accesses early enough to result in speedups for all the programs we tested. Our approach also responds well to software support, which can significantly reduce the number of mispredicted effective addresses, in many cases providing better program speedups and reducing cache bandwidth requirements.',131962,'ISCA \'95 Proceedings of the 22nd annual international symposium on Computer architecture',4),(206422,NULL,'1996',NULL,'ARB: A Hardware Mechanism for Dynamic Reordering of Memory References','To exploit instruction level parallelism, it is important not only to execute multiple memory references per cycle, but also to reorder memory references(especially to execute loads before stores that precede them in the sequential instruction stream. To guarantee correctness of execution in such situations, memory reference addresses have to be disambiguated. This paper presents a novel hardware mechanism, called an Address Resolution Buffer (ARB), for performing dynamic reordering of memory references. The ARB supports the following features: 1) dynamic memory disambiguation in a decentralized manner, 2) multiple memory references per cycle, 3) out-of-order execution of memory references, 4) unresolved loads and stores, 5) speculative loads and stores, and 6) memory renaming. The paper presents the results of a simulation study that we conducted to verify the efficacy of the ARB for a superscalar processor. The paper also shows the ARB\'s application in a multiscalar processor.',117351,'IEEE Transactions on Computers',4),(206423,NULL,'1996',NULL,'Execution Time Analysis of Communicating Tasks in Distributed Systems','Task-execution times are one of the most important parameters in scheduling tasks. Most scheduling algorithms are based on the assumption that either worst-case task-execution times are known to the scheduler or no information on execution times is available at all. While scheduling tasks based on worst-case execution times can guarantee to meet their timing requirements, it may lead to severe under-utilization of CPUs because worst-case execution times could be one or two orders of magnitude larger than the corresponding actual values. Scheduling tasks based on the execution time distribution (instead of worst-case execution times) is known to improve system utilization significantly.In this paper, we propose a model to predict task execution times in a distributed system. The model considers several factors which affect the execution time of each task. These factors are classified into two groups: intrinsic and extrinsic. The intrinsic factors control the flow within a task, while the extrinsic factors include communication and synchronization delays between tasks. By simplifying the extrinsic factors, we represent a distributed system with a simple queuing model. The proposed queuing model consists of two stations: one for computation and the other for communication and synchronization. Information on system utilization can be obtained by converting this queuing model to a Markov chain. The execution time of a task is then derived from the information on system utilization in the form of average and distribution. The model is extended to describe the effects of multiple tasks assigned to a single processing node. The utility of the model is demonstrated with an example.',117351,'IEEE Transactions on Computers',0),(207058,NULL,'1996',NULL,'Folded Petersen Cube Networks: New Competitors for the Hypercubes','We introduce and analyze a new interconnection topology, called the k-dimensional folded Petersen (FPk) network, which is constructed by iteratively applying the Cartesian product operation on the well-known Petersen graph.Since the number of nodes in FPk is restricted to a power of ten, for better scalability we propose a generalization, the folded Petersen cube network FPQn,k = Qn脳FPk, which is a product of the n-dimensional binary hypercube (Qn) and FPk. The FPQn,k topology provides regularity, node- and edge-symmetry, optimal connectivity (and therefore maximal fault-tolerance), logarithmic diameter, modularity, and permits simple self-routing and broadcasting algorithms. With the same node-degree and connectivity, FPQn,k has smaller diameter and accommodates more nodes than Qn+3k, and its packing density is higher compared to several other product networks.This paper also emphasizes the versatility of the folded Petersen cube networks as a multicomputer interconnection topology by providing embeddings of many computationally important structures such as rings, multi-dimensional meshes, hypercubes, complete binary trees, tree machines, meshes of trees, and pyramids. The dilation and edge-congestion of all such embeddings are at most two.',117407,'IEEE Transactions on Parallel and Distributed Systems',7),(208831,NULL,'1996',NULL,'Test Generation with Dynamic Probe Points in High Observability Testing Environment','High observability testing environment allows internal circuit nodes to be used as test points. However, such flexibility requires the development of new ATPG algorithm. Previous reported algorithm does not guarantee full fault-coverage and assumes all internal circuit nodes are test points. The new algorithm described in this paper will generate a full fault-coverage test set for a fanout free combinational circuit. The main characteristic of the algorithm is that it generates test vectors as well as probe points. As a result, the probe points are different for each test vector, and the number of probe points is the minimum for test set generated. Results obtained show that an average of 30% test vector reduction is achieved compared with the conventional test method which uses only output pins as test points',117351,'IEEE Transactions on Computers',11),(209610,NULL,'1996',NULL,'Adaptive Fault-Tolerant Deadlock-Free Routing in Meshes and Hypercubes','We present an adaptive deadlock-free routing algorithm which decomposes a given network into two virtual interconnection networks, VIN1 and VIN2. VIN1 supports deterministic deadlock-free routing, and VIN2 supports fully-adaptive routing. Whenever a channel in VIN1 or VIN2 is available, it can be used to route a message.Each node is identified to be in one of three states: safe, unsafe, and faulty. The unsafe state is used for deadlock-free routing, and an unsafe node can still send and receive messages. When nodes become faulty/unsafe, some channels in VIN2 around the faulty/unsafe nodes are used as the detours of those channels in VIN1 passing through the faulty/unsafe nodes, i.e., the adaptability in VIN2 is transformed to support fault-tolerant deadlock-free routing. Using information on the state of each node\'s neighbors, we have developed an adaptive fault-tolerant deadlock-free routing scheme for n-dimensional meshes and hypercubes with only two virtual channels per physical link.In an n-dimensional hypercube, any pattern of faulty nodes can be tolerated as long as the number of faulty nodes is no more than $\\lceil\\, n/2 \\,\\rceil$. The maximum number of faulty nodes that can be tolerated is 2n驴1, which occurs when all faulty nodes can be encompassed in an (n驴 1)-cube. In an n-dimensional mesh, we use a more general fault model, called a disconnected rectangular block. Any arbitrary pattern of faulty nodes can be modeled as a rectangular block after finding both unsafe and disabled nodes (which are then treated as faulty nodes). This concept can also be applied to k-ary n-cubes with four virtual channels, two in VIN1 and the other two in VIN2. Finally, we present simulation results for both hypercubes and 2-dimensional meshes by using various workloads and fault patterns.',117351,'IEEE Transactions on Computers',7),(212162,NULL,'1996',NULL,'Carry-Save Multiplication Schemes Without Final Addition','Carry-save multipliers require an adder at the last step to convert the carry-sum representation of the most significant half of the result into a non-redundant form. This paper presents n脳n multiplication schemes where this conversion is performed with a circuit operating in parallel with the carry-save array. The most relevant feature of the proposed multipliers is that the full 2n-bit result is produced, unlike similar multiplication schemes presented in the literature.',117351,'IEEE Transactions on Computers',8),(212551,NULL,'1996',NULL,'A Methodology for the Rapid Injection of Transient Hardware Errors','Ultra-dependable computing demands verification of fault-tolerant mechanisms in the hardware. The most popular class of verification methodologies, fault-injection, is fraught with a host of limitations. Methods which are rapid enough to be feasible are not based on actual hardware faults. On the other hand, methods which are based on gate-level faults require enormous time resources. This research tries to bridge that gap by developing a new fault-injection methodology for processors based on a register-transfer-language (RTL) fault model. The fault model is developed by abstracting the effects of low-level faults to the RTL level. This process attempts to be independent of implementation details without sacrificing coverage, the proportion of errors generated by gate-level faults that are successfully reproduced by the RTL fault model. A prototype tool, ASPHALT, is described which automates the process of generating the error patterns. The IBM RISC-Oriented Micro-Processor (ROMP) is used as a basis for experimentation. Over 1.5 million transient faults are injected using a gate-level model. Over 97% of these are reproduced with the RTL model at a speedup factor of over 500:1. These results show that the RTL fault model may be used to greatly accelerate fault-injection experiments without sacrificing accuracy.',117351,'IEEE Transactions on Computers',6),(212567,NULL,'1996',NULL,'System Dependability Evaluation via a Fault List Generation Algorithm','The size and complexity of modern dependable computing systems has significantly compromised the ability to accurately measure system dependability attributes such as fault coverage and fault latency. Fault injection is one approach for the evaluation of dependability metrics. Unfortunately, fault injection techniques are difficult to apply because the size of the fault set is essentially infinite. Current techniques select faults randomly resulting in many fault injection experiments which do not yield any useful information. This research effort has developed a new deterministic, automated dependability evaluation technique using fault injection. The primary objective of this research effort was the development and implementation of algorithms which generate a fault set which fully exercises the fault detection and fault processing aspects of the system. The theory supporting the developed algorithms is presented first. Next, a conceptual overview of the developed algorithms is followed by the implementation details of the algorithms. The last section of this paper presents experimental results gathered via simulation-based fault injection of an Interlocking Control System (ICS). The end result is a deterministic, automated method for accurately evaluating complex dependable computing systems using fault injection.',117351,'IEEE Transactions on Computers',6),(217348,NULL,'1996',NULL,'A Broadcast Algorithm for All-Port Wormhole-Routed Torus Networks','A new approach to broadcast in wormhole-routed two- and three-dimensional torus networks is proposed. The underlying network is assumed to support only deterministic, dimension-ordered unicast routing. The approach extends the graph theoretical concept of dominating nodes by accounting for the relative distance-insensitivity of the wormhole routing switching strategy. The proposed algorithm also takes advantage of an all-port communication architecture, which allows each node to simultaneously transmit messages on different outgoing channels. The resulting broadcast operation is based on a tree structure that uses multiple levels of extended dominating nodes (EDNs). Performance results are presented that confirm the advantage of this method over other approaches.',117407,'IEEE Transactions on Parallel and Distributed Systems',7),(217951,NULL,'1996',NULL,'QoS provisioning in micro-cellular networks supporting multiple classes of traffic','We introduce an adaptive call admission control mechanism for wireless/mobile networks supporting multiple classes of traffic, and discuss a number of resource sharing schemes which can be used to allocate wireless bandwidth to different classes of traffic. The adaptive call admission control reacts to changing new call arrival rates, and the resource sharing mechanism reacts to rapidly changing traffic conditions in every radio cell due to mobility of mobile users. In addition, we have provided an analytical methodology which shows that the combination of the call admission control and the resource sharing schemes guarantees a predefined quality-of-service to each class of traffic. One major advantage of our approach is that it can be performed in a distributed fashion removing any bottlenecks that might arise due to frequent invocation of network call control functions.',266376,'Wireless Networks - Special issue on wireless multimedia networking',3),(218540,NULL,'1996',NULL,'Eliminating operand read latency','Programs generally exhibit load or memory operand read latencies that account for a significant portion of pipeline interlocks or stalls. In this paper we present an approach for the prediction of operand read data during the instruction fetch stage of a pipelined processor. For the X86 programs studied many have a significant percentage of such operand data that can be predicted with a high accuracy.',15762,'ACM SIGARCH Computer Architecture News',4),(218982,NULL,'1996',NULL,'On-Line Planarity Testing','The on-line planarity-testing problem consists of performing the following operations on a planar graph $G$: (i) testing if a new edge can be added to $G$ so that the resulting graph is itself planar; (ii) adding vertices and edges such that planarity is preserved. An efficient technique for on-line planarity testing of a graph is presented that uses $O(n)$ space and supports tests and insertions of vertices and edges in $O(\\log n)$ time, where $n$ is the current number of vertices of $G$. The bounds for tests and vertex insertions are worst-case and the bound for edge insertions is amortized. We also present other applications of this technique to dynamic algorithms for planar graphs.',215936,'SIAM Journal on Computing',9),(219615,NULL,'1996',NULL,'Tango: a hardware-based data prefetching technique for superscalar processors','We present a new hardware-based data prefetching mechanism for enhancing instruction level parallelism and improving the performance of superscalar processors. The emphasis in our scheme is on the effective utilization of slack time and hardware resources not used for the main computation. The scheme suggests a new hardware construct, the program progress graph (PPG), as a simple extension to the branch target buffer (BTB). We use the PPG for implementing a fast pre-program counter pre-PC, that travels only through memory reference instructions (rather than scanning all the instructions sequentially). In a single clock cycle the pre-PC extracts all the predicted memory references in some future block of instructions, to obtain early data prefetching. In addition, the PPG can be used for implementing a pre-processor and for instruction prefetching. The prefetch requests are scheduled to \"range\" with the core requests from the data cache, by using only free time slots on the existing data cache tag ports. Employing special methods for removing prefetch requests that are already in the cache (without utilizing the cache-tag ports bandwidth) and a simple optimization on the cache LRU mechanism reduce the number of prefetch requests sent to the core-cache bus and to the memory (second level) bus. Simulation results on the SPEC92 benchmark for the base line architecture (32 K-byte data cache and 12 cycles fetch latency) show an average speedup of 1.36 (CPI ratio).',191974,'Proceedings of the 29th annual ACM/IEEE international symposium on Microarchitecture',4),(220176,NULL,'1997',NULL,'Simulation-based techniques for dynamic test sequence compaction','Simulation-based techniques for dynamic compaction of test sequences are proposed. The first technique uses a fault simulator to remove test vectors from the partially-specified test sequence generated by a deterministic test generator if the vectors are not needed to detect the target fault, considering that the circuit state may be known. The second technique uses genetic algorithms to fill the unspecified bits in the partially-specified test sequence in order to increase the number of faults detected by the sequence. Significant reductions in test set sizes were observed for all benchmark circuits studied. Fault coverages improved for many of the circuits, and execution times often dropped as well, since fewer faults had to be targeted by the computation-intensive deterministic test generator.',190580,'Proceedings of the 1996 IEEE/ACM international conference on Computer-aided design',11),(220230,NULL,'1996',NULL,'A Necessary and Sufficient Criterion for the Monotonicity of Boolean Functions with Deterministic and Stochastic Applications','It is shown how the property of being monotonous of any n-variables combinational switching function can be checked by evaluating n possibly ternary functions of binary variables in a simple way. Those functions are then shown to play a key role in the sensitivity analysis of stochastic Boolean functions.',117351,'IEEE Transactions on Computers',1),(221178,NULL,'1996',NULL,'Hardware Starting Approximation Method and Its Application to the Square Root Operation','Quadratically converging algorithms for high-order arithmetic operations typically are accelerated by a starting approximation. The higher the precision of the starting approximation, the less number of iterations required for convergence. Traditional methods have used look-up tables or polynomial approximations, or a combination of the two called piecewise linear approximations. This paper provides a revision and major extension to our study [1] proposing a nontraditional method for reusing the hardware of a multiplier. An approximation is described in the form of partial product array (PPA) composed of Boolean elements. The Boolean elements are chosen such that their sum is a high-precision approximation to a high-order arithmetic operation such as square root, reciprocal, division, logarithm, exponential, and trigonometric functions. This paper derives a PPA that produces in the worst case a 16-bit approximation to the square root operation. The implementation of the PPA utilizes an existing 53 bit multiplier design requiring approximately 1,000 dedicated logic gates of function, additional repowering circuits, and has a latency of one multiplication.',117351,'IEEE Transactions on Computers',8),(224603,NULL,'1997',NULL,'Graceful Degradation in Algorithm-Based Fault Tolerant Multiprocessor Systems','Algorithm-based fault tolerance (ABFT) is a technique which improves the reliability of a multiprocessor system by providing concurrent error detection and fault location capability to it. It encodes data at the system level and modifies the algorithm to operate on the encoded data in order to expose both transient and permanent faults in any processor. Work done till now in this area takes care of only the fault detection and location part of the problem. However, if spare processors are not available, then after a faulty processor has been located, the work initially assigned to it has to be mapped to some nonfaulty processors in the system in such a way that the fault tolerance capability of the system is still maintained with as small a degradation in performance as possible. In this paper, we propose an integrated deterministic solution to the above problem which combines concurrent error detection and fault location with graceful degradation. There exists no previous deterministic ABFT method for the design of general t-fault locating systems, even for the case of t = 1. We propose a general method for designing one-fault locating/s-fault detecting systems. We use an extended model for representing ABFT systems. This model considers the processors computing the checks to be a part of the ABFT system, so that faults in the check_computing processors can also be detected and located using a simple diagnosis algorithm, and the checks can be mapped to other nonfaulty processors in the system.',117407,'IEEE Transactions on Parallel and Distributed Systems',6),(224702,NULL,'1997',NULL,'Radix 2 Division with Over-Redundant Quotient Selection','In this paper we present a new radix 2 division algorithm that uses a recurrence employing simple 3-to-2 digit carry-free adders to perform carry-free addition/subtraction for computing the partial remainders in radix 2 signed-digit form. The quotient digit, during any iteration of the division recursion, is generated from the two most-significant radix 2 digits of the partial remainder and independent of the divisor in over-redundant radix 2 digit form (i.e., with digits which belong to the digit set {驴2, 驴1, 0, +1, +2}). The over-redundant quotient digits are then converted to the conventional radix 2 digits (belonging to the set {驴1, 0, +1}) by using a reduction technique. This division algorithm is well suited for IEEE 754 standard operands belonging to the range [1, 2) and is slightly faster than previously proposed radix 2 designs (such as the radix 2 SRT), which do not employ input scaling, since the quotient selection for such algorithms is a function of more than two most-significant radix 2 digits of the partial remainder. In comparison with the designs that employ input scaling, the proposed design although slightly slower saves hardware required for scaling purposes.',117351,'IEEE Transactions on Computers',8),(224847,NULL,'1997',NULL,'Boolean Functions Classification via Fixed Polarity Reed-Muller Forms','In this paper, we present a new method to characterize completely specified Boolean functions. The central theme of the classification is the functional equivalence (a.k.a. Boolean matching). Two Boolean functions are equivalent if there exists input permutation, input negation, or output negation that can transform one function to the other. We have derived a method that can efficiently identify equivalence classes of Boolean functions. The well-known canonical Fixed Polarity Reed-Muller (FPRM) forms are used as a powerful analysis tool. The necessary transformations to derive one function from the other are inherent in the FPRM representations. To identify uniquely each equivalence class, a set of well-known characteristics of Boolean functions and their variables (including linearity, symmetry, total symmetry, self-complement, and self-duality) are employed. It is shown that all the equivalence classes of four-variable functions [10] are uniquely identified where majority of the classes have a single FPRM form as their representative. The Boolean matching has applications in technology mapping and in design of standard cell libraries.',117351,'IEEE Transactions on Computers',1),(224886,NULL,'1997',NULL,'Topologies of Combined (2logN - 1)-Stage Interconnection Networks','A combined (2logN 驴 1)-stage interconnection network (denoted by $\\Delta \\oplus \\Delta \'$) is constructed by concatenating two Omega-equivalent networks (驴 and 驴驴) with the rightmost stage of 驴 and the leftmost stage of 驴驴 overlapped. Benes network and the (2logN 驴 1)-stage shuffle-exchange network are two examples of such networks. Although these two networks have received intensive studies, the research on the topology of entire class of $\\Delta \\oplus \\Delta \'$ networks is very limited so far. In this paper, we study the topological structure of $\\Delta \\oplus \\Delta \'$ networks and propose an algorithm for determining topological equivalence between two given $\\Delta \\oplus \\Delta \'$ networks. We also present a simplified 驴-equivalence checking algorithm as a supporting result.',117351,'IEEE Transactions on Computers',7),(227884,NULL,'1997',NULL,'Balancing push and pull for data broadcast','The increasing ability to interconnect computers through internet-working, wireless networks, high-bandwidth satellite, and cable networks has spawned a new class of information-centered applications based on data dissemination. These applications employ broadcast to deliver data to very large client populations. We have proposed the Broadcast Disks paradigm [Zdon94, Acha95b] for organizing the contents of a data broadcast program and for managing client resources in response to such a program. Our previous work on Broadcast Disks focused exclusively on the “push-based” approach, where data is sent out on the broadcast channel according to a periodic schedule, in anticipation of client requests. In this paper, we study how to augment the push-only model with a “pull-based” approach of using a backchannel to allow clients to send explicit requests for data to the server. We analyze the scalability and performance of a broadcast-based system that integrates push and pull and study the impact of this integration on both the steady state and warm-up performance of clients. Our results show that a client backchannel can provide significant performance improvement in the broadcast environment, but that unconstrained use of the backchannel can result in scalability problems due to server saturation. We propose and investigate a set of three techniques that can delay the onset of saturation and thus, enhance the performance and scalability of the system.',216402,'SIGMOD \'97 Proceedings of the 1997 ACM SIGMOD international conference on Management of data',3),(235665,NULL,'1997',NULL,'Analysis and Randomized Design of Algorithm-Based Fault Tolerant Multiprocessor Systems Under an Extended Model','Reliability of compute-intensive applications can be improved by introducing fault tolerance into the system. Algorithm-based fault tolerance (ABFT) is a low-cost scheme which provides the required fault tolerance to the system through system level encoding. In this paper, we propose randomized construction techniques, under an extended model, for the design of ABFT systems with the required fault tolerance capability. The model considers failures in the processors performing the checking operations.',117407,'IEEE Transactions on Parallel and Distributed Systems',6),(236195,NULL,'1997',NULL,'Compile-Time Scheduling of Dynamic Constructs in Dataflow Program Graphs','Scheduling dataflow graphs onto processors consists of assigning actors to processors, ordering their execution within the processors, and specifying their firing time. While all scheduling decisions can be made at runtime, the overhead is excessive for most real systems. To reduce this overhead, compile-time decisions can be made for assigning and/or ordering actors on processors. Compile-time decisions are based on known profiles available for each actor at compile time. The profile of an actor is the information necessary for scheduling, such as the execution time and the communication patterns. However, a dynamic construct within a macro actor, such as a conditional and a data-dependent iteration, makes the profile of the actor unpredictable at compile time. For those constructs, we propose to assume some profile at compile-time and define a cost to be minimized when deciding on the profile under the assumption that the runtime statistics are available at compile-time. Our decisions on the profiles of dynamic constructs are shown to be optimal under some bold assumptions, and expected to be near-optimal in most cases. The proposed scheduling technique has been implemented as one of the rapid prototyping facilities in Ptolemy. This paper presents the preliminary results on the performance with synthetic examples.',117351,'IEEE Transactions on Computers',0),(244141,NULL,'1997',NULL,'Cellular Automata for Weighted Random Pattern Generation','Fault testing random-pattern-resistant circuits requires that BIST (built-in self-test) techniques generate large numbers of pseudorandom patterns. To shorten these long test lengths, this study describes a cellular automata-based method that efficiently generates weighted pseudorandom BIST patterns. This structure, called a weighted cellular automaton (WCA), uses no external weighting logic. The design algorithm MWCARGO combines generation of the necessary weight sets and design of the WCA. In this study, WCA pattern generators designed by MWCARGO achieved 100 percent coverage of testable stuck-at faults for benchmark circuits with random-pattern-resistant faults. The WCA applies complete tests much faster than existing test-per-scan techniques. At the same time, the hardware overhead of WCA proves to be competitive with that of current test-per-clock schemes.',117351,'IEEE Transactions on Computers',11),(247950,NULL,'1998',NULL,'A Novel Approach to Random Pattern Testing of Sequential Circuits','Random pattern testing methods are known to result in poor fault coverage for most sequential circuits unless costly circuit modifications are made. In this paper, we propose a novel approach to improve the random pattern testability of sequential circuits. We introduce the concept of holding signals at primary inputs and scan flipflops of a partially scanned sequential circuit for a certain length of time, instead of applying a new random vector at each clock cycle. When a random vector is held at the primary inputs of the circuit under test or at the scan flip-flops, the system clock is applied and the primary outputs of the circuit are observed. Information obtained from a testability analysis or test generator is used to determine the number of clock cycles for which each random vector is to be held constant. The method is low cost and the results of our experiment on the benchmark circuits show that it is very effective in providing fault coverage close to the maximum obtainable fault coverage using random patterns with full scan',117351,'IEEE Transactions on Computers',11),(248339,NULL,'1998',NULL,'A Reliable Fail-Safe System','This paper describes a fault-tolerant system that is based on two replicas of a self-checking module and on an error-masking interface. The main contributions of this work rely on the fail-safe/strongly-fail-safe design of the error-masking interface, and on the analysis of the competitiveness of this fault-tolerant scheme with respect to its reliability.',117351,'IEEE Transactions on Computers',6),(248343,NULL,'1998',NULL,'ATPG for Heat Dissipation Minimization During Test Application','A new automatic test pattern generator (ATPG) algorithm is proposed that reduces switching activity (between successive test vectors) during test application. The main objective is to permit safe and inexpensive testing of low power circuits and bare die that might otherwise require expensive heat removal equipment for testing at high speeds. Three new cost functions, namely, transition controllability, observability, and test generation costs, have been defined. It has been shown, for a fanout free circuit under test, that the transition test generation cost for a fault is the minimum number of transitions required to test a given stuck-at fault. The proposed algorithm has been implemented and the generated tests are compared with those generated by a standard PODEM implementation for the larger ISCAS85 benchmark circuits. The results clearly demonstrate that the tests generated using the proposed ATPG can decrease the average number of (weighted) transitions between successive test vectors by a factor of 2 to 23.',117351,'IEEE Transactions on Computers',11),(248344,NULL,'1998',NULL,'A Note on the Complexity of Dijkstra\'s Algorithm for Graphs with Weighted Vertices','Let ${\\cal G}(V,\\ E)$ be a directed graph in which each vertex has a nonnegative weight. The cost of a path between two vertices in $\\cal G$ is the sum of the weights of the vertices on that path. In this paper, we show that, for such graphs, the time complexity of Dijkstra\'s algorithm, implemented with a binary heap, is ${\\cal O}(|E| + |V|\\ \\log\\ |V|).$',117351,'IEEE Transactions on Computers',9),(248893,NULL,'1995',NULL,'Multicluster, mobile, multimedia radio network','A multi-cluster, multi-hop packet radio network architecture for wireless adaptive mobile information systems is presented. The proposed network supports multimedia traffic and relies on both time division and code division access schemes. This radio network is not supported by a wired infrastructure as conventional cellular systems are. Thus, it can be instantly deployed in areas with no infrastructure at all. By using a distributed clustering algorithm, nodes are organized into clusters. The clusterheads act as local coordinators to resolve channel scheduling, perform power measurement/control, maintain time division frame synchronization, and enhance the spatial reuse of time slots and codes. Moreover, to guarantee bandwidth for real time traffic, the architecture supports virtual circuits and allocates bandwidth to circuits at call setup time. The network is scalable to large numbers of nodes, and can handle mobility. Simulation experiments evaluate the performance of the proposed scheme in static and mobile environments.',266373,'Wireless Networks',3),(249958,NULL,'1998',NULL,'Effect of Connection Rerouting on Application Performance in Mobile Networks','The increasing deployment of wireless access technology, along with the emergence of high speed integrated service networks, such as ATM, promises to provide mobile users with ubiquitous access to multimedia information in the near future. One of the key problems in building connection-oriented ATM networks that support host mobility is designing mechanisms for rerouting virtual circuits to maintain data flow to and from mobile hosts. Ideally, VC rerouting must be done fast enough so as to cause minimal disruption to applications while minimizing the signaling overhead. In this paper, we evaluate the impact of several virtual circuit rerouting strategies on application performance. We initially identify the primitive operations required by any rerouting policy and use this to analytically quantify the cost of each rerouting policy in terms of wireless link disruption as a function of various network parameters. We then evaluate the effect of the VC rerouting policy on application-level performance using simulations. Our results show that the effect of rerouting policies are strongly dependent on the transport protocol policies and application QoS requirements, in addition to the network topology.',117351,'IEEE Transactions on Computers',3),(251619,NULL,'1998',NULL,'A Performance Study of Instruction Cache Prefetching Methods','Prefetching methods for instruction caches are studied via trace-driven simulation. The two primary methods are \"fall-through\" prefetch (sometimes referred to as \"one block lookahead\") and \"target\" prefetch. Fall-through prefetches are for sequential line accesses, and a key parameter is the distance from the end of the current line where the prefetch for the next line is initiated. Target prefetches work also for nonsequential line accesses. A prediction table is used and a key aspect is the prediction algorithm implemented by the table. Fall-through prefetch and target prefetch each improve performance significantly. When combined in a hybrid algorithm, their performance improvement is nearly additive. An instruction cache using a combined target and fall-through method can provide the same performance as a two to four times larger cache that does not prefetch. A good prediction method must not only be accurate, but prefetches must be initiated early enough to allow time for the instructions to return from main memory. To quantify this, we define a \"prefetch efficiency\" measure that reflects the amount of memory fetch delay that may be successfully hidden by prefetching. The better prefetch methods (in terms of miss rate) also have very high efficiencies, hiding approximately 90 percent of the miss delay for prefetched lines. Another performance measure of interest is memory traffic. Without prefetching, large line sizes give better hit rates; with prefetching, small line sizes tend to give better overall hit rates. Because smaller line sizes tend to reduce memory traffic, the top-performing prefetch caches produce less memory traffic than the top-performing nonprefetch caches of the same size.',117351,'IEEE Transactions on Computers',4),(251621,NULL,'1998',NULL,'CPU Cache Prefetching: Timing Evaluation of Hardware Implementations','Prefetching into CPU caches has long been known to be effective in reducing the cache miss ratio, but known implementations of prefetching have been unsuccessful in improving CPU performance. The reasons for this are that prefetches interfere with normal cache operations by making cache address and data ports busy, the memory bus busy, the memory banks busy, and by not necessarily being complete by the time that the prefetched data is actually referenced. In this paper, we present extensive quantitative results of a detailed cycle-by-cycle trace-driven simulation of a uniprocessor memory system in which we vary most of the relevant parameters in order to determine when and if hardware prefetching is useful. We find that, in order for prefetching to actually improve performance, the address array needs to be double ported and the data array needs to either be double ported or fully buffered. It is also very helpful for the bus to be very wide (e.g., 16 bytes) for bus transactions to be split and for main memory to be interleaved. Under the best circumstances, i.e., with a significant investment in extra hardware, prefetching can significantly improve performance. For implementations without adequate hardware, prefetching often decreases performance.',117351,'IEEE Transactions on Computers',4),(252751,NULL,'1998',NULL,'Architectural and compiler support for energy reduction in the memory hierarchy of high performance microprocessors','In this paper we propose a technique that uses an additional mini cache located between the I-Cache and the CPU core, and buffers instructions that are nested within loops and are continuously otherwise fetched from the I-Cache. This mechanism is combined with code modifications, through the compiler, that greatly simplify the required hardware, eliminate unnecessary instruction fetching, and consequently reduce signal switching activity and the dissipated energy.We show that the additional cache, dubbed L-Cache, is much smaller and simpler than the I-Cache when the compiler assumes the role of allocating instructions in it. Through simulation, we show that, for the SPECfp95 benchmarks, the I-Cache remains disabled most of the time, and the “cheaper” extra cache is used instead. We present experimental results that validate the effectiveness of this technique, and present the energy gains for most of the SPEC95 benchmarks.',132173,'ISLPED \'98 Proceedings of the 1998 international symposium on Low power electronics and design',4),(256688,NULL,'1998',NULL,'A Theory for Total Exchange in Multidimensional Interconnection Networks','Total exchange (or multiscattering) is one of the important collective communication problems in multiprocessor interconnection networks. It involves the dissemination of distinct messages from every node to every other node. We present a novel theory for solving the problem in any multidimensional (cartesian product) network. These networks have been adopted as cost-effective interconnection structures for distributed-memory multiprocessors. We construct a general algorithm for single-port networks and provide conditions under which it behaves optimally. It is seen that many of the popular topologies, including hypercubes, k-ary n-cubes, and general tori satisfy these conditions. The algorithm is also extended to homogeneous networks with 2k dimensions and with multiport capabilities. Optimality conditions are also given for this model. In the course of our analysis, we also derive a formula for the average distance of nodes in multidimensional networks; it can be used to obtain almost closed-form results for many interesting networks.',117407,'IEEE Transactions on Parallel and Distributed Systems',7),(257317,NULL,'1988',NULL,'Contest: a concurrent test generator for sequential circuits','This paper describes the application of a concurrent fault simulator to automatic test vector generation. As faults are simulated in the fault simulator a cost function is simultaneously computed. A simple cost function is the distance (in terms of the number of gates and flip-flops) of a fault effect from a primary output. The input vector is then modified to reduce the cost function until a test is found. The paper presents experimental results showing the effectiveness of this method in generating tests for combinational and sequential circuits. By defining suitable cost functions, we have been able to generate: 1) initialization sequences, 2) tests for a group of faults, and 3) a test for a given fault. Even asynchronous sequential circuits can be handled by this approach.',67541,'DAC \'88 Proceedings of the 25th ACM/IEEE Design Automation Conference',11),(267949,NULL,'1999',NULL,'Low-Power Divider','The general objective of our work is to develop methods to reduce the energy consumption of arithmetic modules while maintaining the delay unchanged and keeping the increase in the area to a minimum. Here, we illustrate some techniques for dividers realized in CMOS technology. The energy dissipation reduction is carried out at different levels of abstraction: from the algorithm level down to the implementation, or gate, level. We describe the use of techniques such as switching-off not active blocks, retiming, dual voltage, and equalizing the paths to reduce glitches. Also, we describe modifications in the on-the-fly conversion and rounding algorithm and in the redundant representation of the residual in order to reduce the energy dissipation. The techniques and modifications mentioned above are applied to a radix-4, divider, realized with static CMOS standard cells, for which a reduction of 40 percent is obtained with respect to the standard implementation. This reduction is expected to be about 60 percent if low-voltage gates, for dual voltage implementation, are available. The techniques used here should be applicable to a variety of arithmetic modules which have similar characteristics',117351,'IEEE Transactions on Computers',10),(267952,NULL,'1999',NULL,'Multiplexer-Based Array Multipliers','A new algorithm for the multiplication of two n-bit numbers based on the synchronous computation of the partial sums of the two operands is presented. The proposed algorithm permits an efficient realization of the parallel multiplication using iterative arrays. At the same time, it permits high-speed operation. Multiplier arrays for positive numbers and numbers in two\'s complement form based on the proposed technique are implemented. Also, an efficient pipeline form of the proposed multiplication scheme is introduced. All multipliers obtained have low circuit complexity permitting high-speed operation and the interconnections of the cells are regular, well-suited for VLSI realization',117351,'IEEE Transactions on Computers',8),(267987,NULL,'1999',NULL,'Prefetching Using Markov Predictors','Prefetching is one approach to reducing the latency of memory operations in modern computer systems. In this paper, we describe the Markov prefetcher. This prefetcher acts as an interface between the on-chip and off-chip cache and can be added to existing computer designs. The Markov prefetcher is distinguished by prefetching multiple reference predictions from the memory subsystem, and then prioritizing the delivery of those references to the processor. This design results in a prefetching system that provides good coverage, is accurate, and produces timely results that can be effectively used by the processor. We also explored a range of techniques that can be used to reduce the bandwidth demands of prefetching, leading to improved memory system performance. In our cycle-level simulations, the Markov Prefetcher reduces the overall execution stalls due to instruction and data memory operations by an average of 54 percent for various commercial benchmarks while only using two-thirds the memory of a demand-fetch cache organization.',117356,'IEEE Transactions on Computers - Special issue on cache memory and related problems',4),(268128,NULL,'1998',NULL,'Optimizing the Instruction Cache Performance of the Operating System','High instruction cache hit rates are key to high performance. One known technique to improve the hit rate of caches is to minimize cache interference by improving the layout of the basic blocks of the code. However, the performance impact of this technique has been reported for application code only, even though there is evidence that the operating system often uses the cache heavily and with less uniform patterns than applications. It is unknown how well existing optimizations perform for systems code and whether better optimizations can be found. We address this problem in this paper. This paper characterizes, in detail, the locality patterns of the operating system code and shows that there is substantial locality. Unfortunately, caches are not able to extract much of it: Rarely-executed special-case code disrupts spatial locality, loops with few iterations that call routines make loop locality hard to exploit, and plenty of loop-less code hampers temporal locality. Based on our observations, we propose an algorithm to expose these localities and reduce interference in the cache. For a range of cache sizes, associativities, lines sizes, and organizations, we show that we reduce total instruction miss rates by 31-86 percent, or up to 2.9 absolute points. Using a simple model, this corresponds to execution time reductions of the order of 10-25 percent. In addition, our optimized operating system combines well with optimized and unoptimized applications.',117351,'IEEE Transactions on Computers',4),(269426,NULL,'1999',NULL,'Efficient Techniques for Dynamic Test Sequence Compaction','Dynamic test sequence compaction is an effective means of reducing test application time and often results in higher fault coverages and reduced test generation time as well. Three simulation-based techniques for dynamic compaction of test sequences are described. The first technique uses a fault simulator to remove test vectors from the test sequence generated by a test generator if the vectors are not needed to detect the target fault, considering that the circuit state may be known. The second technique uses genetic algorithms to fill the unspecified bits in a partially-specified test sequence in order to increase the number of faults detected by the sequence. The third technique uses test sequences provided by the test generator as seeds in a genetic algorithm, and better sequences are evolved that detect more faults. Significant improvements in test set size, fault coverage, and test generation time have been obtained over previous approaches using combinations of the three techniques.',117351,'IEEE Transactions on Computers',11),(269429,NULL,'1999',NULL,'A Comparative Analysis of Cache Designs for Vector Processing','This paper presents an experimental study on cache memory designs for vector computers. We use an execution-driven simulator to evaluate vector cache performance of a set of application programs from Perfect Club and SPEC92 benchmark suites. Our simulation results uncover a few important facts which were unknown before: First of all, the prime-mapped cache that we newly proposed shows great performance potential in vector processing environment. Because of its conflict-free property, the prime-mapped cache performs significantly better than conventional cache designs for all applications considered. Second, performance results on the benchmarks indicate that data locality in vector processing does exist, although the effects of line size, associativity, replacement algorithm, and prefetching scheme on cache performance are very different from what has been commonly believed. A medium size vector cache (e.g., 128Kbytes) eliminates the necessity for a large number of interleaved memory banks in vector computers. Our experiments show that the vector computer that has a medium size prime-mapped cache with small cache line size and limited amount of prefetching provides significant speedup over conventional vector computers without cache. Performance results reported in this paper can also provide guidance to general-purpose computer designers to enhance cache performance for numerical applications.',117351,'IEEE Transactions on Computers',4),(274462,NULL,'1998',NULL,'Optimal Zero-Aliasing Space Compaction of Test Responses','Many built-in self-testing (BIST) schemes compress the test responses from a k-output circuit to q signature streams, where q 1.',117351,'IEEE Transactions on Computers',11),(274469,NULL,'1998',NULL,'Fast Approximation Algorithms on Maxcut, k-Coloring, and k-Color Ordering for VLSI Applications','There are a number of VLSI problems that have a common structure. We investigate such a structure that leads to a unified approach for three independent VLSI layout problems: partitioning, placement, and via minimization. Along the line, we first propose a linear-time approximation algorithm on maxcut and two closely related problems: k-coloring and maximal k-color ordering problem. The k-coloring is a generalization of the maxcut and the maximal k-color ordering is a generalization of the k-coloring. For a graph G with e edges and n vertices, our maxcut approximation algorithm runs in O(e + n) sequential time yielding a node-balanced maxcut with size at least (w(E) + w(E)/n)/2, improving the time complexity of O(e log e) known before. Building on the proposed maxcut technique and employing a height-balanced binary decomposition, we devise an O((e + n)log k) time algorithm for the k-coloring problem which always finds a k-partition of vertices such that the number of bad (or \"defected\") edges does not exceed (w(E)/k)((n$-$ 1)/n)log k, thus improving both the time complexity O(enk) and the bound e/k known before. The other related problem is the maximal k-color ordering problem that has been an open problem [16]. We show the problem is NP-complete, then present an approximation algorithm building on our k-coloring structure. A performance bound on maximal k-color ordering cost, 2kw(E)/3 is attained in O(ek) time. The solution quality of this algorithm is also tested experimentally and found to be effective.',117351,'IEEE Transactions on Computers',9),(285988,NULL,'1985',NULL,'Matchmaker: an interface specification language for distributed processing','Matchmaker, a language used to specify and automate the generation of interprocess communication interfaces, is presented. The process of and reasons for the evolution of Matchmaker are described. Performance and usage statistics are presented. Comparisons are made between Matchmaker and other related systems. Possible future directions are examined.',185523,'POPL \'85 Proceedings of the 12th ACM SIGACT-SIGPLAN symposium on Principles of programming languages',5),(287325,NULL,'1999',NULL,'Access region locality for high-bandwidth processor memory system design','This paper studies an interesting yet less explored behavior of memory access instructions, called access region locality. Unlike the traditional temporal and spatial data locality that focuses on individual memory locations and how accesses to the locations are inter-related, the access region locality concerns with each static memory instruction and its range of access locations at run time. We consider program\'s data, heap, and stack regions in this paper. Our experimental study using a set of SPEC95 benchmark programs shows that most memory reference instructions access a single region at run time. Also shown is that it is possible to accurately predict the access region of a memory instruction at run time by scrutinizing the addressing mode of the instruction and the past access region history of it. A simple run-time access region predictor is developed that is similar to a branch predictor in structure. We describe and evaluate a superscalar processor with two distinct sets of memory pipelines, driven by the access region predictor. Experimentalresultsindicate that the proposed mechanism is very effective in providing high memory bandwidth to the processor, resulting in comparable or better performance than a conventional memory design with a heavily multi-ported data cache that can lead to much higher hardware complexity.',192260,'Proceedings of the 32nd annual ACM/IEEE international symposium on Microarchitecture',4),(288986,NULL,'1976',NULL,'Bounds on the Complexity of the Longest Common Subsequence Problem','The problem of finding a longest common subsequence of two strings is discussed. This problem arises in data processing applications such as comparing two files and in genetic applications such as studying molecular evolution. The difficulty of computing a longest common subsequence of two strings is examined using the decision tree model of computation, in which vertices represent “equal - unequal” comparisons. It is shown that unless a bound on the total number of distinct symbols is assumed, every solution to the problem can consume an amount of time that is proportional to the product of the lengths of the two strings. A general lower bound as a function of the ratio of alphabet size to string length is derived. The case where comparisons between symbols of the same string are forbidden is also considered and it is shown that this problem is of linear complexity for a two-symbol alphabet and quadratic for an alphabet of three or more symbols.',135870,'Journal of the ACM (JACM)',9),(289005,NULL,'1976',NULL,'An Efficient Implementation of Edmonds\' Algorithm for Maximum Matching on Graphs','A matching on a graph is a set of edges, no two of which share a vertex. A maximum matching contains the greatest number of edges possible. This paper presents an efficient implementation of Edmonds\' algorithm for finding a maximum matching. The computation time is proportional to V3, where V is the number of vertices; previous implementations of Edmonds\' algorithm have computation time proportional to V4. The implementation is based on a system of labels that encodes the structure of alternating paths.',135870,'Journal of the ACM (JACM)',9),(290178,NULL,'1985',NULL,'Rectilinear shortest paths with rectangular barriers','We address ourselves to an instance of the Shortest Path problem with obstacles where a shortest path in the Manhattan (or L1) distance is sought between two points (source and destination) and the obstacles are n disjoint rectangles with sides parallel to the coordinate axes. A plane sweep technique is applied rather than the graph theoretic approach frequently used in the literature. We show that there has to be a path of minimum length between the two given points which is monotone in at least one of x or y directions. Then we present an algorithm of time complexity &Ogr;(n log n) for constructing that path and show that our algorithm is optimal.Lastly, we address the query form of this problem in which given a source point and n obstacles, after &Ogr;(n log n) time for preprocessing, a shortest path from the source point to a query point avoiding all the obstacles can be reported in &Ogr;(t + log n) time, where t is the number of turns on the path.',211169,'SCG \'85 Proceedings of the first annual symposium on Computational geometry',9),(290237,NULL,'1999',NULL,'Optimal Deadline Assignment for Scheduling Soft Aperiodic Tasks in Hard Real-Time Environments','In this paper, we present a new scheduling approach for servicing soft aperiodic requests in a hard real-time environment, where a set of hard periodic tasks is scheduled using the Earliest Deadline First algorithm. The main characteristic of the proposed algorithm is that it achieves full processor utilization and optimal aperiodic responsiveness, still guaranteeing the execution of the periodic tasks. Another interesting feature of the proposed algorithm is that it can easily be tuned to balance performance versus complexity for adapting it to different application requirements. Schedulability issues, performance results, and implementation complexity of the algorithm are discussed and compared with other methods, such as Background, the Total Bandwidth Server, and the Slack Stealer. Resource reclaiming and extensions to more general cases are also considered. Extensive simulations show that a substantial improvement can be achieved with a little increase of complexity, ranging from the performance of the Total Bandwidth Server up to the optimal behavior.',117351,'IEEE Transactions on Computers',0),(290247,NULL,'1999',NULL,'A Cone-Based Genetic Optimization Procedure for Test Generation and Its Application to n$n$-Detections in Combinational Circuits','Test generation procedures based on genetic optimization were shown to be effective in achieving high fault coverage for benchmark circuits. In this work, we propose a representation of test patterns for genetic optimization based test generation, where subsets of inputs are considered as indivisible entities. Using this representation, crossover between two test patterns $t_1$ and $t_2$ copies all the values of each subset either from $t_1$ or from $t_2$. By keeping input subsets undivided, activation and propagation capabilities of $t_1$ and $t_2$ are expected to be captured and carried over to the new test patterns. Experimental results presented show that the proposed scheme results in complete stuck-at test sets and $n$-detection test sets for combinational circuits, even in cases where other procedures report incomplete fault coverages.',117351,'IEEE Transactions on Computers',11),(290622,NULL,'1999',NULL,'Stress-Based and Path-Based Fault Injection','The objective of fault injection is to mimic the existence of faults and to force the exercise of the fault tolerance mechanisms of the target system. To maximize the efficacy of each injection, the locations, timing, and conditions for faults being injected must be carefully chosen. Faults should be injected with a high probability of being accessed. This paper presents two fault injection methodologies驴stress-based injection and path-based injection; both are based on resource activity analysis to ensure that injections cause fault tolerance activity and, thus, the resulting exercise of fault tolerance mechanisms. The difference between these two methods is that stress-based injection validates the system dependability by monitoring the run-time workload activity at the system level to select faults that coincide with the locations and times of greatest workload activity, while path-based injection validates the system from the application perspective by using an analysis of the program flow and resource usage at the application program level to select faults during the program execution. These two injection methodologies focus separately on the system and process viewpoints to facilitate the testing of system dependability. Details of these two injection methodologies are discussed in this paper, along with their implementations, experimental results, and advantages and disadvantages.',117351,'IEEE Transactions on Computers',6),(290624,NULL,'1999',NULL,'The Necessary Conditions for Clos-Type Nonblocking Multicast Networks','Efficient interconnection networks are critical in providing low latency, high bandwidth communication in parallel and distributed computing systems with hundreds or thousands of processors. The well-known Clos network or $v(m,n,r)$ network can be extended to provide full one-to-many or multicast capability. In this paper, we consider several typical routing control strategies for Clos-type nonblocking multicast networks and derive the necessary conditions under which this type of network is nonblocking for arbitrary multicast assignments in the strict sense as well as under these control strategies. The necessary conditions obtained are represented as the number of middle stage switches $m \\geq \\Theta\\left(n {\\frac{\\log r}{\\log \\log r}} \\right )$. These results match the sufficient nonblocking condition for the currently best available explicitly constructed, constant stage nonblocking multicast network [8], [9], and provide a basis for the optimal design of this type of multicast network.',117351,'IEEE Transactions on Computers',7),(290630,NULL,'1999',NULL,'Branch Prediction, Instruction-Window Size, and Cache Size: Performance Trade-Offs and Simulation Techniques','Design parameters interact in complex ways in modern processors, especially because out-of-order issue and decoupling buffers allow latencies to be overlapped. Trade-offs among instruction-window size, branch-prediction accuracy, and instruction- and data-cache size can change as these parameters move through different domains. For example, modeling unrealistic caches can under- or overstate the benefits of better prediction or a larger instruction window. Avoiding such pitfalls requires understanding how all these parameters interact. Because such methodological mistakes are common, this paper provides a comprehensive set of SimpleScalar simulation results from SPECint95 programs, showing the interactions among these major structures. In addition to presenting this database of simulation results, major mechanisms driving the observed trade-offs are described. The paper also considers appropriate simulation techniques when sampling full-length runs with the SPEC reference inputs. In particular, the results show that branch mispredictions limit the benefits of larger instruction windows, that better branch prediction and better instruction cache behavior have synergistic effects, and that the benefits of larger instruction windows and larger data caches trade off and have overlapping effects. In addition, simulations of only 50 million instructions can yield representative results if these short windows are carefully selected.',117351,'IEEE Transactions on Computers',4),(290676,NULL,'1999',NULL,'Combined Task and Message Scheduling in Distributed Real-Time Systems','This paper presents an algorithm for off-line scheduling of communicating tasks with precedence and exclusion constraints in distributed hard real-time systems. Tasks are assumed to communicate via message passing based on a time-bounded communication paradigm, such as the real-time channel [1]. The algorithm uses a branch-and-bound (B&B) technique to search for a task schedule by minimizing maximum task lateness (defined as the difference between task completion time and task deadline), and exploits the interplay between task and message scheduling to improve the quality of solution. It generates a complete schedule at each vertex in the search tree, and can be made to yield a feasible schedule (found before reaching an optimal solution), or proceed until an optimal task schedule is found. We have conducted an extensive simulation study to evaluate the performance of the proposed algorithm. The algorithm is shown to scale well with respect to system size and degree of intertask interactions. It also offers good performance for workloads with a wide range of CPU utilizations and application concurrency. For larger systems and higher loads, we introduce a greedy heuristic that is faster but has no optimality properties. We have also extended the algorithm to a more general resource-constraint model, thus widening its application domain.',117407,'IEEE Transactions on Parallel and Distributed Systems',0),(295869,NULL,'1999',NULL,'Run-Time Cache Bypassing','The growing disparity between processor and memory performance has made cache misses increasingly expensive. Additionally, data and instruction caches are not always used efficiently, resulting in large numbers of cache misses. Therefore, the importance of cache performance improvements at each level of the memory hierarchy will continue to grow. In numeric programs, there are several known compiler techniques for optimizing data cache performance. However, integer (nonnumeric) programs often have irregular access patterns that are more difficult for the compiler to optimize. In the past, cache management techniques such as cache bypassing were implemented manually at the machine-language-programming level. As the available chip area grows, it makes sense to spend more resources to allow intelligent control over the cache management. In this paper, we present an approach to improving cache effectiveness, taking advantage of the growing chip area, utilizing run-time adaptive cache management techniques, optimizing both performance and cost of implementation. Specifically, we are aiming to increase data cache effectiveness for integer programs. We propose a microarchitecture scheme where the hardware determines data placement within the cache hierarchy based on dynamic referencing behavior. This scheme is fully compatible with existing Instruction Set Architectures. This paper examines the theoretical upper bounds on the cache hit ratio that cache bypassing can provide for integer applications, including several Windows applications with OS activity. Then, detailed trace-driven simulations of the integer applications are used to show that the implementation described in this paper can achieve performance close to that of the upper bound.',117351,'IEEE Transactions on Computers',4),(296486,NULL,'1999',NULL,'A New Self-Routing Multicast Network','In this paper, we propose a design for a new self-routing multicast network which can realize arbitrary multicast assignments between its inputs and outputs without any blocking. The network design uses a recursive decomposition approach and is based on the binary radix sorting concept. All functional components of the network are reverse banyan networks. Specifically, the new multicast network is recursively constructed by cascading a binary splitting network and two half-size multicast networks. The binary splitting network, in turn, consists of two recursively constructed reverse banyan networks. The first reverse banyan network serves as a scatter network and the second reverse banyan network serves as a quasisorting network. The advantage of this approach is to provide a way to self-route multicast assignments through the network and a possibility to reuse part of network to reduce the network cost. The new multicast network we design is compared favorably with the previously proposed multicast networks. It uses $O(n\\log^2 n)$ logic gates, and has $O(\\log^2 n)$ depth and $O(\\log^2 n)$ routing time where the unit of time is a gate delay. By reusing part of the network, the feedback implementation of the network can further reduce the network cost to $O(n\\log n)$.',117407,'IEEE Transactions on Parallel and Distributed Systems',7),(296742,NULL,'2000',NULL,'Fault-Tolerant Adaptive and Minimal Routing in Mesh-Connected Multicomputers Using Extended Safety Levels','The minimal routing problem in mesh-connected multicomputers with faulty blocks is studied, Two-dimensional meshes are used to illustrate the approach. A sufficient condition for minimal routing in 2D meshes with faulty blocks is proposed. Unlike many traditional models that assume all the nodes know global fault distribution, our approach is based on the concept of an extended safety level, which is a special form of limited fault information. The extended safety level information is captured by a vector associated with each node. When the safety level of a node reaches a certain level (or meets certain conditions), a minimal path exists from this node to any nonfaulty nodes in 2D meshes. Specifically, we study the existence of minimal paths at a given source node, limited distribution of fault information, and minimal routing itself. We propose three fault-tolerant minimal routing algorithms which are adaptive to allow all messages to use any minimal path. We also provide some general ideas to extend our approaches to other low-dimensional mesh-connected multicomputers such as 2D tori and 3D meshes. Our approach is the first attempt to address adaptive and minimal routing in 2D meshes with faulty blocks using limited fault information.',117407,'IEEE Transactions on Parallel and Distributed Systems',7),(299229,NULL,'2000',NULL,'Discrete Interval Truth Values Logic and Its Application','In this paper, we focus on functions defined on a special subset of the power set of $\\left\\{0, 1, \\ldots, r-1\\right\\}$ (the elements in the subset will be called discrete interval truth values) and operations on the truth values. The operations discussed in this paper will be called regular because they are one of the extensions of the regularity, which was first introduced by Kleene in his ternary logic. Mukaidono investigated some properties of ternary functions which can be represented by the regular operations. He called such ternary functions 驴regular ternary logic functions.驴 Regular ternary logic functions are useful for representing and analyzing ambiguities such as transient states and/or initial states in binary logic circuits that Boolean functions cannot cope with. Furthermore, they are also applied to studies of fail-safe systems for binary logic circuits. In this paper, we will discuss an extension of regular ternary logic functions to functions on the discrete interval truth values. First, we will suggest an extension of the regularity, in the sense of Kleene, into operations on the discrete interval truth values. We will then present some mathematical properties of functions on the discrete interval truth values consisting of regular operations and one application of these functions.',117351,'IEEE Transactions on Computers',1),(299230,NULL,'2000',NULL,'Threshold-Based Mechanisms to Discriminate Transient from Intermittent Faults','This paper presents a class of count-and-threshold mechanisms, collectively named $\\alpha$-count, which are able to discriminate between transient faults and intermittent faults in computing systems. For many years, commercial systems have been using transient fault discrimination via threshold-based techniques. We aim to contribute to the utility of count-and-threshold schemes, by exploring their effects on the system. We adopt a mathematically defined structure, which is simple enough to analyze by standard tools. $\\alpha$-count is equipped with internal parameters that can be tuned to suit environmental variables (such as transient fault rate, intermittent fault occurrence patterns). We carried out an extensive behavior analysis for two versions of the count-and-threshold scheme, assuming, first, exponentially distributed fault occurrencies and, then, more realistic fault patterns.',117351,'IEEE Transactions on Computers',6),(299831,NULL,'2000',NULL,'Optimal All-to-All Personalized Exchange in Self-Routable Multistage Networks','All-to-all personalized exchange is one of the most dense collective communication patterns and occurs in many important applications in parallel computing. Previous all-to-all personalized exchange algorithms were mainly developed for hypercube and mesh/torus networks. Although the algorithms for a hypercube may achieve optimal time complexity, the network suffers from unbounded node degrees and thus has poor scalability in terms of I/O port limitation in a processor. On the other hand, a mesh/torus has a constant node degree and better scalability in this aspect, but the all-to-all personalized exchange algorithms have higher time complexity. In this paper, we propose an alternative approach to efficient all-to-all personalized exchange by considering another important type of networks, multistage networks, for parallel computing systems. We present a new all-to-all personalized exchange algorithm for a class of unique-path, self-routable multistage networks. We first develop a generic method for decomposing all-to-all personalized exchange patterns into some permutations which are realizable in these networks, and then present a new all-to-all personalized exchange algorithm based on this method. The newly proposed algorithm has $O(n)$ time complexity for an $n \\times n$ network, which is optimal for all-to-all personalized exchange. By taking advantage of fast switch setting of self-routable switches and the property of a single input/output port per processor in a multistage network, we believe that a multistage network could be a better choice for implementing all-to-all personalized exchange due to its shorter communication latency and better scalability.',117407,'IEEE Transactions on Parallel and Distributed Systems',7),(300930,NULL,'2000',NULL,'Scheduling Distributed Real-Time Tasks with Minimum Jitter','The problem of scheduling real-time tasks with minimum jitter is particularly important in many control applications; nevertheless, it has rarely been studied in the scientific literature. This paper presents a unconventional scheduling approach for distributed static systems where tasks are periodic and have arbitrary deadlines, precedence, and exclusion constraints. The solution presented in this work not only creates feasible schedules, but also minimizes jitter for periodic tasks. We present a general framework consisting of an abstract architecture model and a general programming model. We show how to design a surprisingly simple and flexible scheduling method based on simulated annealing. Experimental results demonstrate the significant improvement of our algorithm over earliest deadline first and rate monotonic algorithms.',117351,'IEEE Transactions on Computers',0),(303693,NULL,'2000',NULL,'A case for end system multicast (keynote address)','The conventional wisdom has been that IP is the natural protocol layer for implementing multicast related functionality. However, ten years after its initial proposal, IP Multicast is still plagued with concerns pertaining to scalability, network management, deployment and support for higher layer functionality such as error, flow and congestion control. In this paper, we explore an alternative architecture for small and sparse groups, where end systems implement all multicast related functionality including membership management and packet replication. We call such a scheme End System Multicast. This shifting of multicast support from routers to end systems has the potential to address most problems associated with IP Multicast. However, the key concern is the performance penalty associated with such a model. In particular, End System Multicast introduces duplicate packets on physical links and incurs larger end-to-end delay than IP Multicast. In this paper, we study this question in the context of the Narada protocol. In Narada, end systems self-organize into an overlay structure using a fully distributed protocol. In addition, Narada attempts to optimize the efficiency of the overlay based on end-to-end measurements. We present details of Narada and evaluate it using both simulation and Internet experiments. Preliminary results are encouraging. In most simulations and Internet experiments, the delay and bandwidth penalty are low. We believe the potential benefits of repartitioning multicast functionality between end systems and routers significantly outweigh the performance penalty incurred.',190872,'Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems',3),(303696,NULL,'2000',NULL,'PLM: fast convergence for cumulative layered multicast transmisson schemes','A major challenge in the Internet is to deliver live audio/video content with a good quality and to transfer files to large number of heterogeneous receivers. Multicast and cumulative layered transmission are two mechanisms of interest to accomplish this task efficiently. However, protocols using these mechanisms suffer from slow convergence time, lack of inter-protocol fairness or TCP-fairness, and loss induced by the join experiments.In this paper we define and investigate the properties of a new multicast congestion control protocol (called PLM) for audio/video and file transfer applications based on a cumulative layered multicast transmission. A fundamental contribution of this paper is the introduction and evaluation of a new and efficient technique based on packet pair to infer which layers to join. We evaluated PLM for a large variety of scenarios and show that it converges fast to the optimal link utilization, induces no loss to track the available bandwidth, has inter-protocol fairness and TCP-fairness, and scales with the number of receivers and the number of sessions. Moreover, all these properties hold in self similar and multifractal environment.',190872,'Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems',3),(304007,NULL,'2000',NULL,'Understanding the backward slices of performance degrading instructions','For many applications, branch mispredictions and cache misses limit a processor\'s performance to a level well below its peak instruction throughput. A small fraction of static instructions, whose behavior cannot be anticipated using current branch predictors and caches, contribute a large fraction of such performance degrading events. This paper analyzes the dynamic instruction stream leading up to these performance degrading instructions to identify the operations necessary to execute them early. The backward slice (the subset of the program that relates to the instruction) of these performance degrading instructions, if small compared to the whole dynamic instruction stream, can be pre-executed to hide the instruction\'s latency. To overcome conservative dependance assumptions that result in large slices, speculation can be used, resulting in speculative slices.This paper provides an initial characterization of the backward slices of L2 data cache misses and branch mispredictions, and shows the effectiveness of techniques, including memory dependence prediction and control independence, for reducing the size of these slices. Through the use of these techniques, many slices can be reduced to less than one tenth of the full dynamic instruction stream when considering the 512 instructions before the performance degrading instruction.',191947,'Proceedings of the 27th annual international symposium on Computer architecture',4),(308701,NULL,'2000',NULL,'Procedures for Static Compaction of Test Sequences for Synchronous Sequential Circuits','We propose three static compaction techniques for test sequences of synchronous sequential circuits. We apply the proposed techniques to test sequences generated for benchmark circuits by various test generation procedures. The results show that the test sequences generated by all the test generation procedures considered can be significantly compacted. The compacted sequences thus have shorter test application times and smaller memory requirements. As a by-product, the fault coverage is sometimes increased as well. Additionally, the ability to significantly reduce the length of the test sequences indicates that it may be possible to reduce test generation time if superfluous input vectors are not generated.',117351,'IEEE Transactions on Computers',11),(309440,NULL,'2000',NULL,'Intersystem location update and paging schemes for multitier wireless networks','Global wireless networks enable mobile users to communicate regardless of their locations. One of the most important issues is location management in a highly dynamic environment because mobile users may roam between different wireless networks, network operators, and geographical regions. In this paper, a location tracking mechanism is introduced, which consists of intersystem location updates using the concept of boundary location area (BLA) and paging using the concept of boundary location register (BLR). The BLA is determined by a dynamic location update policy in which the velocity and the quality of service (QoS) are taken into account on a per-user basis. The BLR is used to maintain the records of mobile users crossing the boundary of networks. This mechanism not only reduces location tracking costs but also significantly decreases call loss rates and average paging delays. The performance evaluation of the proposed schemes is provided to demonstrate their effectiveness in multitier wireless networks.',157807,'MobiCom \'00 Proceedings of the 6th annual international conference on Mobile computing and networking',3),(311544,NULL,'2000',NULL,'Minimized Power Consumption for Scan-Based BIST','Power consumption of digital systems may increasesignificantly during testing. In this paper, systems equipped with ascan-based built-in self-test like the STUMPS architecture areanalyzed, the modules and modes with the highest power consumptionare identified, and design modifications to reduce power consumptionare proposed. The design modifications include some gating logic formasking the scan path activity during shifting, and the synthesis ofadditional logic for suppressing random patterns which do notcontribute to increase the fault coverage. These design changesreduce power consumption during BIST by several orders of magnitude,at very low cost in terms of area and performance.',135282,'Journal of Electronic Testing: Theory and Applications - special issue on the European test workshop 1999',10),(316837,NULL,'2000',NULL,'Exploiting Omissive Faults in Synchronous Approximate Agreement','In a fault-tolerant distributed system, it is often necessary for nonfaulty processes to agree on the value of a shared data item. The criterion of Approximate Agreement does not require processes to achieve exact agreement on a value; rather, they need only agree to within a predefined numerical tolerance. Approximate Agreement can be achieved through convergent voting algorithms. Previous research has studied convergent voting algorithms under mixed-mode or hybrid fault models, such as the Thambidurai and Park Hybrid fault model, comprised of three fault modes: asymmetric, symmetric, and benign. This paper makes three major contributions to the state of the art in fault-tolerant convergent voting. 1) We partition both the asymmetric and symmetric fault modes into disjoint omissive and transmissive submodes. The resulting five-mode hybrid fault model is a superset of previous hybrid fault models. 2) We present a new family of voting algorithms, called Omission Mean Subsequence Reduced (OMSR), which implicitly recognize and exploit omissive behavior in malicious faults while still maintaining full Byzantine fault tolerance. 3) We show that OMSR voting algorithms are more fault-tolerant than previous voting algorithms if any of the currently active faults is omissive.',117351,'IEEE Transactions on Computers',6),(319121,NULL,'1984',NULL,'Hardware protection against software piracy','A system that prevents illicit duplication of proprietary software is suggested. It entails the customization of the programs for each computer by encryption. The use of a public key cryptogram for this purpose means that anyone can customize programs, but neither other programmers nor the people having complete access to the target computer can obtain copies that will run on other machines. A possible implementation of the system is considered in some detail. It is based on a hardware security unit that is attached to the computer and that decrypts and obeys some parts of the program.',52621,'Communications of the ACM',5),(319987,NULL,'1978',NULL,'The evolution of the DECsystem 10','The DECsystem 10, also known as the PDP-10, evolved from the PDP-6 (circa 1963) over five generations of implementations to presently include systems covering a price range of five to one. The origin and evolution of the hardware, operating system, and languages are described in terms of technological change, user requirements, and user developments. The PDP-10\'s contributions to computing technology include: accelerating the transition from batch oriented to time sharing computing systems; transferring hardware technology within DEC (and elsewhere) to minicomputer design and manufacturing; supporting minicomputer hardware and software development; and serving as a model for single user and timeshared interactive minicomputer/microcomputer systems.',52687,'Communications of the ACM - Special issue on computer architecture',5),(320221,NULL,'1978',NULL,'Right brother trees','Insertion and deletion algorithms are provided for the class of right (or one-sided) brother trees which have O (log n) performance. The importance of these results stems from the close relationship of right brother trees to one-sided height-balanced trees which have an insertion algorithm operating in O (log2 n). Further, although both insertion and deletion can be carried out in O (log n) time for right brother trees, it appears that the insertion algorithm is inherently much more difficult than the deletion algorithm—the reverse of what one usually obtains.',52621,'Communications of the ACM',9),(320810,NULL,'1976',NULL,'Formal verification of parallel programs','Two formal models for parallel computation are presented: an abstract conceptual model and a parallel-program model. The former model does not distinguish between control and data states. The latter model includes the capability for the representation of an infinite set of control states by allowing there to be arbitrarily many instruction pointers (or processes) executing the program. An induction principle is presented which treats the control and data state sets on the same ground. Through the use of “place variables,” it is observed that certain correctness conditions can be expressed without enumeration of the set of all possible control states. Examples are presented in which the induction principle is used to demonstrate proofs of mutual exclusion. It is shown that assertions-oriented proof methods are special cases of the induction principle. A special case of the assertions method, which is called parallel place assertions, is shown to be incomplete. A formalization of “deadlock” is then presented. The concept of a “norm” is introduced, which yields an extension, to the deadlock problem, of Floyd\'s technique for proving termination. Also discussed is an extension of the program model which allows each process to have its own local variables and permits shared global variables. Correctness of certain forms of implementation is also discussed. An Appendix is included which relates this work to previous work on the satisfiability of certain logical formulas.',52621,'Communications of the ACM',2),(320845,NULL,'2001',NULL,'Detailed routing architectures for embedded programmable logic IP cores','As the complexity of integrated circuits increases, the ability to make post-fabrication changes to fixed ASIC chips will become more and more attractive. This ability can be realized using programmable logic cores. These cores are blocks of programmable logic that can be embedded into a fixed-function ASIC or a custom chip. Such cores differ from stand-alone FPGAs in that they can take on a variety of shapes and sizes. With this in mind, we investigate the detailed routing characteristics of rectangular programmable logic cores. We quantify the effects of having different x and y channel capacities, and show that the optimum ratio between the x and y channel widths for a rectangular core is between 1.2 and 1.5. We also present a new switch block family optimized for rectangular cores. Compared to a simple extension of an existing switch block, our new architecture leads to an 8.7% improvement in density with little effect on speed. Finally, we show that if the channel widths and switch block are chosen carefully the penalty for using a rectangular core (compared to a square core with the same logic capacity) is small; for a core with an aspect ratio of 2:1, the area penalty is 1.6% and the speed penalty is 1.1%.',100297,'FPGA \'01 Proceedings of the 2001 ACM/SIGDA ninth international symposium on Field programmable gate arrays',10),(320887,NULL,'1976',NULL,'Texture and reflection in computer generated images','In 1974 Catmull developed a new algorithm for rendering images of bivariate surface patches. This paper describes extensions of this algorithm in the areas of texture simulation and lighting models. The parametrization of a patch defines a coordinate system which is used as a key for mapping patterns onto the surface. The intensity of the pattern at each picture element is computed as a weighted average of regions of the pattern definition function. The shape and size of this weighting function are chosen using digital signal processing theory. The patch rendering algorithm allows accurate computation of the surface normal to the patch at each picture element, permitting the simulation of mirror reflections. The amount of light coming from a given direction is modeled in a similar manner to the texture mapping and then added to the intensity obtained from the texture mapping. Several examples of images synthesized using these new techniques are included.',52621,'Communications of the ACM',2),(321651,NULL,'1972',NULL,'Use of the Hough transformation to detect lines and curves in pictures','Hough has proposed an interesting and computationally efficient procedure for detecting lines in pictures. This paper points out that the use of angle-radius rather than slope-intercept parameters simplifies the computation further. It also shows how the method can be used for more general curve fitting, and gives alternative interpretations that explain the source of its efficiency.',52621,'Communications of the ACM',2),(322098,NULL,'2000',NULL,'Deterministic built-in test pattern generation for high-performance circuits using twisted-ring counters','We present a new approach for built-in test pattern generation based on the reseeding of twisted-ring counters (TRCs). The proposed technique embeds a precomputed deterministic test set for the circuit under test (CUT) in a short test sequence produced by a TRC. The TRC is designed using existing circuit flip-flops and does not add to hardware overhead beyond what is required for basic scan design. The test control logic is simple, uniform for all circuits, and can be shared among multiple CUTs. Furthermore, the proposed method requires no mapping logic between the test generator circuit and the CUT; hence it imposes no additional performance penalty. Experimental results for the ISCAS benchmark circuits show that it is indeed possible to embed the entire precomputed test set in a TRC sequence using only a small number of seeds.',117462,'IEEE Transactions on Very Large Scale Integration (VLSI) Systems - Special issue on the 11th international symposium on system-level synthesis and design (ISSS\'98)',11),(322334,NULL,'1970',NULL,'Experiments with the M & N tree-searching program','The M & N procedure is an improvement to the mini-max backing-up procedure widely used in computer programs for game-playing and other purposes. It is based on the principle that it is desirable to have many options when making decisions in the face of uncertainty. The mini-max procedure assigns to a MAX (MIN) node the value of the highest (lowest) valued successor to that node. The M & N procedure assigns to a MAX (MIN) node some function of the M (N) highest (lowest) valued successors. An M & N procedure was written in LISP to play the game of kalah, and it was demonstrated that the M & N procedure is significantly superior to the mini-max procedure. The statistical significance of important conclusions is given. Since information on statistical significance has often been lacking in papers on computer experiments in the artificial intelligence field, these experiments can perhaps serve as a model for future work.',52621,'Communications of the ACM',2),(330191,NULL,'2000',NULL,'Novel Test Pattern Generators for Pseudoexhaustive Testing','Pseudoexhaustive testing of a combinational circuit involves applying all possible input patterns to all its individual output cones. The testing ensures detection of all detectable multiple stuck-at faults in the circuit and all detectable combinational faults within individual cones. Test pattern generators based on coding theory principles are not tailored to a specific circuit as they do not utilize any structural information. They usually generate test sets that are several orders of magnitude larger than the minimum size pseudoexhaustive test set required for a specific circuit. In this paper, we describe hardware efficient test pattern generators that employ knowledge of the circuit output cone structures for generating minimal test sets. Using our techniques, we have designed generators that generate minimum size test sets for the ISCAS benchmark circuits.',117351,'IEEE Transactions on Computers',11),(331644,NULL,'2001',NULL,'Eliminating Overflow for Large-Scale Mobility Databases in Cellular Telephone Networks','In a cellular phone system, mobility databases called visitor location registers (VLRs) are used to temporarily hold the subscription information of the roaming users who visit the service area of the VLR. When the users leave the VLR area, the corresponding records in the VLR are deleted. Due to user mobility, the capacity of the VLR may not be large enough to hold information for all visitors in the VLR area at some time periods. This issue is called VLR overflow. This paper describes a record replacement policy to allow mobile users to receive services in the VLR overflow situation. We utilize analytic modeling to investigate the performance of the replacement policy. The study indicates that our approach effectively eliminates the VLR overflow problem with insignificant extra overhead.',117351,'IEEE Transactions on Computers',3),(334436,NULL,'2001',NULL,'Push-Based Information Delivery in Two Stage Satellite-Terrestrial Wireless Systems','One of the prominent objectives of National/Global Information Infrastructure is to provide all types of users global access to information. Satellite broadcast data delivery has inherent advantages, such as scalability and location independent availability, in achieving this objective. However, users need expensive and cumbersome equipment to receive and transmit satellite signals. Furthermore, as the amount of information being broadcast increases, average user latency increases as well. Often, users in a geographical locality have similar interests, which can be better served by employing a local broadcast schedule. In this context, a two stage satellite-terrestrial wireless broadcast system can provide more efficient service in terms of lower average user latency and cheaper and more convenient user equipment. In such a system, the main server broadcasts information via satellite to the geographically distributed local ground stations. Every ground station has limited buffer capacity to store the data broadcast by the satellite. According to their buffer content, and the interests of their users, local stations deliver the information to their users via terrestrial wireless channel. We develop novel methods for the joint cache management and scheduling problem encountered in these systems. Our results demonstrate that such two stage systems are feasible and they can provide more efficient data delivery compared to the single stage systems.',117351,'IEEE Transactions on Computers',3),(334438,NULL,'2001',NULL,'Comment on \'On-Line Scheduling Policies for a Class of IRIS Real-Time Tasks\'','Scheduling policies for real-time tasks which receive rewards that depend on the amount of service received were presented by Dey et al. The policies utilized heuristic approaches to maximize the total accrued reward. In this comment, we propose an extension of their work that may reduce the average computational complexity.',117351,'IEEE Transactions on Computers',0),(336337,NULL,'2001',NULL,'An approach to test compaction for scan circuits that enhances at-speed testing','We propose a new approach to the generation to compact test sets for scan circuits. Compaction refers here to a reduction in the test application time. The proposed procedure generates an initial test set that is likely to have a low test application time. It then applies an existing static compaction procedure to this initial test set to further compact it. As a by-product, the proposed procedure also results in long primary input sequences, which are applied at-speed. This contributes to the detection of delay defects. We demonstrate through experimental results the advantages of this approach over earlier ones as a method for generating test sets with minimal test application time and long primary input sequences.',192313,'Proceedings of the 38th annual Design Automation Conference',11),(336915,NULL,'2001',NULL,'A true single-phase 8-bit adiabatic multiplier','This paper presents the design and evaluation of an 8-bit adiabatic multiplier. Both the multiplier core and its built-in self-test logic have been designed using a true single-phase adiabatic logic family. Energy is supplied to the adiabatic circuitry via a sinusoidal power-clock waveform that is generated on-chip. In HSPICE simulations with post-layout extracted parasitics, our design functions correctly at clock frequencies exceeding 200 MHz. The total dissipation of the multiplier core and self-test circuitry approaches 130pJ per operation at 200MHz. Our 11,854-transistor chip has been fabricated in a 0.5&mgrm standard CMOS process with an active area of 0.470mm$^2$. Correct chip operation has been validated for operating frequencies up to 130MHz, the limit of our experimental setup. Measured dissipation correlates well with HSPICE simulations.',192313,'Proceedings of the 38th annual Design Automation Conference',10),(339219,NULL,'2001',NULL,'SPINS: security protocols for sensor networks','As sensor networks edge closer towards wide-spread deployment, security issues become a central concern. So far, much research has focused on making sensor networks feasible and useful, and has not concentrated on security.We present a suite of security building blocks optimized for resource-constrained environments and wireless communication. SPINS has two secure building blocks: SNEP and &mgr;TESLA SNEP provides the following important baseline security primitives: Data confidentiality, two-party data authentication, and data freshness. A particularly hard problem is to provide efficient broadcast authentication, which is an important mechanism for sensor networks. &mgr;TESLA is a new protocol which provides authenticated broadcast for severely resource-constrained environments. We implemented the above protocols, and show that they are practical even on minimal hardware: the performance of the protocol suite easily matches the data rate of our network. Additionally, we demonstrate that the suite can be used for building higher level protocols.',193134,'Proceedings of the 7th annual international conference on Mobile computing and networking',3),(340165,NULL,'2001',NULL,'Reliable group rekeying: a performance analysis','In secure group communications, users of a group share a common group key. A key server sends the group key to authorized new users as well as performs group rekeying for group users whenever the key changes. In this paper, we investigate scalability issues of reliable group rekeying, and provide a performance analysis of our group key management system (called keygem) based upon the use of key trees. Instead of rekeying after each join or leave, we use periodic batch rekeying to improve scalability and alleviate out-of-sync problems among rekey messages as well as between rekey and data messages. Our analyses show that batch rekeying can achieve large performance gains. We then investigate reliable multicast of rekey messages using proactive FEC. We observe that rekey transport has an eventual reliability and a soft real-time requirement, and that the rekey workload has a sparseness property, that is, each group user only needs to receive a small fraction of the packets that carry a rekey message sent by the key server. We also investigate tradeoffs between server and receiver bandwidth requirements versus group rekey interval, and show how to determine the maximum number of group users a key server can support.',190885,'Proceedings of the 2001 conference on Applications, technologies, architectures, and protocols for computer communications',3),(342296,NULL,'2001',NULL,'A High-Bandwidth Memory Pipeline for Wide Issue Processors','Providing adequate data bandwidth is extremely important for a future wide-issue processor to achieve its full performance potential. Adding a large number of ports to a data cache, however, becomes increasingly inefficient and can add to the hardware complexity significantly. This paper takes an alternative or complementary approach for providing more data bandwidth, called data decoupling. This paper especially studies an interesting, yet less explored, behavior of memory access instructions, called access region locality, which is concerned with each static memory instruction and its range of access locations at runtime. Our experimental study using a set of SPEC95 benchmark programs shows that most memory access instructions reference a single region at runtime. Also shown is that it is possible to accurately predict the access region of a memory instruction at runtime by scrutinizing the addressing mode of the instruction and the past access history of it. We describe and evaluate a wide-issue superscalar processor with two distinct sets of memory pipelines and caches, driven by the access region predictor. Experimental results indicate that the proposed mechanism is very effective in providing high memory bandwidth to the processor, resulting in comparable or better performance than a conventional memory design with a heavily multiported data cache that can lead to much higher hardware complexity.',117351,'IEEE Transactions on Computers',4),(342338,NULL,'2001',NULL,'The performance of public key-enabled kerberos authentication in mobile computing applications','Authenticating mobile computing users can require a significant amount of processing and communications resources-particularly when protocols based on public key encryption are invoked. These resource requirements can result in unacceptable response times for the user. In this paper, we analyze adaptations of the public key-enabled Kerberos network authentication protocol to a mobile platform by measuring the service time of a \"skeleton\" implementation and constructing a closed queuing network model. Our adaptation of Kerberos introduces a proxy server between the client and the server to mitigate potential performance deficiencies and add functional benefits. Our analysis indicates that assistance from the proxy makes public key Kerberos a viable authentication protocol from a performance perspective. However, as wireless network speeds increase from current 2G levels to the 3G targets, the proxy can become a response time liability. The proxy\'s role in the protocol, while warranted in current applications, will have to be re-modeled and re-considered as both wireless transmission speeds and proxy processing speeds increase.',47091,'CCS \'01 Proceedings of the 8th ACM conference on Computer and Communications Security',3),(345540,NULL,'2002',NULL,'Negotiation-based protocols for disseminating information in wireless sensor networks','In this paper, we present a family of adaptive protocols, called SPIN (Sensor Protocols for Information via Negotiation), that efficiently disseminate information among sensors in an energy-constrained wireless sensor network. Nodes running a SPIN communication protocol name their data using high-level data descriptors, called meta-data. They use meta-data negotiations to eliminate the transmission of redundant data throughout the network. In addition, SPIN nodes can base their communication decisions both upon application-specific knowledge of the data and upon knowledge of the resources that are available to them. This allows the sensors to efficiently distribute data given a limited energy supply. We simulate and analyze the performance of four specific SPIN protocols: SPIN-PP and SPIN-EC, which are optimized for a point-to-point network, and SPIN-BC and SPIN-RL, which are optimized for a broadcast network. Comparing the SPIN protocols to other possible approaches, we find that the SPIN protocols can deliver 60% more data for a given amount of energy than conventional approaches in a point-to-point network and 80% more data for a given amount of energy in a broadcast network. We also find that, in terms of dissemination rate and energy usage, the SPIN protocols perform close to the theoretical optimum in both point-to-point and broadcast networks.',266374,'Wireless Networks - Selected Papers from Mobicom\'99',3),(345575,NULL,'2001',NULL,'Designing a Modern Memory Hierarchy with Hardware Prefetching','In this paper, we address the severe performance gap caused by high processor clock rates and slow DRAM accesses. We show that, even with an aggressive, next-generation memory system using four Direct Rambus channels and an integrated one-megabyte level-two cache, a processor still spends over half its time stalling for L2 misses. Our experimental analysis begins with an effort to tune our baseline memory system aggressively: incorporating optimizations to reduce DRAM row buffer misses, reordering miss accesses to reduce queuing delay, and adjusting the L2 block size to match each channel organization. We show that there is a large gap between the block sizes at which performance is best and at which miss rate is minimized. Using those results, we evaluate a hardware prefetch unit integrated with the L2 cache and memory controllers. By issuing prefetches only when the Rambus channels are idle, prioritizing them to maximize DRAM row buffer hits, and giving them low replacement priority, we achieve a 65 percent speedup across 10 of the 26 SPEC2000 benchmarks, without degrading the performance of the others. With eight Rambus channels, these 10 benchmarks improve to within 10 percent of the performance of a perfect L2 cache.',117351,'IEEE Transactions on Computers',4),(353073,NULL,'2000',NULL,'Real-Time Systems and Software','From the Publisher:Emphasizing concepts and principles, this book provides readers with an accessible approach to software design. It presents several examples of commercial and research systems throughout the chapters to explain and justify the concepts. And the material presented is technically diverse, including discussions of state machines, logic, concurrent programming, and scheduling algorithms.',202777,'Real-Time Systems and Software',5),(369504,NULL,'1989',NULL,'Software Engineering','From the Book:Preface: Software systems are now ubiquitous. Virtually all electrical equipment now includes some kind of software; software is used to help run manufacturing industry, schools and universities, health care, finance and government; many people now use software of different kinds for entertainment and education. The specification, development, management and evolution of these software systems make up the discipline of software engineering. Even simple software systems have a high inherent complexity so engineering principles have to be used in their development. Software engineering is therefore an engineering discipline where software engineers use methods and theory from computer science and apply this cost-effectively to solve difficult problems. These difficult problems have meant that many software development projects have not been successful. However, most modern software provides good service to its users; we should not let high-profile failures obscure the real successes of software engineers over the past 30 years. Books inevitably reflect the opinions and prejudices of their authors. Some readers will inevitably disagree with my opinions and with the choice of material which I include. Such disagreement is a healthy reflection of the diversity of the discipline and is essential for its evolution. Nevertheless, I hope that all software engineers and software engineering students can find something of interest here. Although the book is intended as a general introduction to software engineering, it is biased, to some extent, towards my own interests in system requirements engineering and critical systems. I think these areparticularlyimportant for software engineering in the 21st century where the challenge we face is to ensure that our software meets the real needs of its users without causing damage to them or to the environment. I dislike zealots of any kind, whether they are academics preaching the benefits of formal methods or salesmen trying to convince me that some tool or method is the answer to software development problems. There are no simple solutions to the problems of software engineering and we need a wide spectrum of tools and techniques to solve software engineering problems. I therefore don\'t describe commercial design methods or CASE systems but paint a broad picture of software engineering methods and tools. Software engineering research has made tremendous strides over the past 15 years but there has been a relatively slow diffusion of this research into industrial practice. The principal challenge which we now face is not the development of new techniques and methods but the transfer of advanced software engineering research into everyday use. I see this book as a contributor to this process. I therefore discuss some techniques, such as viewpoints for requirements engineering, which are reasonably well developed but which are not yet widely used in industry. Finally, it is impossible to over-emphasize the importance of people in the software engineering process. People specify, design and implement systems which help other people with their work. Most of the difficulties of very large system engineering are not technical problems but are the problems of managing large numbers of people with diverse priorities, abilities and interests. Software engineering techniques and tools are only effective when applied in a context which respects these different skills and abilities. Changes from the fourth edition Like many software systems, this book has grown and changed since its first edition was published in 1982. This latest edition started as a relatively minor update of the fourth edition but, in the course of writing the book, I decided that more significant revision and re-engineering was necessary. Although much of the material in the fourth edition has been retained, the following changes have been made: There are five completely new chapters covering computer-based system engineering, requirements analysis, architectural design, process improvement and software re-engineering. The book has been restructured into eight parts covering an introduction to software engineering, requirements and specification, design, dependable systems development, verification and validation, CASE, management, and software evolution. There have been radical revisions of the material on requirements engineering, object-oriented and functional design, and CASE. Project management is introduced in the first part of the book then covered in more detail in a separate section which incorporates previous material on human factors. There is more emphasis on quality management. In previous editions, I have presented program examples in Ada as I consider this an excellent language for large-scale software engineering. However, Ada has not become as widely used as was once predicted. C or C++ are the programming languages of choice for most personal computer and workstation applications. Because of this wide use, I have included C++ as well as Ada versions of most of the program examples in the book. For safety-critical systems, however, I think it unwise to use a language which includes potentially unsafe constructs. Those examples are, therefore, only presented in Ada. I considered for a long time whether it would be appropriate to include a new chapter on professional and ethical issues. I decided not to do so because the topic is so subjective that it is difficult to present in a balanced way in a single chapter. There are no absolutes in this area and it is best addressed in an interactive context rather than as a chapter of a book. However, I have included a brief discussion of these issues in the introduction to the book. I have also included possible ethical and professional topics for discussion as exercises in many chapters. Links to WWW pages on this topic are included in the Web page whose URL is given below. The further reading associated with each chapter has been updated from previous editions. However, in many cases, articles written in the 1980s are still the best introduction to some topics. As new articles which are useful become available, I will include them on the Web page. The author index in previous editions has been removed. Rather, each entry in the References section includes the page numbers where it has been referenced. Readership The book is aimed at students in undergraduate and graduate courses and at software engineers in commerce and industry. It may be used in general software engineering courses or in courses such as advanced programming, software specification, software design or management. Practitioners may find the book useful as general reading and as a means of updating their knowledge on particular topics such as requirements engineering, architectural design, dependable systems development and process improvement. Wherever practicable, the examples in the text have been given a practical bias to reflect the type of applications which software engineers must develop. I assume that readers have a basic familiarity with programming and modern computer systems. Some examples rely on knowledge of basic data structures such as stacks, lists and queues. The chapters on formal specification assume knowledge of very elementary set theory. No other mathematical background is required. Using the book as a course text There are three main types of software engineering courses where this book can be used: General introductory courses in software engineering. For students who have no previous software engineering experience, you can start with the introductory section then pick and choose the introductory chapters from the different sections of the book. This will give students a general overview of the subject with the opportunity of more detailed study for those students who are interested. Introductory or intermediate courses on specific topics in software engineering such as software specification, design or dependable systems development. Each of the parts in the book can serve as a text in its own right for an introductory or intermediate course on that topic. More advanced courses in specific software engineering topics. In this case, the chapters in the book form a foundation for the course which must be supplemented with further reading which explores the topic in more detail. All chapters include my suggestions for further reading. The benefit of a general text like this is that it can be used in several different related courses. At Lancaster, we use the text in an introductory software engineering course, in courses on specification, design and critical systems and in a software management course where it is supplemented with further reading. With a single text, students are presented with a consistent view of the subject. They also like the extensive coverage because they don\'t have to buy several different books. This book covers all suggested material in Units SE2 to SE5 in the ACM/IEEE 1991 Curriculum. It also includes material to supplement an introductory programming text which would normally cover Unit SE1 and all material in the suggested course entitled \'Advanced Software Engineering\'. Supplements The following supplements are available: An instructor\'s guide including hints on teaching the material in each chapter, class and term project suggestions, and solutions to some of the exercises. This is available in Postscript or on paper from Addison-Wesley. A set of overhead projector transparencies for each chapter. These are available in Postscript and in Microsoft Powerpoint format. Source code for most of the individual program examples including supplementary code required for compilation. An introduction to the Ada programming language. Information on course presentation using electronically mediated communication and links to material for that approach to teaching. These are available, free of charge, over the Internet at URL: ...',NULL,'Software Engineering',5),(370028,NULL,'1988',NULL,'Object-Oriented Software Construction','From the Publisher:Object-Oriented Software Construction, second edition is the comprehensive reference on all aspects of object technology, from design principles to Object-Oriented techniques, Design by Contract, Object-Oriented analysis, concurrency, persistence, abstract data types and many more. Written by a pioneer in the field, contains an in-depth analysis of both methodological and technical issues.Two-color printing provides for clear figures and readable software extracts. Comes with a CD-ROM containing: the complete hyperlinked text, for easy reference; software to read the text on major industry platforms; supplementary material (reusable components, mathematical complements); and a complete graphical Object-Oriented development environment supporting the concepts of the book.',170278,'Object-Oriented Software Construction',5),(379868,NULL,'1995',NULL,'Advanced Topics in Dataflow Computing and Multithreading','From the Publisher:Examines recent advances in design, modeling, and implementation of dataflow and multithreaded computers. The text contains reports concerning many of the world\'s leading projects engaged in the continuing evolution and application of dataflow concepts. It covers the broad range of dataflow principles in program representation - from language design to processor architecture - and compiler optimization techniques. The book includes papers on massively parallel distributed memory and multithreaded architecture design, synchronization and pipelined design, and superpipelined data-driven VLSI processors. Other sections discuss stream data types, the development of well-structured software, and parallelization of dataflow programs. It also details an analytical model for the behavior of dataflow graphs, compares a centralized work distribution scheme with a distributed scheme, and presents a comprehensive approach to understanding workload management schemes. Altogether, the text introduces the reader to dataflow concepts that show how functional programming ideas can be harnessed to exploit the power of parallel computing.',20114,'Advanced Topics in Dataflow Computing and Multithreading',5),(383409,NULL,'1997',NULL,'Advanced Computer Architectures','From the Publisher:This timely book provides an unconventional and up-to-date overview of all the important computer architectures and is one of the first texts to present all the relevant concepts of advanced architecture classes by exploring their design spaces. Advanced Computer Architectures will prove an indispensable guide for anyone who needs to be acquainted with the relevant concepts and solutions introduced in recent years to the dramatically changing world of computer architecture. For the student of advanced level courses in computer architecture, this book will provide a comprehensive and accessible overview of the subject whilst its strong practical orientation will make it an invaluable reference for the practitioner.',19387,'Advanced Computer Architectures',5),(390489,NULL,'2000',NULL,'DRAM Circuit Design: A Tutorial','From the Publisher:DRAM (Dynamic Random Access Memory) circuits are the most manufactured integrated circuits in production with annual sales of $25 billion. The future will bring embedding DRAM with data processors for complete systems on a chip. This has increased the number of design engineers doing DRAM design. DRAM Circuit Design: A Tutorial teaches the introductory-level design of DRAM memory chips. Topics covered include: DRAM Array, The Peripheral Circuitry, Global Circuitry and Considerations, Voltage Converters, and synchronization in DRAMs.',80690,'DRAM Circuit Design: A Tutorial',10),(394043,NULL,'1998',NULL,'PC 99 System Design Guide','From the Publisher:Offering a wealth of details about design specifications and guidelines for upcoming PCs, this title is a technical reference for designing PCs and peripherals for the Microsoft Windows family of operating systems.',180387,'PC 99 System Design Guide',5),(395143,NULL,'1994',NULL,'Advanced Concepts in Operating Systems','From the Publisher:Operating systems have evolved substantially over the past two decades,and there is a need for a book which can explain major developments and changes in this dynamic field. This is such a book. Comprehensive,and useful as a text and reference,Advanced Concepts in Operating Systems lays down all the concepts and mechanisms involved in the design of advanced operating systems. The discussion is reinforced by many examples and cases',19406,'Advanced Concepts in Operating Systems',5),(405130,NULL,'1990',NULL,'Computer Organization','From the Publisher:Always praised for its comprehensive yet accessible treatment and real system examples. The book highlights modern developments in computer design,I/O and performance.',NULL,'Computer Organization',5),(408272,NULL,'1980',NULL,'Fitting Equations to Data: Computer Analysis of Multifactor Data','From the Publisher:Helps any serious data analyst with a computer to recognize the strengths and limitations of data, to test the assumptions implicit in the least squares methods used to fit the data, to select appropriate forms of the variables, to judge which combinations of variables are most influential, and to state the conditions under which the fitted equations are applicable. This edition includes numerous extensions and new devices such as component and component-plus-residual plots, cross verification with a second sample, and an index of required x-precision; also, the search for better subset equations is enlarged to cover 262,144 alternatives. The methods described have been applied in agricultural, environmental, management, marketing, medical, physical, and social sciences. Mathematics is kept to the level of college algebra.',98074,'Fitting Equations to Data: Computer Analysis of Multifactor Data',2),(427553,NULL,'2000',NULL,'Simulation based test generation for scan designs','We describe a simulation-based test generation procedure for scan designs. A test sequence generated by this procedure consists of a sequence of one or more primary input vectors embedded between a scan-in operation and a scan-out operation. We consider the set of faults that can be detected by test sequences of this form, compared to the case where scan is applied with every test vector. The proposed procedure constructs test sequences that traverse as many pairs of fault-free/faulty states as possible, and thus avoids the use of branch-and-bound test generation techniques. Additional techniques are incorporated into this basic procedure to enhance its effectiveness.',190875,'Proceedings of the 2000 IEEE/ACM international conference on Computer-aided design',11),(430945,NULL,'2002',NULL,'Robust Adaptive Metrics for Deadline Assignment in Distributed Hard Real-Time Systems','Distributed real-time applications usually consist of several component tasks and must be completed by its end-to-end (E-T-E) deadline. As long as the E-T-E deadline of an application is met, the strategy used for dividing it up for component tasks does not affect the application itself. One would therefore like to “slice” each application E-T-E deadline and assign the slices to component tasks so as to maximize the schedulability of the component tasks, and hence the application. Distribution of the E-T-E deadline over component tasks is a difficult and important problem since there exists a circular dependency between deadline distribution and task assignment. We propose a new deadline-distribution scheme which has two major improvements over the best scheme known to date. It can distribute task deadlines prior to task assignment and relies on new adaptive metrics that yield significantly better performance in the presence of high resource contention. The deadline-distribution problem is formulated for distributed hard real-time systems with relaxed locality constraints, where schedulability analysis must be performed at pre-run-time, and only a subset of the tasks are constrained by pre-assignment to specific processors. Although it is applicable to any scheduling policy, the proposed deadline-distribution scheme is evaluated for a non-preemptive, time-driven scheduling policy. Using extensive simulations, we show that the proposed adaptive metrics deliver much better performance (in terms of success ratio and maximum task lateness) than their non-adaptive counterparts. In particular, the simulation results indicate that, for small systems, the adaptive metrics can improve the success ratio by as much as an order of magnitude. Moreover, the new adaptive metrics are found to exhibit very robust performance over a large variety of application and architecture scenarios.',202759,'Real-Time Systems',0),(433317,NULL,'2002',NULL,'Two-Level Pointer Forwarding Strategy for Location Management in PCS Networks','For a PCS network to effectively deliver services to its mobile users, it must have an efficient way to keep track of the mobile users. The location management fulfills this task through location registration and paging. To reduce the signaling traffic, many schemes such as Local Anchor (LA) scheme, per-user caching scheme and pointer forwarding scheme have been proposed in the past. In this paper, we present a new location management scheme which intends to mitigate the signaling traffic as well as reduce the tracking delay in the PCS systems. In this strategy, we choose a set of VLRs traversed by users as the Mobility Agents (MA), which form another level of management in order to make some registration signaling traffic localized. The idea is as follows: Instead of always updating to the HLR, which would become the bottleneck otherwise, many location updates are carried out in the mobility agents. Thus, the two-level pointer forwarding scheme is designed to reduce the signaling traffic: Pointers can be set up between VLRs as the traditional pointer forwarding scheme and can also be set up between MAs. The numerical results show that this strategy can significantly reduce the network signaling traffic for users with low CMR without increasing much of the call setup delay.',117395,'IEEE Transactions on Mobile Computing',3),(434375,NULL,'2000',NULL,'Optimizing the Dissemination of Mobile Agents for Distributed Information Filtering','The promise of reduced communication costs-decreased application latency or reduced network load-often motivates programmers to use mobile agent technology. The authors analyze data distributed among various remote data servers using one filter method and present an approach for coordinating mobile agent dissemination that minimizes communication costs. Trace-driven validation studies for various constellations show that cost savings of up to 90% can be achieved under Internet conditions.',117207,'IEEE Concurrency',3),(437354,NULL,'1995',NULL,'Fault Injection','Fault-injection involves the deliberate insertion of faults or errors into a computer system in order to determine its response. It has proven to be an effective method for measuring the parameters of analytical dependability models, validating existing fault-tolerant systems, synthesizing new fault-tolerant designs, and observing how systems behave in the presence of faults. Growing dependence on computers in life- and cost-critical applications makes it increasingly important to understand and utilize this technique. This paper motivates the use of fault- injection and develops a taxonomy for interpreting fault-injection experiments. Background on how faults affect computer systems is provided. Results from several recent fault-injection studies are reviewed. Tools that facilitate the use of fault-injection are examined, and areas for future research are discussed.',56357,'Computer',6),(438973,NULL,'1995',NULL,'Single-Clock Partial Scan','Existing flip-flop selection and test generation methods for partial scan designs assume the use of a separate scan clock. With a separate clock for the scan operation, the states of the non-scan flip-flops can be frozen during the scan operation and any state can be scanned into the scan register without affecting the states of the non-scan flip-flops. Under this assumption, test vectors can be efficiently generated by a sequential circuit test generator. However, this requirement results in the need for multiple clock trees and posts problems in routing of clock signals that, in general, are subject to a tight constraint on clock skew.In this article, we lift this assumption and address the problems of test generation, scan flip-flop selection and ordering of scan registers for partial scan designs that use the system clock for the scan operation. An existing test generation algorithm is modified to incorporate the scan-shifting concept for such designs. This modified test generation scheme also implicitly compacts the test vectors and results in significantly reduction in the length of test vectors as shown by our experimental results on ISCAS89 benchmark circuits. The test vectors generated by this method can be applied at speed and thus can also detect delay faults. We show that cycle breaking remains to be an effective heuristic for scan flip-flop selection for our test generation method. The ordering of scan flip-flops in the scan chain affects the final fault coverage for the new test generation method. We formulate the ordering problem as a combinatorial optimization problem. Experimental results on test generation and ordering of scan flip-flops are presented to show the effectiveness of our approaches.',117210,'IEEE Design & Test',11),(438999,NULL,'1995',NULL,'Testing Defects in Scan Chains','Applying scan-based DFT, I/sub DDQ/ testing, or both to sequential circuits does not ensure bridging-fault detection, which depends on the resistance of the fault and circuit level parameters. With a \"transparent\" scan chain, however, the tester can use both methods to detect manufacturing process defects effectively-including difficult-to-detect shorts in the scan chain. The author presents a strategy for making the scan chain transparent. The test complexity of such a chain is very small, regardless of the number of flip-flops it contains.',117210,'IEEE Design & Test',11),(439861,NULL,'1994',NULL,'Transient Fault Tolerance in Digital Systems','It is hard to shield systems effectively from transient faults (fault avoidance techniques). So some other means must be employed to assure appropriate levels of transient fault tolerance (insensitivity to transient faults). They are based on fault-masking and fault recovery ideas. Having analyzed this problem, the author identifies critical design points and outlines some practical solutions that refer to efficient on-line detectors (detecting errors during the system operation) and error handling procedures. This framework provides a basis for understanding transient fault problems in digital systems. It can be helpful in selecting optimum techniques to mask or eliminate transient fault effects in developed systems.',117256,'IEEE Micro',6),(439873,NULL,'1994',NULL,'Predicting and Precluding Problems with Memory Latency','By examining the rate at which successive generations of processor and DRAM cycle times have been diverging over time, we can track the latency problem of computer memory systems. Our research survey starts with the fundamentals of single-level caches and moves to the need for multilevel cache hierarchies. We look at some of the current techniques for boosting cache performance, especially compiler-based methods for code restructuring and instruction and data prefetching. These two areas will likely yield improvements for a much larger domain of applications in the future.',117256,'IEEE Micro',4),(440012,NULL,'1996',NULL,'MicroUnity\'s MediaProcessor Architecture','A broadband mediaprocessor extends and streamlines a general-purpose computer system to attain the goal of communicating and processing digital video, audio, data and RF signals at broadband rates using compiled, downloadable, software rather than special-purpose hardware. The instruction set, system facilities, and initial implementations of an architectural family of broadband mediaprocessors are introduced, and compiled software development is illustrated with an example and description of the development environment.',117256,'IEEE Micro',5),(442014,NULL,'1992',NULL,'A Systolic Algorithm for the k-Nearest Neighbors Problem','The authors present a systolic algorithm and its variations for the k-nearest neighbors problem (kNNP). Multiple-shot queries with different ranges (k values) can be served in a pipelined fashion. A partitioning scheme is developed to handle large size problems. Performance of the algorithm is analyzed. Formulas for the optimal array size in terms of computation time and area-time-time product (ATT) are derived. The algorithm can solve a multiple-shot kNNP in N+2 square root N*K systolic steps using square root N*K processing elements, where N is the problem size (i.e. the number of points), and K is the sum of all k-values.',117351,'IEEE Transactions on Computers',9),(442015,NULL,'1992',NULL,'A Fault-Tolerant Communication Scheme for Hypercube Computers','A fault-tolerant communication scheme that facilitates near-optimal routing and broadcasting in hypercube computers subject to node failures is described. The concept of an unsafe node is introduced to identify fault-free nodes that may cause communication difficulties. It is shown that by only using \'feasible\' paths that try to avoid unsafe nodes, routing and broadcasting can be substantially simplified. A computationally efficient routing algorithm that uses local information is presented. It can route a message via a path of length no greater than p+2, where p is the minimum distance from the source to the destination, provided that not all nonfaulty nodes in the hypercube are unsafe. Broadcasting can be achieved under the same fault conditions with only one more time unit than the fault-free case. The problems posed by deadlock in faulty hypercubes are discussed, and deadlock-free implementations of the proposed communication schemes are presented.',117351,'IEEE Transactions on Computers',7),(442024,NULL,'1988',NULL,'Your Favorite Parallel Algorithms Might Not Be as Fast as You Think','A problem that requires I inputs, K outputs and I computations is to be solved on a d-dimensional parallel processing machine (usually dor=3). Finite transmission speed and other real-world conditions are assumed. It is proved that the time needed to solve the problem is t= Omega /sub max/ (I/sup 1/d/, K/sup 1/d/, T/sup 1/(d+1)/). This result is demonstrated for the standard algorithm for multiplying two n*n matrices.',117351,'IEEE Transactions on Computers',9),(442031,NULL,'1988',NULL,'DPM: A Measurement System for Distributed Programs','A framework for measuring the performance of distributed programs is presented. This framework includes a model of distributed programs, a description of the measurement principles and methods, and a guideline for implementing these ideas. The author describes a measurement system called the Distributed Programs Monitor (DPM), which he has constructed on the basis of these concepts. DPM has been implemented and used for measurement studies on two different operating systems, DEMOS/MP and Berkeley Unix.',117351,'IEEE Transactions on Computers',5),(442050,NULL,'1988',NULL,'The Universality of a Class of Modified Single-Stage Shuffle/Exchange Networks','A class of modified single-state shuffle/exchange (S/E) networks with reconfigurable connections is described. The capability of these modified networks to simulate every arbitrary permutation and other multistage S/E networks is studied. In particular, the upper bounds on simulating six multistage S/E networks are given. It is also shown that C.L. Wu and T.Y. Feng\'s (1981) modified single-stage S/E network is optimal in terms of its universality and cost for the networks with one reconfigurable connection. However, some of the modified networks with two reconfigurable connections have better capability although they are slightly more expensive than Wu and Feng\'s network. Also proposed is a link folding scheme that can be used to reduce the connection complexity of implementing the connection selection mechanism.',117351,'IEEE Transactions on Computers',7),(442052,NULL,'1988',NULL,'Fault Tolerance in Multiprocessor Systems Without Dedicated Redundancy','An algorithm called RAFT (recursive algorithm for fault tolerance) for achieving fault tolerance in multiprocessor systems is described. Through the use of a combination of dynamic space- and time- redundancy techniques, RAFT achieves fault tolerance in the presence of permanent as well as intermittent faults. Performance and reliability of multiprocessor systems using RAFT are determined as a function of individual processor reliability and the total number of fault modes in a processor. RAFT-based systems are superior to triple modular redundancy (TMR) systems in hardware economy and provide comparable reliability. A multiprocessor architecture adopting RAFT is given.',117351,'IEEE Transactions on Computers',6),(442057,NULL,'1992',NULL,'On-the-Fly Rounding (Computing Arithmetic)','In implementations of operations based on digit-recurrence algorithms such as division, left-to-right multiplication and square root, the result is obtained in digit-serial form, from most significant digit to least significant. To reduce the complexity of the result-digit selection and allow the use of redundant addition, the result-digit has values from a signed-digit set. As a consequence, the result has to be converted to conventional representation, which can be done on-the-fly as the digits are produced, without the use of a carry-propagate adder. The authors describe three ways to modify this conversion process so that the result is rounded. The resulting operation is fast because no carry-propagate addition is needed. The schemes described apply also to online arithmetic operations.',117351,'IEEE Transactions on Computers',8),(442101,NULL,'1993',NULL,'Reducing Iteration Time When Result Digit is Zero for Radix 2 SRT Division and Square Root with Redundant Remainders','A new architecture is presented for shared radix 2 division and square root whose main characteristic is the ability to avoid any addition/subtraction, when the digit 0 has been selected. The solution presented uses a redundant representation of the partial remainder, while keeping the advantages of classical solutions. It is shown how the next digit of the result can be selected even when the remainder is not updated, and the subsequent tradeoff is presented. The proposed architecture is also extended in order to consider other implementations.',117351,'IEEE Transactions on Computers',8),(442117,NULL,'1993',NULL,'Systolic Modular Multiplication','A systolic array for modular multiplication is presented using the ideally suited algorithm of P.L. Montgomery (1985). Throughput is one modular multiplication every clock cycle, with a latency of 2n+2 cycles for multiplicands having n digits. Its main use would be where many consecutive multiplications are done, as in RSA cryptosystems.',117351,'IEEE Transactions on Computers',8),(442119,NULL,'1993',NULL,'A Polynomial Time Algorithm for Reconfiguring Multiple-Track Models','A polynomial time algorithm for solving the combinatorial problem that underlies the reconfiguration issues in the m1/2-track-m-spare model, for any arbitrary m, is discussed. The following combinatorial problem is solved: Given a set of points in a two-dimensional grid, find a set of noninteracting straight lines such that every line starts at a point and connects to one of the boundaries of the grid, there are no more than m lines overlapping in any row or column of the grid, and there are no near-miss situations. The time complexity of the algorithm is shown to be O(m mod F mod /sup 2/), where mod F is the number of faulty processors.',117351,'IEEE Transactions on Computers',9),(442137,NULL,'1993',NULL,'Rearrangeable Three-Stage Interconnection Networks and Their Routing Properties','A rearrangeable network is an interconnection network which can achieve all possible permutations of its inputs\' connections to its outputs. One class of rearrangeable networks, which have been studied are Clos three-stage networks. Earlier procedures to route such networks rapidly require an excessive amount of hardware, either in the network itself or in the device required to compute the routing. A class of rearrangeable three-stage networks which is both compact and which can be routed quickly is introduced, along with a routing scheme. Switches are added to the network so as to reduce the interdependence between the switch settings, allowing faster routing while only moderately increasing network complexity. A network with O(Nlog/sup 1.5/N) hardware and O(log/sup 1.5/N) depth is derived from O(log/sup 2.5/N) switch setup time.',117351,'IEEE Transactions on Computers',7),(442159,NULL,'1993',NULL,'Optimal and Efficient Probabilistic Distributed Diagnosis Schemes','The distributed self-diagnosis of a multiprocessor/multicomputer system based on interprocessor tests with imperfect fault coverage that permits intermittently faulty processors is addressed. Focusing on probabilistic diagnosis methods, the authors define several different categories of probabilistic diagnosis based on the type of fault syndrome information used in the diagnosis. Rigorous probabilistic analysis is then used to derive diagnosis algorithms optimal in terms of diagnostic accuracy for the diagnosis categories introduced. Analysis and simulations are used to evaluate the performance of the diagnosis algorithms introduced.',117351,'IEEE Transactions on Computers',6),(442167,NULL,'1993',NULL,'Some Fundamental Properties of Multiple-Valued Kleenean Functions and Determination of Their Logic Formulas','Multiple-valued Kleenean functions that are models of a Kleene algebra and are logic functions expressed by logic formulas composed of variables, constants, and logic operations AND OR, and NOT are discussed. The set of Kleenean functions, is a model with the largest number of logic functions among existing models of a Kleene algebra, such as fuzzy logic functions, regular ternary logic functions, and B-ternary logic functions. Mainly, it is shown that any p-valued Kleenean function is derived from a monotonic ternary input functions and any p-valued unate function is derived from a unate binary input function. The mapping relations between them and the method to determine the logic formula of the Kleenean function and unate function from that of the monotonic ternary input function and unate binary input function, respectively, are classified. 7-or-less-valued Kleenean functions and unate functions of 3-or-fewer variables are enumerated. It is known that the number of p-valued Kleenean functions increases stepwise and that of unate functions increases smoothly as p becomes larger.',117351,'IEEE Transactions on Computers',1),(442169,NULL,'1993',NULL,'Test-Pattern Generation Based on Reed-Muller Coefficients','Reed-Muller coefficients are used to generate a test-pattern selection procedure for detecting single stuck-at faults. This procedure is based on the heuristics deduced from the way in which the spectral coefficients are affected by such faults. The changes that the spectral coefficients undergo are also compared with the fault models used most frequently to model defects in combinational circuits. The proposed pattern-generating procedure does not need a simulation of the circuit for each of the possible stuck-at faults, and its complexity is proportional to the number of gates in the circuit. The proportionality constant increases exponentially as the number of inputs in the circuit increases. To evaluate the performance of the proposed method, its application to some benchmark circuits, including the ALU 74181, is presented.',117351,'IEEE Transactions on Computers',11),(442183,NULL,'1993',NULL,'Deadlock-Free Fault-Tolerant Routing in Injured Hypercubes','Wormhole routing with the e-cube algorithm is an excellent solution for deadlock-free interprocess communication in healthy hypercubes. However, it does not work for injured hypercubes where some nodes and/or links are faulty. The authors propose a new deadlock-free routing scheme in an injured hypercube with the wormhole routing capability. All previously proposed schemes suggest the use of virtual channels to avoid the cycle of resource dependency. By contrast, the authors\' scheme is based on the re-establishment of a routing path to the destination, but it does not always yield a shortest path between the source and destination. The proposed routing scheme uses either wormhole routing or staged routing, depending on the availability of one or more healthy (n-2)-cubes within an injured n-cube.',117351,'IEEE Transactions on Computers',7),(442192,NULL,'1993',NULL,'The Effect of Code Expanding Optimizations on Instruction Cache Design','Shows that code expanding optimizations have strong and nonintuitive implications on instruction cache design. Three types of code expanding optimizations are studied in this paper: instruction placement, function inline expansion, and superscalar optimizations. Overall, instruction placement reduces the miss ratio of small caches. Function inline expansion improves the performance for small cache sizes, but degrades the performance of medium caches. Superscalar optimizations increase the miss ratio for all cache sizes. However, they also increase the sequentiality of instruction access so that a simple load forwarding scheme effectively cancels the negative effects. Overall, the authors show that with load forwarding, the three types of code expanding optimizations jointly improve the performance of small caches and have little effect on large caches.',117351,'IEEE Transactions on Computers',4),(442332,NULL,'1994',NULL,'Very-High Radix Division with Prescaling and Selection by Rounding','A division algorithm in which the quotient-digit selection is performed by rounding the shifted residual in carry-save form is presented. To allow the use of this simple function, the divisor (and dividend) is prescaled to a range close to one. The implementation presented results in a fast iteration because of the use of carry-save forms and suitable recodings. The execution time is calculated and several convenient values of the radix are selected. Comparison with other dividers for radices 2/sup 9/ to 2/sup 18/ is performed using the same assumptions.',117351,'IEEE Transactions on Computers',8),(442357,NULL,'1994',NULL,'Performance Evaluation of a Decoded Instruction Cache for Variable Instruction Length Computers','A Decoded INstruction Cache (DINC) is a buffer between the instruction decoder and other instruction pipeline stages. In this paper, we explain how techniques that reduce the branch penalty on a DINC, can improve CPU performance. We also analyze the impact of some of the design parameters of DINC\'s on variable instruction length computers. Our study indicates that tuning the mapping of the instructions into the cache can improve performance substantially. Tuning must be based on the instruction length distribution for a specific architecture. In addition, the associativity degree has a greater effect on the DINC\'s performance than on the performance of regular caches. We discuss the difference between the performance of DINC\'s and other caches, when longer cache lines are used. We present a model to estimate the miss rate based on its characteristics, that are discussed and analyzed throughout this paper. Our conclusions are based on both analytical study and trace driven simulations of several integer UNIX applications.',117351,'IEEE Transactions on Computers',4),(442391,NULL,'1995',NULL,'On Computing Connected Components of Line Segments','It is shown that given a set of n line segments, their connected components can be computed in time O(n4/3log3n). A bound of o(n4/3) for this problem would imply a similar bound for detecting, for a given set of n points and n lines, whether some point lies on some of the lines. This problem, known as Hopcroft驴s problem, is believed to have a lower bound of 驴(n4/3). For the special case when for each segment both endpoints fall inside the same face of the arrangement induced by the set of segments, we give an algorithm that runs in O(nlog 3n) time.',117351,'IEEE Transactions on Computers',9),(442394,NULL,'1995',NULL,'Effective Hardware-Based Data Prefetching for High-Performance Processors','Memory latency and bandwidth are progressing at a much slower pace than processor performance. In this paper, we describe and evaluate the performance of three variations of a hardware function unit whose goal is to assist a data cache in prefetching data accesses so that memory latency is hidden as often as possible. The basic idea of the prefetching scheme is to keep track of data access patterns in a Reference Prediction Table (RPT) organized as an instruction cache. The three designs differ mostly on the timing of the prefetching. In the simplest scheme (basic), prefetches can be generated one iteration ahead of actual use. The lookahead variation takes advantage of a lookahead program counter that ideally stays one memory latency time ahead of the real program counter and that is used as the control mechanism to generate the prefetches. Finally the correlated scheme uses a more sophisticated design to detect patterns across loop levels.These designs are evaluated by simulating the ten SPEC benchmarks on a cycle-by-cycle basis. The results show that 1) the three hardware prefetching schemes all yield significant reductions in the data access penalty when compared with regular caches, 2) the benefits are greater when the hardware assist augments small on-chip caches, and 3) the lookahead scheme is the preferred one cost-performance wise.',117351,'IEEE Transactions on Computers',4),(442401,NULL,'1995',NULL,'Frames: A Simple Characterization of Permutations Realized by Frequently Used Networks','Rearrangeable multistage networks such as the Benes network realize any permutation, yet their routing algorithms are not cost-effective. On the other hand, there exist inexpensive routing algorithms for nonrearrangeable networks, but no simple technique exists to characterize all the permutations realized on these networks. This paper introduces the concept of frame and shows how it can be used to characterize all the permutations realized on various multistage interconnection networks. They include subnetworks of baseline, Benes, and cascaded baseline and shuffle-exchange networks.',117351,'IEEE Transactions on Computers',7),(442418,NULL,'1995',NULL,'A Fast Radix-4 Division Algorithm and its Architecture','In this paper we present a fast radix-4 division algorithm for floating point numbers. This method is based on Svoboda驴s division algorithm and the radix-4 redundant number system. The algorithm involves a simple recurrence with carry-free addition and employs prescaling of the operands. In the proposed divider implementation, each radix-4 digit (belonging to set {驴3, 驴, +3}) of the quotient and partial remainder is encoded using two radix-2 digits (belonging to the set {驴1, 0, +1}) and this leads to hardware simplicity. The quotient digits are determined by observing three most-significant radix-2 digits of the partial remainder and independent of the divisor. The architecture presented for the proposed algorithm is faster than previously proposed radix-4 dividers, which require at least four digits of the partial remainder to be observed to determine quotient digits.',117351,'IEEE Transactions on Computers',8),(442496,NULL,'1995',NULL,'Fault-Tolerant Routing in Distributed Loop Networks','The ring network is a popular network topology for implementation in local area networks and other configurations. But it has a disadvantage of high diameter and large communication delay. So loop networks were introduced with fixed-jump links added over the ring. In this paper, we characterize some values for the number of nodes for which the lower bound on the diameter of loop networks is achieved. We also give an O(驴) time algorithm (where 驴 is the diameter of the graph) for finding a shortest path between any two nodes of a general loop network. We also propose a scheme to find a near optimal path (not more than one over the optimal) in case of a single node or link failure.',117351,'IEEE Transactions on Computers',7),(442503,NULL,'1996',NULL,'A Fault-Tolerant Routing Strategy in Hypercube Multicomputers','We investigate fault-tolerant routing which aims at finding feasible minimum paths in a faulty hypercube. The concept of unsafe node and its extension are used in our scheme. A set of stringent criteria is proposed to identify the possibly bad candidates for forwarding a message. As a result, the number of such undesirable nodes is reduced without sacrificing the functionality of the mechanism. Furthermore, the notion of degree of unsafeness for classifying the unsafe nodes is introduced to facilitate the design of efficient routing algorithms which rely on having each node keep the states of its nearest neighbors. We show that a feasible path of length no more than the Hamming distance between the source and the destination plus four can always be established by the routing algorithm as long as the hypercube is not fully unsafe. The issue of deadlock freeness is also addressed in this research. More importantly, another fault-tolerant routing algorithm, which requires only a constant of five virtual networks in wormhole routing to ensure the property of deadlock freeness for a hypercube of any size, is presented in this paper.',117351,'IEEE Transactions on Computers',7),(442510,NULL,'1996',NULL,'Transposition Networks as a Class of Fault-Tolerant Robust Networks','The paper proposes designs of interconnection networks (graphs) which can tolerate link failures. The networks under study belong to a subclass of Cayley graphs whose generators are subsets of all possible transpositions. We specifically focus on star and bubble-sort networks. Our approach is to augment existing dimensions (or generators) with one or more dimensions. If the added dimension is capable of replacing any arbitrary failed dimension, it is called a wildcard dimension. It is shown that, up to isomorphism among digits used in labeling the vertices, the generators of the star graph are unique. The minimum number of extra dimensions needed to acquire i wildcard dimensions is derived for the star and bubble-sort networks. Interestingly, the optimally augmented star network coincides with the Transposition network, Tn. Transposition networks are studied rigorously. These networks are shown to be optimally fault-tolerant. Tn is also shown to possess wide containers with short length. Fault-diameter of Tn is shown to be n. While the Tn can efficiently embed star and bubble-sort graphs, it can also lend itself to an efficient embedding of meshes and hypercubes.',117351,'IEEE Transactions on Computers',7),(442511,NULL,'1996',NULL,'Multiphase Complete Exchange: A Theoretical Analysis','Abstract-Complete Exchange requires each of N processors to send a unique message to each of the remaining N驴 1 processors. For a circuit switched hypercube with N = 2d processors, the Direct and Standard algorithms for Complete Exchange are time optimal for very large and very small message sizes, respectively. For intermediate sizes, a hybrid Multiphase algorithm is better. This carries out Direct exchanges on a set of subcubes whose dimensions are a partition of the integer d. The best such algorithm for a given message size m could hitherto only be found by enumerating all partitions of d.The Multiphase algorithm is analyzed assuming a high performance communication network. It is proved that only algorithms corresponding to equipartitions of d (partitions in which the maximum and minimum elements differ by at most one) can possibly be optimal. The run times of these algorithms plotted against m form a hull of optimality. It is proved that, although there is an exponential number of partitions, 1) the number of faces on this hull is $\\Theta \\left( {\\sqrt d} \\right)$, 2) the hull can be found in $\\Theta \\left( {\\sqrt d} \\right)$ time, and 3) once it has been found, the optimal algorithm for any given m can be found in 驴(log d) time.These results provide a very fast technique for minimizing communication overhead in many important applications, such as matrix transpose, fast Fourier transform, and alternating directions implicit (ADI).',117351,'IEEE Transactions on Computers',9),(442512,NULL,'1996',NULL,'A New Error Analysis Based Method for Tolerance Computation for Algorithm-Based Checks','Abstract-Algorithm-based techniques are based on checking for the preservation of certain properties possessed by global data following a set of computations. This often involves the introduction of a check variable which is updated in such a manner that, in the absence of roundoff errors, it equals the value of some function which involves all the data elements participating in the algorithm. However, the fact that roundoff errors accumulate in different ways in the updates involving the check variables and the computations involving data elements make it highly unlikely that the equality is preserved exactly for an implementation of the algorithm on a real computer. Thus, the check step involves verifying the preservation of the equality to within a tolerance value. In this brief contribution, we propose a method for determination of the tolerance based on error analysis techniques. We present results on three numerical algorithms which show the effectiveness of our approach for data sets of varying sizes and data ranges.',117351,'IEEE Transactions on Computers',2),(442515,NULL,'1996',NULL,'BIST Test Pattern Generators for Two-Pattern Testing-Theory and Design Algorithms','Testing for delay and CMOS stuck-open faults requires two-pattern tests, and typically a large number of two pattern tests are needed. Built-in self-test (BIST) schemes are attractive for comprehensive testing of such faults. BIST test pattern generators (TPGs) for two-pattern testing, should be designed to ensure high transition coverage. In this paper, necessary and sufficient conditions to ensure complete/maximal transition coverage for linear feedback shift register (LFSR) and cellular automata (CA) have been derived. The theory developed here identifies all LFSR/CA TPGs that maximize transition coverage under any given TPG size constraint. It is shown that LFSRs with primitive feedback polynomials with large number of terms are better for two-pattern testing. Also, CA are shown to be better TPGs than LFSRs for two pattern testing, independent of their feedback rules. Based on the necessary sufficient conditions, efficient algorithms to design optimal TPGs for two-pattern testing have been developed. Experiments on benchmark circuits indicate that TPGs designed using the procedures outlined in this paper obtain high robust path delay fault coverage in short test lengths.',117351,'IEEE Transactions on Computers',11),(442520,NULL,'1996',NULL,'GF(2m) Multiplication and Division Over the Dual Basis','In this paper an algorithm for GF(2m) multiplication/division is presented and a new, more generalized definition of duality is proposed. From these the bit-serial Berlekamp multiplier is derived and shown to be a specific case of a more general class of multipliers. Furthermore, it is shown that hardware efficient, bit-parallel dual basis multipliers can also be designed. These multipliers have a regular structure, are easily extended to different GF(2m) and hence suitable for VLSI implementations. As in the bit-serial case these bit-parallel multipliers can also be hardwired to carry out constant multiplication. These constant multipliers have reduced hardware requirements and are also simple to design. In addition, the multiplication/division algorithm also allows a bit-serial systolic finite field divider to be designed. This divider is modular, independent of the defining irreducible polynomial for the field, easily expanded to different GF(2m) and its longest delay path is independent of m.',117351,'IEEE Transactions on Computers',8),(442533,NULL,'1996',NULL,'On-Line Scheduling Policies for a Class of IRIS (Increasing Reward with Increasing Service) Real-Time Tasks','We consider a real-time task model where a task receives a \"reward\" that depends on the amount of service received prior to its deadline. The reward of the task is assumed to be an increasing function of the amount of service that it receives, i.e., the task has the property that it receives increasing reward with increasing service (IRIS). We focus on the problem of on-line scheduling of a random arrival sequence of IRIS tasks on a single processor with the goal of maximizing the average reward accrued per task and per unit time. We describe and evaluate several policies for this system through simulation and through a comparison with an unachievable upper bound. We observe that the best performance is exhibited by a two-level policy where the top-level algorithm is responsible for allocating the amount of service to tasks and the bottom-level algorithm, using the earliest deadline first (EDF) rule, is responsible for determining the order in which tasks are executed. Furthermore, the performance of this policy approaches the theoretical upper bound in many cases. We also show that the average number of preemptions of a task under this two-level policy is very small.',117351,'IEEE Transactions on Computers',0),(442534,NULL,'1996',NULL,'Distance-Constrained Scheduling and Its Applications to Real-Time Systems','In hard real-time systems, each task must not only be functionally correct but also meet its timing constraints. A common approach to characterizing hard real-time tasks with repetitive requests is the periodic task model [1]. In the periodic task model, every task needs to be executed once during each of its periods. The execution of a task in one period is independent of the execution of the same task in another period. Hence, the executions of the same task in two consecutive periods may be right next to each other, or at the far ends of the two periods. While the periodic task model can serve as a simple paradigm for scheduling tasks with repetitive requests, it may not be suitable for all real-time applications. For example, in some real-time systems, the temporal distance between the finishing times of any two consecutive executions of the same task must be less than or equal to a given value. In other words, each execution of a task has a deadline relative to the finishing time of the previous execution of the same task. Scheduling algorithms designed for the periodic task model may not provide efficient solutions for tasks with temporal distance constraints.In this paper, we propose the (preemptive) distance-constrained task system model which can serve as a more intuitive and adequate scheduling model for \"repetitive\" task executions. We design an efficient scheduling scheme for the model, and derive a schedulability condition for the scheduling scheme. The schedulability condition is a measure for providing the fundamental predictability requirement in hard real-time applications. To show the usefulness of the distance-constrained task model and its scheduling scheme, we also discuss how to apply the scheduling scheme to real-time sporadic task scheduling and to real-time communications.',117351,'IEEE Transactions on Computers',0),(442537,NULL,'1996',NULL,'A New Architecture for a Parallel Finite Field Multiplier with Low Complexity Based on Composite Fields','In this paper a new bit-parallel structure for a multiplier with low complexity in Galois fields is introduced. The multiplier operates over composite fields GF((2n)m), with k = nm. The Karatsuba-Ofman algorithm is investigated and applied to the multiplication of polynomials over GF(2n). It is shown that this operation has a complexity of order $O(k^{{\\rm log}_23})$ under certain constraints regarding k. A complete set of primitive field polynomials for composite fields is provided which perform modulo reduction with low complexity. As a result, multipliers for fields GF(2k) up to k = 32 with low gate counts and low delays are listed. The architectures are highly modular and thus well suited for VLSI implementation.',117351,'IEEE Transactions on Computers',8),(442543,NULL,'1997',NULL,'Skewed Associativity Improves Program Performance and Enhances Predictability','Performance tuning becomes harder as computer technology advances. One of the factors is the increasing complexity of memory hierarchies. Most modern machines now use at least one level of cache memory. To reduce execution stalls, cache misses must be very low. Software techniques used to improve locality have been developed for numerical codes, such as loop blocking and copying. Unfortunately, the behavior of direct mapped and set associative caches is still erratic when large data arrays are accessed. Execution time can vary drastically for the same loop kernel depending on uncontrolled factors such as array leading size. The only software method available to improve execution time stability is the copying of frequently used data, which is costly in execution time. Users are not usually cache organization experts. They are not aware of such phenomena and have no control over it.In this paper, we show that the recently proposed four-way skewed associative cache yields very stable execution times and good average miss ratios on blocked algorithms. As a result, execution time is faster and much more predictable than with conventional caches. It is therefore possible to use larger block sizes in blocked algorithms, which will further reduce blocking overhead costs.',117351,'IEEE Transactions on Computers',4),(442598,NULL,'2002',NULL,'Using Application Benefit for Proactive Resource Allocation in Asynchronous Real-Time Distributed Systems','This paper presents two proactive resource allocation algorithms, called RBA* and OBA, for asynchronous real-time distributed systems. The algorithms consider an application model where timeliness requirements are expressed using Jensen\'s benefit functions and propose adaptation functions to describe anticipated application workload during future time intervals. Furthermore, the algorithms consider an adaptation model, where application processes are dynamically replicated for sharing workload increases and a switched real-time Ethernet network as the underlying system model. Given such models, the objective of the algorithms is to maximize aggregate application benefit and minimize aggregate missed deadline ratio. Since determining the optimal allocation is computationally intractable, the algorithms heuristically compute near-optimal resource allocations in polynomial-time. While RBA* analyzes process response times to determine resource allocation decisions, which is computationally expensive, OBA analyzes processor overloads to compute its decisions in a much faster way. RBA* incurs a quadratic amortized complexity in terms of process arrivals for its most computationally intensive component when DASA is used as the underlying scheduling algorithm, whereas OBA incurs a logarithmic amortized complexity for the corresponding component. Our benchmark-driven experimental studies reveal that RBA* produces a higher aggregate benefit and lower missed deadline ratio than OBA.',117351,'IEEE Transactions on Computers',0),(444173,NULL,'1992',NULL,'Extended Hypercube: A Hierarchical Interconnection Network of Hypercubes','A new interconnection topology-the extended hypercube-consisting of an interconnection network of k-cubes is discussed. The extended hypercube is a hierarchical, expansive, recursive structure with a constant predefined building block. The extended hypercube retains the positive features of the k-cube at different levels of hierarchy and at the same time has some additional advantages like reduced diameter and constant degree of a node. The paper presents an introduction to the topology of the extended hypercube and analyzes its architectural potential in terms of message routing and executing a classof highly parallel algorithms. Topological properties and performance studies of the extended hypercube are presented.',117407,'IEEE Transactions on Parallel and Distributed Systems',7),(444235,NULL,'1993',NULL,'Fibonacci Cubes-A New Interconnection Topology','A novel interconnection topology called the Fibonacci cube is shown to possessattractive recurrent structures in spite of its asymmetric and relatively sparseinterconnections. Since it can be embedded as a subgraph in the Boolean cube(hypercube) and it is also a supergraph of other structures, the Fibonacci cube may findapplications in fault-tolerant computing. For a graph with N nodes, the diameter, theedge connectivity, and the node connectivity of the Fibonacci cube are in the logarithmicorder of N. It is also shown that common system communication primitives can beimplemented efficiently.',117407,'IEEE Transactions on Parallel and Distributed Systems',7),(444252,NULL,'1993',NULL,'A Novel Concurrent Error Detection Scheme for FFT Networks','The algorithm-based fault tolerance techniques have been proposed to obtain reliableresults at very low hardware overhead. Even though 100% fault coverage can betheoretically obtained by using these techniques, the system performance, i.e., faultcoverage and throughput, can be drastically reduced due to many practical problems,e.g., round-off errors. A novel algorithm-based fault tolerance scheme is proposed forfast Fourier transform (FFT) networks. It is shown that the proposed scheme achieves100% fault coverage theoretically. An accurate measure of the fault coverage for FFTnetworks is provided by taking the round-off error into account. The proposed scheme isshown to provide concurrent error detection capability to FFT networks with lowhardware overhead, high throughput, and high fault coverage.',117407,'IEEE Transactions on Parallel and Distributed Systems',6),(444365,NULL,'1994',NULL,'Fully Adaptive Minimal Deadlock-Free Packet Routing in Hypercubes, Meshes, and other Networks: Algorithms and Simulations','This paper deals with the problem of packet-switched routing in parallel machines.Several new routing algorithms for different interconnection networks are presented.While the new techniques apply to a wide variety of networks, routing algorithms will beshown for the hypercube, the two-dimensional mesh, and the shuffle-exchange. Althoughthe new techniques are designed for packet routing, they can be used alternatively forvirtual cut-through routing models. The techniques presented for hypercubes and meshesare fully-adaptive and minimal. A fully-adaptive and minimal routing is one in which allpossible minimal paths between a source and a destination are of potential use at thetime a message is injected into the network. Minimal paths followed by messagesultimately depend on the local congestion encountered in each node of the network. Allof the new techniques are completely free of deadlock situations.',117407,'IEEE Transactions on Parallel and Distributed Systems',7),(444391,NULL,'1994',NULL,'Almost Certain Fault Diagnosis Through Algorithm-Based Fault Tolerance','Algorithm-based fault tolerance has been proposed as a technique to detect incorrectcomputations in multiprocessor systems. In algorithm-based fault tolerance, processorsproduce data elements that are checked by concurrent error detection mechanisms. Weinvestigate the efficacy of this approach for diagnosis of processor faults. Becausechecks are performed on data elements, the problem of location of data errors must firstbe solved. We propose a probabilistic model for the faults and errors in a multiprocessorsystem and use it to evaluate the probabilities of correct error location and faultdiagnosis. We investigate the number of checks that are necessary to guarantee errorlocation with high probability. We also give specific check assignments that accomplishthis goal. We then consider the problem of fault diagnosis when the locations oferroneous data elements are known. Previous work on fault diagnosis required that thedata sets produced by different processors be disjoint. We show, for the first time, thatfault diagnosis is possible with high probability, even in systems where processorscombine to produce individual data elements.',117407,'IEEE Transactions on Parallel and Distributed Systems',6),(444452,NULL,'1995',NULL,'Sequential Hardware Prefetching in Shared-Memory Multiprocessors','To offset the effect of read miss penalties on processor utilization in shared-memory multiprocessors, several software- and hardware-based data prefetching schemes have been proposed. A major advantage of hardware techniques is that they need no support from the programmer or compiler.Sequential prefetching is a simple hardware-controlled prefetching technique which relies on the automatic prefetch of consecutive blocks following the block that misses in the cache, thus exploiting spatial locality. In its simplest form, the number of prefetched blocks on each miss is fixed throughout the execution. However, since the prefetching efficiency varies during the execution of a program, we propose to adapt the number of prefetched blocks according to a dynamic measure of prefetching effectiveness. Simulations of this adaptive scheme show reductions of the number of read misses, the read penalty, and of the execution time by up to 78%, 58%, and 25% respectively.',117407,'IEEE Transactions on Parallel and Distributed Systems',4),(444475,NULL,'1995',NULL,'A Theory of Deadlock-Free Adaptive Multicast Routing in Wormhole Networks','A theory for the design of deadlock-free adaptive routing algorithms for wormhole networks was proposed in [12], [16]. This theory supplies the sufficient conditions for an adaptive routing algorithm to be deadlock-free, even when there are cyclic dependencies between channels. Also, two design methodologies were proposed. Multicast communication refers to the delivery of the same message from one source node to an arbitrary number of destination nodes. A tree-like routing scheme is not suitable for hardware-supported multicast in wormhole networks because it produces many headers for each message, drastically increasing the probability of a message being blocked. A path-based multicast routing model was proposed in [25] for multicomputers with 2D-mesh and hypercube topologies. In this model, messages are not replicated at intermediate nodes. This paper develops the theoretical background for the design of deadlock-free adaptive multicast routing algorithms. This theory is valid for wormhole networks using the path-based routing model. It is also valid when messages with a single destination and multiple destinations are mixed together. The new channel dependencies produced by messages with several destinations are studied. Also, two theorems are proposed, developing conditions to verify that an adaptive multicast routing algorithm is deadlock-free, even when there are cyclic dependencies between channels. As an example, the multicast routing algorithms presented in [25] are extended, so that they can take advantage of the alternative paths offered by the network.',117407,'IEEE Transactions on Parallel and Distributed Systems',7),(444480,NULL,'1995',NULL,'Optimal Multicast Communication in Wormhole-Routed Torus Networks','This paper presents efficient algorithms that implement one-to-many, or multicast, communication in wormhole-routed torus networks. By exploiting the properties of the switching technology and the use of virtual channels, a minimum-time multicast algorithm is presented for n-dimensional torus networks that use deterministic, dimension-ordered routing of unicast messages. The algorithm can deliver a multicast message to m - 1 destinations in $\\lceil {\\bf log_2} \\, {\\mbi m} \\rceil$ message-passing steps, while avoiding contention among the constituent unicast messages. Performance results of a simulation study on torus networks with up to 4096 nodes are also given.',117407,'IEEE Transactions on Parallel and Distributed Systems',7),(444495,NULL,'1995',NULL,'Routing in Modular Fault-Tolerant Multiprocessor Systems','In this paper, we consider a class of modular multiprocessor architectures in which spares are added to each module to cover for faulty nodes within that module, thus forming a fault-tolerant basic block (FTBB). In contrast to reconfiguration techniques that preserve the physical adjacency between active nodes in the system, our goal is to preserve the logical adjacency between active nodes by means of a routing algorithm which delivers messages successfully to their destinations. We introduce two-phase routing strategies that route messages first to their destination FTBB, and then to the destination nodes within the destination FTBB. Such a strategy may be applied to a variety of architectures including binary hypercubes and three-dimensional tori. In the presence of f faults in hypercubes and tori, we show that the worst case length of the message route is min {驴+f, (K+ 1)驴}+c where 驴 is the shortest path in the absence of faults, K is the number of spare nodes in an FTBB, and c is a small constant. The average routing overhead is much lower than the worst case overhead.',117407,'IEEE Transactions on Parallel and Distributed Systems',7),(445820,NULL,'1994',NULL,'Fixed-Priority Sensitivity Analysis for Linear Compute Time Models','Several formal results exist that allow an analytic determination of whether a particular scheduling discipline can feasibly schedule a given set of hard real-time periodic tasks. In most cases, these results provide little more than a \'yes\' or \'no\' answer. In practice, it is also useful to know how sensitive scheduling feasibility is to changes in the characteristics of the task set. This paper presents algorithms that allow a system developer to determine, for fixed-priority preemptive scheduling of hard real-time periodic tasks on a uniprocessor, how sensitive schedule feasibility is to changes in the computation times of various software components. The algorithms allow a system developer to determine what changes in task computation times can be made while preserving schedule feasibility (or what changes are needed to achieve feasibility). Both changes to the computation time of a single task and changes to the computation times of a specified subset of the tasks are analyzable. The algorithms also allow a decomposition of tasks into modules, where a module may be a component of multiple tasks.',117420,'IEEE Transactions on Software Engineering',0),(449083,NULL,'2002',NULL,'Analyzing peer-to-peer traffic across large networks','The use of peer-to-peer (P2P) applications is growing dramaticaliy, particularly for sharing large video/audio files and software. In this paper, we analyze P2P traffic by measuring flow-level information collected at multiple border routers across a large ISP network, and report our investigation of three popular P2P systems -- FastTrack, Gnutella, and DirectConnect. We characterize the P2P traffic observed at a single ISP and its impact on the underlying network. We observe very skewed distribution in the traffic across the network at different levels of spatial aggregation (IP, prefix, AS). All three P2P systems exhibit significant dynamics at short times scale and particularly at the IP address level Still, the fraction of P2P traffic contributed by each prefix is much more stable than the corresponding distribution of either Web traffic or overall traffic. The high volume and good stability properties of P2P traffic indicates that the P2P workload is a good candidate for being managed via application-specific layer-3 traffic engineering in an ISP\'s network.',192006,'Proceedings of the 2nd ACM SIGCOMM Workshop on Internet measurment',3),(449868,NULL,'2003',NULL,'Directed diffusion for wireless sensor networking','Advances in processor, memory, and radio technology will enable small and cheap nodes capable of sensing, communication, and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed-diffusion paradigm for such coordination. Directed diffusion is data-centric in that all communication is for named data. All nodes in a directed-diffusion-based network are application aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network (e.g., data aggregation). We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network analytically and experimentally. Our evaluation indicates that directed diffusion can achieve significant energy savings and can outperform idealized traditional schemes (e.g., omniscient multicast) under the investigated scenarios.',117498,'IEEE/ACM Transactions on Networking (TON)',3),(452191,NULL,'2002',NULL,'Generation of robust phonetic set and decision tree for Mandarin using chi-square testing','A phonetic representation of a language is used to describe the corresponding pronunciation and synthesize the acoustic model of any vocabulary. A phonetic representation with smaller phonetic units such as SAMPA-C for Mandarin Chinese and decision trees for parameter sharing are broadly applied to deal with the problem of large numbers of recognition units. However, the confusable phonetic representation in SAMPA-C generally degrades the recognition performance. In this paper, a statistical method based on chi-square testing is used to investigate the phonetic unit characteristics that are confusing and develop a more reliable phonetic set, named modified SAMPA-C. A corresponding question set for the modified SAMPA-C and a two-level splitting criterion are also proposed to effectively and efficiently construct the decision trees. Experiments using continuous Mandarin telephone speech recognition were conducted. Experimental results show that an encouraging improvement in recognition performance can be obtained. The proposed approaches represent a good compromise between the demands of accurate acoustic modeling and the limitations imposed by insufficient training data.',222539,'Speech Communication',2),(464972,NULL,'1997',NULL,'An Euler Path Based Technique for Deadlock-free Multicasting','The existing algorithms for deadlock-free multicasting in interconnection networks assume the Hamiltonian property in the networks topology. However, these networks fail to be Hamiltonian in the presence of faults. This paper investigates the use of Euler circuits in deadlock-free multicasting. Not only are Euler circuits known to exist in all connected networks, a fast polynomial-time algorithm exists to find an Euler circuit in a network. We present a multicasting algorithm which works for both regular and irregular topologies. Our algorithm is applicable to store-and-forward as well as wormhole-routed networks. We show that at most two virtual channels are required per physical channel for any connected network. We also prove that no virtual channels are required to achieve deadlock-free multicasting on a large class of networks. Unlike other existing algorithms for deadlock-free multicasting in faulty networks, our algorithm requires a small amount of information to be stored at each node. The potential of our technique is further illustrated with the help of various examples. A performance analysis on wormhole-routed networks shows that our routing algorithm outperforms existing multicasting procedures.',116424,'ICPP \'97 Proceedings of the international Conference on Parallel Processing',7),(469732,NULL,'1996',NULL,'The Necessary Conditions for Clos-Type Nonblocking Multicast Networks','Efficient interconnection networks are critical to the performance of large communication systems and parallel and distributed computing systems with hundreds or thousands of communication components. The well-known Clos network or v(m,n,r) network can be extended to provide full one-to-many or multicast capability. In this paper, we consider several typical routing control strategies for Clos-type nonblocking multicast networks, and derive the necessary conditions under which this type of network is nonblocking for arbitrary multicast assignments in the strict sense as well as under these control strategies. The necessary conditions derived are represented as the number of middle switches m \\geq \\Theta\\left(n \\frac{\\log r}{\\log \\log r} \\right ). These results match the sufficient nonblocking condition for the currently best available explicitly constructed, constant stage nonblocking multicast network and provide a basis for the optimal design of this type of multicast network.',131623,'IPPS \'96 Proceedings of the 10th International Parallel Processing Symposium',7),(470075,NULL,'1997',NULL,'Adaptive Fault-Tolerant Wormhole Routing Algorithms for Hypercube and Mesh Interconnection','In this paper, we present adaptive fault-tolerant deadlock-free routing algorithms for hypercubes and meshes by using only 3 virtual channels and 2 virtual channels respectively. Based on the concept of unsafe nodes, we design a routing algorithm for hypercubes that can tolerate at least n-1 node faults and can route a message via a path of length no more than the Hamming distance between the source and destination plus four. We also develop a routing algorithm for meshes that can tolerate any block faults, as long as the distance between any two nodes in different faulty blocks is at least 2 in each dimension.',131625,'IPPS \'97 Proceedings of the 11th International Symposium on Parallel Processing',7),(482594,NULL,'2001',NULL,'Filtering Techniques to Improve Trace-Cache Efficiency','Abstract: The trace cache is becoming an important building block of modern, wide-issue processors. So far, trace cache related research has been focused on increasing fetch bandwidth. Trace-caches have been shown too effectively increase the number of \"useful\" instructions that can be fetched into the machine, thus enabling more instructions to be executed each cycle. However, trace cache has another important benefit that got less attention in recent research: especially for variable length ISA, such as Intel\'s IA-32 architecture (X86), reducing instruction decoding power is particularly attractive. Keeping the instruction traces in decoded format, implies the decoding power is only paid upon the build of a trace, thus reducing the overall power consumption of the system. This paper has three main contributions: it indicates that trace cache optimizations directed to reducing power consumption arc do not necessarily coincide with optimizations directed to increasing fetch bandwidth; it extends our understanding on how well the trace cache utilizes its resources and introduces a new trace-cache organization based on filtering techniques. The knowledge obtained from the analysis of the traces\' behavioral patterns motivates the use of filtering techniques. The new trace-cache organization increases the effective instruction-fetch bandwidth in conjunction with reducing the power consumption of the trace-cache system. We observe that (1) the majority of traces that are inserted into the trace-cache are rarely used again before being replaced; (2) the majority of the instructions delivered for execution originate from the fewer traces that are heavily and repeatedly used; and that (3) techniques that aim to improve instruction-fetch bandwidth may increase the number of traces built during program execution. Based on these observations, we propose splitting the trace cache into two components: the filter trace-cache (FTC) and the main trace-cache (MTC). Traces are first inserted into the FTC that is used to filter out the infrequently used traces; traces that prove \"useful\" are later moved into the MTC itself. The FTC/MTC organization exhibits an important benefit: it decreases the number of traces built, thus reducing power consumption while improving overall performance. For medium-size applications, the FTC/MTC pair reduces the number of trace builds by 16% in average. As extension of the filtering concept that involves adding a second level (L2) trace-cache that stores less frequent traces that are replaced in the FTC or the MTC. The extra level of caching allows for order-of-magnitude reduction in the number of trace builds. Second level trace cache proves particularly useful for applications with large instruction footprints.',190888,'Proceedings of the 2001 International Conference on Parallel Architectures and Compilation Techniques',4),(482755,NULL,'2002',NULL,'Using the Compiler to Improve Cache Replacement Decisions','Memory performance is increasingly determining microprocessor performance and technology trends are exacerbating this problem. Most architectures use set-associative caches with LRU replacement policies to combine fast access with relatively low miss rates. To improve replacement decisions in set-associative caches, we develop a new set of compiler algorithms that predict which data will and will not be reused and provide these hints to the architecture.We prove that the hints either match or improve hit rates over LRU. We describe a practical one-bit cache-line tag implementation of our algorithm, called evict-me. On a cache replacement, the architecture will replace a line for which the evict-me bit is set, or if none is set, it will use the LRU bits. We implement our compiler analysis and its output in the Scale compiler. On a variety of scientific programs, using the evict-me algorithm in both the level 1and 2 caches improves simulated cycle times by up to 34% over the LRU policy by increasing hit rates. In addition, a combination of simple hardware prefetching and evict-me works together to further improve performance.',190910,'Proceedings of the 2002 International Conference on Parallel Architectures and Compilation Techniques',4),(508237,NULL,'2002',NULL,'Exploiting the Prefetching Effect Provided by Executing Mispredicted Load Instructions','As the degree of instruction-level parallelism in superscalar architectures increases, the gap between processor and memory performance continues to grow requiring more aggressive techniques to increase the performance of the memory system. We propose a new technique, which is based on the wrong-path execution of loads far beyond instruction fetch-limiting conditional branches, to exploit more instruction-level parallelism by reducing the impact of memory delays. We examine the effects of the execution of loads down the wrong branch path on the performance of an aggressive issue processor. We find that, by continuing to execute the loads issued in the mispredicted path, even after the branch is resolved, we can actually reduce the cache misses observed on the correctly executed path. This wrong-path execution of loads can result in a speedup of up to 5% due to an indirect prefetching effect that brings data or instruction blocks into the cache for instructions subsequently issued on the correctly predicted path. However, it also can increase the amount of memory traffic and can pollute the cache. We propose the Wrong Path Cache (WPC) to eliminate the cache pollution caused by the execution of loads down mispredicted branch paths. For the configurations tested, fetching the results of wrong path loads into a fully associative 8-entry WPC can result in a 12% to 39% reduction in L1 data cache misses and in a speedup of up to 37%, with an average speedup of 9%, over the baseline processor.',90760,'Euro-Par \'02 Proceedings of the 8th International Euro-Par Conference on Parallel Processing',4),(513702,NULL,'1992',NULL,'A High-Speed DES Implementation for Network Applications','A high-speed data encryption chip implementing the Data Encryption Standard (DES) has been developed. The DES modes of operation supported are Electronic Code Book and Cipher Block Chaining. The chip is based on a gallium arsenide (GaAs) gate array containing 50K transistors. At a clock frequency of 250 MHz, data can be encrypted or decrypted at a rate of 1 GBit/second, making this the fastest single-chip implementation reported to date. High performance and high density have been achieved by using custom-designed circuits to implement the core of the DES algorithm. These circuits employ precharged logic, a methodology novel to the design of GaAs devices. A pipelined flow-through architecture and an efficient key exchange mechanism make this chip suitable for low-latency network controllers.',66135,'CRYPTO \'92 Proceedings of the 12th Annual International Cryptology Conference on Advances in Cryptology',10),(545199,NULL,'1997',NULL,'Fault-Tolerant Hierarchical Interconnection Networks Constructed by Additional Bypass Linking with Graph-Node Coloring','A hierarchical redundancy scheme using additional bypass linking (ABL) is proposed for constructing fault-tolerant massively parallel computers with hierarchically ring-connected, mesh-connected, and cube-connected interconnection networks. This scheme enables a redundant hierarchical network to be constructed with a simple nested structure by using a feature of the ABL scheme, which is that it uses graph-node coloring for bypass allocation to the component network in order to minimize the number of bypass links and that the redundant component network can be treated as a processing element (PE) with a fixed number of external connections if these connections are provided in the bypass links. The spare circuits with hierarchical structures can greatly enhance PE-to-PE communications while achieving strong fault tolerance.',75861,'DFT \'97 1997 Workshop on Defect and Fault-Tolerance in VLSI Systems',7),(545487,NULL,'1998',NULL,'On the Current Behavior of Faulty and Fault-Free ICs and the Impact on Diagnosis','The purpose of this paper is to analyze the current behavior of faulty and fault free integrated circuits (ICs) and its impact on diagnosis. More specifically, we first show that normal sub-threshold current can be modeled by a Gaussian distribution. Then, we investigate faulty IC current variations caused to the load connected to nodes involved in bridging faults. Finally, we propose some modifications to a diagnosis method based on maximum likelihood estimation to deal with these behaviors.',75862,'DFT \'98 Proceedings of the 13th International Symposium on Defect and Fault-Tolerance in VLSI Systems',6),(545753,NULL,'2000',NULL,'A Gossip-Based Reliable Multicast for Large-Scale High-Throughput Applications','Group-based reliable multicast is an important building block for distributed applications. For large systems, however, traditional approaches do not scale well due to centralized recovery mechanisms and excessive message overhead. In this paper, we present a reliable probabilistic multicast, rpbcast that is a hybrid of the centralized and gossip-based approaches. In particular, rpbcast extends previous work by supporting high packet rates and many active senders. Rpbcast uses gossip as the primary retransmission mechanism and only contact loggers if gossips fail. Large groups of active senders are supported using negative gossip that specifies those messages a receiver is missing instead of those messages it received. Moreover, we show that negative gossip allows pull-based recovery and converges faster than push-based recovery. Rpbcast also applies hashing techniques to reduce message overhead and approximate group membership for garbage collection. We describe the key features of rpbcast and present simulation results.',81057,'DSN \'00 Proceedings of the 2000 International Conference on Dependable Systems and Networks (formerly FTCS-30 and DCCA-8)',3),(553248,NULL,'1998',NULL,'Compact two-pattern test set generation for combinational and full scan circuits','This paper presents two algorithms for generatingcompact test sets for combinational and full scan circuits under the transition and CMOS stuck-open faultmodels; Redundant Vector Elimination (RVE) and Essential Fault Reduction (EFR). These algorithms together with the dynamic compaction algorithm are incorporated into an advanced ATPG system for combinational circuits, called MinTest. The test sets generated by MinTest are 30% smaller than the previouslypublished two-pattern test set compaction results forthe ISCAS85 and full scan version of the ISCAS89benchmark circuits.',132989,'ITC \'98 Proceedings of the 1998 IEEE International Test Conference',11),(555268,NULL,'2001',NULL,'SCAMP: Peer-to-Peer Lightweight Membership Service for Large-Scale Group Communication','Gossip-based protocols have received considerable attention for broadcast applications due to their attractive scalability and reliability properties. The reliability of probabilistic gossip schemes studied so far depends on each user having knowledge of the global membership and choosing gossip targets uniformly at random. The requirement of global knowledge is undesirable in large-scale distributed systems.In this paper, we present a novel peer-to-peer membership service which operates in a completely decentralized manner in that nobody has global knowledge of membership. However, membership information is replicated robustly enough to support gossip with high reliability. Our scheme is completely self-organizing in the sense that the size of local views naturally converges to the \'right\' value for gossip to succeed. This \'right\' value is a function of system size, but is achieved without any node having to know the system size. We present the design, theoretical analysis and preliminary evaluation of SCAMP. Simulations show that its performance is comparable to that of previous schemes which use global knowledge of membership at each node.',167605,'NGC \'01 Proceedings of the Third International COST264 Workshop on Networked Group Communication',3),(565412,NULL,'1995',NULL,'Asynchronous 2-D discrete cosine transform core processor','To lend additional insight into the reality of self-timed design, this paper proposes a large-scale, application specific, asynchronous design-a CCITT compatible asynchronous DCT/IDCT processor. The prototype DCT/IDCT processor uses two-phase transition signaling and a bounded delay approach to implement a modified version of Sutherland\'s micropipeline. The layout of the core processor was designed using standard cell and custom techniques to integrate 150,000 transistors in a 2 /spl mu/ SCMOS technology. This investigation presents the prototype DCT/IDCT processor design and the resulting measures of speed, power, and area.',115310,'ICCD \'95 Proceedings of the 1995 International Conference on Computer Design: VLSI in Computers and Processors',10),(569822,NULL,'2003',NULL,'Design and Implementation of High-Performance RNS Wavelet Processors Using Custom IC Technologies','The design of high performance, high precision, real-time digital signal processing (DSP) systems, such as those associated with wavelet signal processing, is a challenging problem. This paper reports on the innovative use of the residue number system (RNS) for implementing high-end wavelet filter banks. The disclosed system uses an enhanced index-transformation defined over Galois fields to efficiently support different wavelet filter instantiations without adding any extra cost or additional look-up tables (LUT). A selection of a small wordwidth modulus set are the keys for attaining low-complexity and high-throughput. An exhaustive comparison against existing two\'s complement (2C) designs for different custom IC technologies was carried out. Results reveal a performance improvement of up to 100% for high-precision RNS-based systems. These structures demonstrated to be well suited for field programmable logic (FPL) assimilation as well as for CBIC (cell-based integrated circuit) technologies.',135925,'Journal of VLSI Signal Processing Systems',10),(571850,NULL,'2003',NULL,'Error Analysis and Detection Procedures for a Hardware Implementation of the Advanced Encryption Standard','The goal of the Advanced Encryption Standard (AES) is to achieve secure communication. The use of AES does not, however, guarantee reliable communication. Prior work has shown that even a single transient error occurring during the AES encryption (or decryption) process will very likely result in a large number of errors in the encrypted/decrypted data. Such faults must be detected before sending to avoid the transmission and use of erroneous data. Concurrent fault detection is important not only to protect the encryption/decryption process from random faults. It will also protect the encryption/decryption circuitry from an attacker who may maliciously inject faults in order to find the encryption secret key. In this paper, we first describe some studies of the effects that faults may have on a hardware implementation of AES by analyzing the propagation of such faults to the outputs. We then present two fault detection schemes: The first is a redundancy-based scheme while the second uses an error detecting code. The latter is a novel scheme which leads to very efficient and high coverage fault detection. Finally, the hardware costs and detection latencies of both schemes are estimated.',117351,'IEEE Transactions on Computers',6),(571858,NULL,'2003',NULL,'Probabilistic Reliable Dissemination in Large-Scale Systems','The growth of the Internet raises new challenges for the design of distributed systems and applications. In the context of group communication protocols, gossip-based schemes have attracted interest as they are scalable, easy to deploy, and resilient to network and process failures. However, traditional gossip-based protocols have two major drawbacks: 1) They rely on each peer having knowledge of the global membership and 2) being oblivious to the network topology, they can impose a high load on network links when applied to wide-area settings. In this paper, we provide a theoretical analysis of gossip-based protocols which relates their reliability to key system parameters (system size, failure rates, and number of gossip targets). The results provide guidelines for the design of practical protocols. In particular, they show how reliability can be maintained while alleviating drawback 1) by providing each peer with only a small subset of the total membership information and drawback 2) by organizing members into a hierarchical structure that reflects their proximity according to some network-related metric. We validate the analytical results by simulations and verify that the hierarchical gossip protocol considerably reduces the load on the network compared to the original, nonhierarchical protocol.',117407,'IEEE Transactions on Parallel and Distributed Systems',3),(571864,NULL,'2003',NULL,'Request Redirection Algorithms for Distributed Web Systems','Replication of information among multiple servers is necessary to support high request rates to popular Web sites. We consider systems that maintain one interface to the users, even if they consist of multiple nodes with visible IP addresses that are distributed among different networks. In these systems, the first-level dispatching is achieved through the Domain Name System (DNS) during the address lookup phase. Distributed Web systems can use some request redirection mechanism as a second-level dispatching because the DNS routing scheme has limited control on offered load. Redirection is always executed by the servers, but there are many alternatives that are worth of investigation. In this paper, we explore the combination of DNS dispatching with redirection schemes that use centralized or distributed control on the basis of global or local state information. In the fully distributed schemes, DNS dispatching is carried out by simple algorithms because load sharing are taken by some redirection mechanisms that each server activates autonomously. On the other hand, in fully centralized schemes, redirection is used as a tool to enforce the decisions taken by the same centralized entity that provides the first-level dispatching. We also investigate some hybrid strategies. We conclude that the distributed algorithms are preferable over the centralized counterpart because they provide stable performance, can take content-aware dispatching decisions, can limit the percentage of redirected requests and, last, but not least, their implementation is much simpler than that required by the centralized schemes.',117407,'IEEE Transactions on Parallel and Distributed Systems',3),(571869,NULL,'2003',NULL,'Geometric Spanners for Wireless Ad Hoc Networks','We propose a new geometric spanner for static wireless ad hoc networks, which can be constructed efficiently in a localized manner. It integrates the connected dominating set and the local Delaunay graph to form a backbone of the wireless network. Priori arts showed that both structures can be constructed locally with bounded communication costs. This new spanner has these following attractive properties: 1) the backbone is a planar graph, 2) the node degree of the backbone is bounded from above by a positive constant, 3) it is a spanner for both hops and length, 4) it can be constructed locally and is easy to maintain when the nodes move around, and 5) moreover, the communication cost of each node is bounded by a constant. Simulation results are also presented for studying its practical performance.',117407,'IEEE Transactions on Parallel and Distributed Systems',7),(572499,NULL,'2003',NULL,'Towards context sensitive information inference','Humans can make hasty, but generally robust judgements about what a text fragment is, or is not, about. Such judgements are termed information inference. This article furnishes an account of information inference from a psychologistic stance. By drawing on theories from nonclassical logic and applied cognition, an information inference mechanism is proposed that makes inferences via computations of information flow through an approximation of a conceptual space. Within a conceptual space information is represented geometrically. In this article, geometric representations of words are realized as vectors in a high dimensional semantic space, which is automatically constructed from a text corpus. Two approaches were presented for priming vector representations according to context. The first approach uses a concept combination heuristic to adjust the vector representation of a concept in the light of the representation of another concept. The second approach computes a prototypical concept on the basis of exemplar trace texts and moves it in the dimensional space according to the context. Information inference is evaluated by measuring the effectiveness of query models derived by information flow computations. Results show that information flow contributes significantly to query model effectiveness, particularly with respect to precision. Moreover, retrieval effectiveness compares favorably with two probabilistic query models, and another based on semantic association. More generally, this article can be seen as a contribution towards realizing operational systems that mimic text-based human reasoning.',135904,'Journal of the American Society for Information Science and Technology - Mathematical, logical, and formal methods in information retrieval',2),(577044,NULL,'2002',NULL,'Pointer cache assisted prefetching','Data prefetching effectively reduces the negative effects of long load latencies on the performance of modern processors. Hardware prefetchers employ hardware structures to predict future memory addresses based on previous patterns. Thread-based prefetchers use portions of the actual program code to determine future load addresses for prefetching.This paper proposes the use of a pointer cache, which tracks pointer transitions, to aid prefetching. The pointer cache provides, for a given pointer\'s effective address, the base address of the object pointed to by the pointer. We examine using the pointer cache in a wide issue superscalar processor as a value predictor and to aid prefetching when a chain of pointers is being traversed. When a load misses in the L1 cache, but hits in the pointer cache, the first two cache blocks of the pointed to object are prefetched. In addition, the load\'s dependencies are broken by using the pointer cache hit as a value prediction.We also examine using the pointer cache to allow speculative precomputation to run farther ahead of the main thread of execution than in prior studies. Previously proposed thread-based prefetchers are limited in how far they can run ahead of the main thread when traversing a chain of recurrent dependent loads. When combined with the pointer cache, a speculative thread can make better progress ahead of the main thread, rapidly traversing data structures in the face of cache misses caused by pointer transitions.',192290,'Proceedings of the 35th annual ACM/IEEE international symposium on Microarchitecture',4),(577050,NULL,'2002',NULL,'Compiler managed micro-cache bypassing for high performance EPIC processors','Advanced microprocessors have been increasing clock rates, well beyond the Gigahertz boundary. For such high performance microprocessors, a small and fast data micro cache (ucache) is important to overall performance, and proper management of it via load bypassing has a significant performance impact. In this paper, we propose and evaluate a hardware-software collaborative technique to manage ucache bypassing for EPIC processors. The hardware supports the ucache bypassing with a flag in the load instruction format, and the compiler employs static analysis and profiling to identify loads that should bypass the ucache. The collaborative method achieves a significant improvement in performance for the SpecInt2000 benchmarks. On average, about 40%, 30%, 24%, and 22% of load references are identified to bypass 256B, 1K, 4K, and 8K sized ucaches, respectively. This reduces the ucache miss rates by 39%, 32%, 28%, and 26%. The number of pipeline stalls from loads to their uses is reduced by 13%, 9%, 6%, and 5%. Meanwhile, the L1 and L2 cache misses remain largely unchanged. For the 256B ucache, bypassing improves overall performance on average by 5%.',192290,'Proceedings of the 35th annual ACM/IEEE international symposium on Microarchitecture',4),(578163,NULL,'1972',NULL,'Microprogrammed implementation of computer measurement techniques','Microprogramming has been accepted as a valuable tool in several areas of system design. However, microprogramming has not generally been used as a tool for evaluating the performance of computer systems. This paper describes the implementation of several techniques useful for program monitoring, debugging and system measurement using the microprogramable features of an existing computer system. The measurement system is completely transparent to almost all target programs. Given an existing system with a writable control store, a microprogram measurement system may be the most flexible, inexpensive, reliable, and high-speed means of monitoring the performance of a computer system.',152689,'MICRO 5 Conference record of the 5th annual workshop on Microprogramming',5),(578746,NULL,'2002',NULL,'Enhancing Davis Putnam with extended binary clause reasoning','The backtracking based Davis Putnam (DPLL) procedure remains the dominant method for deciding the satisfiability of a CNF formula. In recent years there has been much work on improving the basic procedure by adding features like improved heuristics and data structures, intelligent backtracking, clause learning, etc. Reasoning with binary clauses in DPLL has been a much discussed possibility for achieving improved performance, but to date solvers based on this idea have not been competitive with the best unit propagation based DPLL solvers. In this paper we experiment with a DPLL solver called 2CLS+EQ that makes more extensive use of binary clause reasoning than has been tried before. The results are very encouraging-2CLS+EQ is competitive with the very best DPLL solvers. The techniques it uses also open up a number of other possibilities for increasing our ability to solve SAT problems.',85684,'Eighteenth national conference on Artificial intelligence',2),(579054,NULL,'2003',NULL,'Wave steering to integrate logic and physical syntheses','Wave steering is a unified logic and physical synthesis scheme that algorithmically generates high-throughput circuits with fast turn-around times. Binary decision diagram (BDD)-type structures are altered to satisfy certain electrical constraints, embedded in silicon with pass transistor logic (PTL), and pipelined to very fine granularity using a novel two-phase clocking scheme. This direct PTL mapping of a logic representation provides good electrical estimations to a front-end tool like the logic synthesizer at an early phase of the design cycle. We apply our wave steering technique to high throughput computation-intensive datapath combinational circuits. We achieve an average speedup of 4.2 times compared to standard cell (SC) implementations of high performance arithmetic circuits at the cost of only about 76% average increase in area. The results look extremely encouraging; all the more so, considering that we also achieve an average reduction of 27% in latency and 15% in power compared to SC circuits.',117466,'IEEE Transactions on Very Large Scale Integration (VLSI) Systems - Special section on system-level interconnect prediction (SLIP)',10),(582204,NULL,'2003',NULL,'Partitioned first-level cache design for clustered microarchitectures','The high clock frequencies of modern superscalar processors make the wire delay incurred in moving data across the processor chip a significant concern. As frequencies continue to increase, it will become more difficult for a centralized first level data cache to supply the timely data bandwidth required by superscalar processors.This paper presents a complete solution for the partitioning of the first level of the memory hierarchy. The first level data cache is split into several independent partitions, which are arbitrarily distributable across the processor die. After being decoded, memory instructions are sent to the reservation stations of the functional unit adjacent to the cache partition that they are most likely to access. The partition assignments for both static instructions and cache data are dynamically changed to adapt to data access patterns. A data cache line is permitted to reside in only one partition at a time, allowing each store to update only a single partition, and allowing the partitioning and simplification of the memory disambiguation logic. The partitioned cache achieves a reduction in cache access latency through a combination of reduced wire delay and reduced cache array size. A partitioned cache with eight 8KB direct-mapped partitions maintains a hit rate greater than that of a 32KB direct-mapped cache. A machine utilizing the partitioned cache outperforms a machine with a conventional 64KB direct-mapped cache by 4.5% and a machine with a 64KB 8-way set-associative cache by 7.0%, when cache latencies estimated through the use of the CACTI cache simulation tool are taken into account.',116491,'ICS \'03 Proceedings of the 17th annual international conference on Supercomputing',4),(583010,NULL,'1998',NULL,'A Ring Architecture Strategy for BIST Test Pattern Generation','This article presents a new effective BIST scheme that achieves 100% fault coverage with low hardware overhead, and without any modification of the circuit under test, i.e., no test point insertion. The set of patterns generated by a pseudo-random pattern generator (e.g. an LFSR) is transformed into a new set of patterns that provides the desired fault coverage. To transform these patterns, a ring architecture composed by a set of masks is used. During on-chip test pattern generation, each mask is successively selected to map the original pattern sequence into a new test sequence. We describe an efficient algorithm that constructs a ring of masks from the test cubes provided by an automatic test pattern generator (ATPG) tool. Moreover, we show that rings of masks are implemented very simply and with low silicon area cost, without the need of any logic synthesis tool; a combinational mapping logic corresponding to the masks is placed between the LFSR and the CUT, together with a looped shift register that acts as a mask selecting circuit. Experimental results are given at the end of the paper, demonstrating the effectiveness of the proposed approach in terms of area overhead, fault coverage and test sequence length.',35789,'ATS \'98 Proceedings of the 7th Asian Test Symposium',11),(584212,NULL,'1996',NULL,'Dynamic Logic in Four-Phase Micropipelines','Micropipelines are self-timed pipelines with characteristics that suggest they may be applicable to low-power circuits. They were originally designed with two-phase control, but four-phase control appears to offer benefits for CMOS implementations.In low-power applications static circuit behavior is desirable since it allows activity to cease (and hence power to be saved) without loss of state. However, dynamic circuits offer the benefits of increased speed and lower switched capacitance. Therefore low-power designs often employ dynamic logic with additional latches or charge-retention circuits to give pseudo-static behavior. These additions increase the cost and power consumption of the dynamic circuits, thereby compromising their potential advantages. Circuits are proposed in this paper that allow dynamic logic to operate efficiently within a four-phase micropipeline framework without the above-mentioned encumbrances whilst still retaining externally static behavior.',35506,'ASYNC \'96 Proceedings of the 2nd International Symposium on Advanced Research in Asynchronous Circuits and Systems',10),(584342,NULL,'2001',NULL,'PCA-1: A Fully Asynchronous, Self-Reconfigurable LSI','This paper describes the asynchronous device features of PCA-1, which is the first VLSI to realize the Plastic Cell Architecture (PCA). PCA is an autonomously reconfigurable hardware architecture consisting of a programmable logic layer and a network of built-in facilities. To realize run-time generation and the deletion of circuit objects with variable grain, the circuits on the logic layer are programmed as self-timed circuits using Look-Up Tables (LUTs). The built-in facilities of PCA-1 are also designed as self-timed circuits to enhance scalability and to minimize power consumption. LSI chips are successfully fabricated, and experimental results are mentioned.',35492,'ASYNC \'01 Proceedings of the 7th International Symposium on Asynchronous Circuits and Systems',10),(584349,NULL,'2002',NULL,'SPA \" A Synthesisable Amulet Core for Smartcard pplications','SPA is a synthesised, self-timed, ARM-compatible processor core. The use of synthesis was mandated by a need for rapid implementation. This has proved to be very effective, albeit with increased cost in terms of area and performance compared with earlier non-synthesised processors. SPA is employed in an experimental smartcard chip which is being designed to evaluate the applicability of self-timed logic in security-sensitive devices. The Balsa synthesis system is used to generate dual-rail logic with some enhancements to improve security against non-invasive attacks. A complete system-on-chip is being synthesised with a only small amount of hand design being employed to boost the throughput of the on-chip interconnection system.',35493,'ASYNC \'02 Proceedings of the 8th International Symposium on Asynchronus Circuits and Systems',10),(585398,NULL,'1995',NULL,'A complex-number multiplier using radix-4 digits','This paper describes the design of a 16/spl times/16 complex-number multiplier developed as part of the arithmetic datapath of a complex-number digital signal processor. The complex-number multiplier internally uses binary signed digits for fast multiplication and compact layout. It employs the traditional three-multiplication scheme while minimizing the logic and delay associated with the three extra pre-multiplication binary additions which that scheme requires. The minimization comes from producing the redundant binary sum for each of the pre-multiplication binary additions with minimal hardware, and then recoding the redundant sums as radix-4 multiplier operands. The radix-4 operands halve the number of summands to be added in each of the three real multiplier units. Furthermore, an additional factor of two reduction in the number of summands is effectuated by our coding scheme for representing binary signed digits. The result is a fast and compact complex-number multiplier.',33621,'ARITH \'95 Proceedings of the 12th Symposium on Computer Arithmetic',8),(585484,NULL,'2003',NULL,'A VLSI Algorithm for Modular Multiplication/Division','We propose an algorithm for modular multiplication/division suitable for VLSI implementation. The algorithm is based on Montgomery\'s method for modular multiplication and on the extended Binary GCD algorithm for modular division. It can perform either of these operationswith a reduced amount of hardware. Both calculations are carried out through iterations of simple operations such as shifts and additions/subtractions. The radix-2 signed-digit representation is employed so that all additions and subtractions are performed without carry propagation. A modular multiplier/divider based on this algorithm has a linear array structure with a bit-slice feature and carries out an n-bit modular multiplication in at most \\left\\lfloor {\\frac{{2(n + 2)}}{3}} \\right\\rfloor+ 3 clock cycles and an n-bit modular division in at most 2n+5 clock cycles, where the length of the clock cycle is constant and independent of n.',33615,'ARITH \'03 Proceedings of the 16th IEEE Symposium on Computer Arithmetic (ARITH-16\'03)',8),(585555,NULL,'1997',NULL,'Interfacing Synchronous and Asynchronous Modules Within a High-Speed Pipeline','This paper describes a new technique for integrating asynchronous modules within a high-speed synchronous pipeline. Our design eliminates potential metastability problems by using a clock generated by a stoppable ring oscillator, which is capable of driving the large clock load found in present day microprocessors. Using the ATACS design tool, we designed highly optimized transistor-level circuits to control the ring oscillator and generate the clock and handshake signals with minimal overhead. Our interface architecture requires no redesign of the synchronous circuitry. % and only a few extra transistors to implement the interface controller. Incorporating asynchronous modules in a high-speed pipeline improves performance by exploiting data-dependent delay variations. Since the speed of the synchronous circuitry tracks the speed of the ring oscillator under different processes, temperatures, and voltages, the entire chip operates at the speed dictated by the current operating conditions, rather than being governed by the worst-case conditions. These two factors together can lead to a significant improvement in average-case performance. The interface design is tested using the 0.6 micron HP CMOS14B process in HSPICE.',34476,'ARVLSI \'97 Proceedings of the 17th Conference on Advanced Research in VLSI (ARVLSI \'97)',10),(586410,NULL,'1996',NULL,'A spectral method for Boolean function matching','An approach to Boolean matching with respect to NPN operations, i.e. negation of the function, permutation of the inputs and negation of the inputs, is presented. The method is based on a canonical form defined in the Hadamard spectral domain. When applied to technology mapping, the idea is to keep the canonical function with each library cell, and to compute the canonical function for a subcircuit. The match is then accomplished using hashing on the canonical functions.',83615,'EDTC \'96 Proceedings of the 1996 European conference on Design and Test',1),(588249,NULL,'1999',NULL,'Experimental Validation of High-Speed Fault-Tolerant Systems Using Physical Fault Injection','This paper introduces a new methodology for validation of dependable systems based on physical fault injection. The approach defines the elements of the injection environment and the requirements that are necessary to control the injection process with fine granularity, allowing for the elimination of glitches and not valid experiments and therefore making the validation process more accurate. We also show the main features of a high-speed pin level fault injection tool, AFIT (Advanced Fault Injection Tool), that incorporates most of the requirements necessary for the application of this methodology. As a practical case study we have validated FASST, a fault tolerant multiprocessor system composed of several fail-silent processor modules. The dependability of the system has been shown, including the influence of the error detection levels in the coverage and latency of the error.',70231,'DCCA \'99 Proceedings of the conference on Dependable Computing for Critical Applications',6),(588945,NULL,'1996',NULL,'Fault-Tolerant Shuffle-Exchange and de Bruijn Networks Capable of Quick Broadcasting','The construction of fault-tolerant processor arrays with shuffle-exchange or base-2 de Bruijn networks using an advanced spare-connection scheme for k-out-of-n redundancies called \"generalized additional bypass linking\" is described. The connection scheme uses bypass links with wired OR connections to spare processing elements (PEs) without external switches, and can reconfigure complete arrays by tolerating faulty portions in these PEs and links. The wired OR connections help to limit the number of connections to the spare PEs, and these connections are made so that the primary PEs are at an inter-PE distance of 3 or more away from each other and are connected to the same bypass link in parallel. The designs of spare-connections were formulated as a node coloring problem of a graph with a minimum distance of 3, and the chromatic numbers that correspond to the number of connections of spare PEs were evaluated by using a newly-devised coloring algorithm. The proposed scheme can be used for constructing various k-out-of-n configurations capable of quick broadcasting by using spare circuits, and is superior to conventional schemes in terms of extra PE connections and reconfiguration control.',75860,'DFT \'96 Proceedings of the 1996 Workshop on Defect and Fault-Tolerance in VLSI Systems',7),(591106,NULL,'2002',NULL,'Job Scheduling for Prime Time vs. Non-Prime Time','Current job scheduling systems for massively parallel machines and Beowulf-class compute clusters support batch scheduling involving two classes of queues: prime time vs. non-prime time. Jobs running in these queue classes must satisfy different criteria with respect to job-size, runtime, or other resource needs. These constraints are designed to delay big jobs to non-prime time in order to provide better quality service during the prime time work-day hours.This paper surveys existing prime time/non-prime time scheduling policies and investigates the sensitivity of scheduling performance to changes in the jobsize and run-time limits allowed during prime time vs. non-prime time. Our simulation study, using real workload traces from theNASA NAS IBM SP/2 cluster, gives strong evidence for the use of specific prime time limits and sheds light on the per-formance trade-offs regarding response times, utilization, short term scheduling algorithm (FCFS vs. EASY backfilling), and success and overflow rates.',50219,'CLUSTER \'02 Proceedings of the IEEE International Conference on Cluster Computing',0),(591419,NULL,'1996',NULL,'A scalable and highly available web server','We describe a prototype scalable and highly available web server, built on an IBM SP-2 system, and analyze its scalability. The system architecture consists of a set of logical front-end or network nodes and a set of back-end or data nodes connected by a switch, and a load balancing component. A combination of TCP routing and Domain Name Server (DNS) techniques are used to balance the load across the Front-end nodes that run the Web (httpd) daemons. The scalability achieved is quantified and compared with that of the known DNS technique. The load on the back-end nodes is balanced by striping the data objects across the back-end nodes and disks. High availability is provided by detecting node or daemon failures and reconfiguring the system appropriately. The scalable and highly available web server is combined with parallel databases, and other back-end servers, to provide integrated scalable and highly available solutions.',53166,'COMPCON \'96 Proceedings of the 41st IEEE International Computer Conference',3),(593763,NULL,'1997',NULL,'Optimal suffix tree construction with large alphabets','The suffix tree of a string is the fundamental data structure of combinatorial pattern matching. Weiner (1973), who introduced the data structure, gave an O(n)-time algorithm for building the suffix tree of an n-character string drawn from a constant size alphabet. In the comparison model, there is a trivial /spl Omega/(n log n)-time lower bound based on sorting, and Weiner\'s algorithm matches this bound trivially. For integer alphabets, a substantial gap remains between the known upper and lower bounds, and closing this gap is the main open question in the construction of suffix trees. There is no super-linear lower bound, and the fastest known algorithm was the O(n log n) time comparison based algorithm. We settle this open problem by closing the gap: we build suffix trees in linear time for integer alphabet.',98782,'FOCS \'97 Proceedings of the 38th Annual Symposium on Foundations of Computer Science',9),(596365,NULL,'2000',NULL,'Scalable and Secure Resource Location','In this paper we present Captain Cook, a service that continuously monitors resources in the Internet, and allows clients to locate resources using this information. Captain Cook maintains a tree-based representation of all the collected resource information. The leaves in the tree contain directly measured resource information, while internal nodes are generated using condensation functions that aggregate information in child nodes. We present examples of how such information may be used for cluster management, application-level routing and placement of servers, and pervasive computing. The nodes are automatically replicated, updates being propagated using a novel hierarchical gossip protocol. We analyze how well this protocol behaves, and conclude that updates propagate quickly in spite of scale, failed nodes, and message loss. We describe how Captain Cook can be made secure using Public Key Certificates without compromising its scalability.',109951,'HICSS \'00 Proceedings of the 33rd Hawaii International Conference on System Sciences-Volume 4 - Volume 4',3),(596852,NULL,'1983',NULL,'HAL: A block level HArdware Logic simulator','A special purpose hardware machine, which simulates up to one half million gates and 2M byte RAM ICs at a 5 millisecond clock speed, is described. This is accomplished with a HArdware Logic (HAL) simulator. This performance is achieved with 32 distributed special parallel processors, which utilize Block Oriented Simulation Technique. The technique promises a good cost hardware logic simulator.',67536,'DAC \'83 Proceedings of the 20th Design Automation Conference',10),(597698,NULL,'1983',NULL,'An experiment in microprocessor-based distributed digital simulation','This paper discusses the design of a distributed simulation system which will utilize off-the-shelf microprocessors in its implementation. Alternative approaches to the assignment of simulation functions and processes are presented. A project currently underway at Texas A & M University is described which considers the impact of distributed architectures on the design of simulation language support systems. The emphasis in this research project is to produce an operational prototype which can be used to establish the feasibility and utility of distributed simulation.',268339,'WSC \'83 Proceedings of the 15th conference on Winter simulation - Volume 1',5),(598026,NULL,'1982',NULL,'Some requirements for architectural support of software debugging','Architectural support of high-level, symbolic debugging is described at three levels of abstraction: the user\'s view of desired debugging functionality, the debugger implementor\'s view of architectural requirements that support the functionality, and the computer architect\'s view of architectural features or attributes that implement the requirements. References are made where possible to computing systems that meet the requirements. The paper is written from the viewpoint of debugger implementors, and is addressed primarily to computer architects.',34932,'ASPLOS I Proceedings of the first international symposium on Architectural support for programming languages and operating systems',5),(599224,NULL,'1978',NULL,'Storage concepts in a software-reliability-directed computer architecture','Because of the tremendous difficulty of producing reliable software (application and system software) and the serious consequences of software errors, a new solution is being explored: the development of a new computer architecture that will substantially enhance the reliability of the programs executing above it. This paper concentrates on the storage concepts (e.g., data representations and addressing) in the architecture.',131944,'ISCA \'78 Proceedings of the 5th annual symposium on Computer architecture',5),(599480,NULL,'1977',NULL,'Cpoda - a demand assignment protocol for satnet','A new contention-based demand assignment protocol (CPODA) is described. The protocol is designed to handle packetized data and voice traffic in a multiple access satellite broadcast channel, where the channel is typically shared by hundreds of earth stations. A sophisticated traffic environment is assumed, including multiple priority and delay class distinctions, variable message lengths, and an arbitrary load distribution among the stations. In addition, stations may have different receiving rate capabilities. The CPODA protocol is part of a broader research project concerned with developing protocols for a one-hop satellite network (SATNET). The CPODA and other protocols are currently undergoing experimental evaluation using an INTELSAT IV-A satellite and four earth stations in the Atlantic region.',216028,'SIGCOMM \'77 Proceedings of the fifth symposium on Data communications',3),(599986,NULL,'1974',NULL,'System description of the JHU emulation laboratory','The emulation laboratory described in this paper is being implemented at Johns Hopkins University as a research and educational facility. Currently, we are planning to use this laboratory to support research in the following areas: 1) Examination of and experimentation with novel system architectures, 2) Evaluation of various directly executed language (DEL) structures and the effective structure of their associated base machines [4], and 3) Dynamic analysis of the Performance of current machine organizations by a combination of emulation and embedded data collection routines. The design of the emulation laboratory and, particularly, the host machine have evolved from several seminars given at Johns Hopkins [1], [2], [3]. From these discussions we felt that there were several important features that our laboratory should include.',152691,'MICRO 7 Conference record of the 7th annual workshop on Microprogramming',5),(600466,NULL,'1978',NULL,'Experience with a microprogrammed Interlisp system','This paper presents the design of an Interlisp system running on a microprogrammed minicomputer. We discuss the constraints imposed by compatibility requirements and by the hardware, the important design decisions, and the most prominent successes and failures of our design, and offer some suggestions for future designers of small Lisp systems. This extended abstract contains only qualitative results. Supporting measurement data will be presented at MICRO-11.',152668,'MICRO 11 Proceedings of the 11th annual workshop on Microprogramming',5),(602117,NULL,'1967',NULL,'The AED approach to generalized computer-aided design','This paper has been written in response to a request for an up-to-date broad view of the approach to computer-aided design taken by the M.I.T. Computer-Aided Design Project. Included in the suggestion was the hope that such a description would help to illuminate, expecially for people who are not system programmers, the major features which any computer-aided design system must have in order to be a useful practical tool. This has proved to be a difficult assignment, because there are several audiences involved. Engineers, designers, managers, programmers, and system programmers all have different interests in the problems of computer-aided design. With several notable exceptions, however, all of these types of people seem to lack a clear understanding of what “design” means, and just what is required to really involve the computer in the design process.',15637,'ACM \'67 Proceedings of the 1967 22nd national conference',5),(602128,NULL,'1967',NULL,'A comparative analysis of two concepts in the generation of uniform pseudo-random numbers','In recent years, considerable attention has been given to find reliable methods capable of producing, within a digital computer, pseudo-random numbers obeying the uniform distribution on the unit interval. Apparently, the most popular method has been the congruence algorithm whose basic form Xi+1 &equil; aX1 + b mod 2m (1) can be easily implemented on a binary computer with word size of m bits. Since its introduction, a number of papers1-3 have been written in which techniques, such as suggesting formulae1 to compute optimal values for a and b, have been presented to improve the statistical properties of the method. As a consequence, several versions with values for a and b to suit everybody\'s needs are now in existence. One must be aware that an analysis based on statistical testing cannot be entirely conclusive, especially if the power of some tests used is not known. Nevertheless, the comparative analysis of this study does indicate that a generator based on Tausworthe\'s concept exhibits a statistical behavior that is as good if not superior to that of the congruence algorithm. Therefore, the following advantage in its use are apparent: (1) Its functional form and statistical behavior are entirely machine independent. (2) It has been shown analytically that it generates values of a random variable uniformly distributed on the unit interval. (3) It can be easily programmed in FORTRAN without sacrificing any of its characteristics. (To the author\'s knowledge, none of these advantages can be claimed by any of the existing congruence algorithms.)',15637,'ACM \'67 Proceedings of the 1967 22nd national conference',2),(602634,NULL,'1977',NULL,'The CAP project - an interim evaluation','The CAP project has included the design and construction of a computer with an unusual and very detailed structure of memory protection, and subsequently the development of an operating system which fully exploits the protection facilities. The present paper passes the work in review and draws conclusions about good and bad aspects of the system. The basic architecture of the CAP machine is described in [1] and a largely prospective description of the protection system is given in [2]. The project was started as an experiment in hardware memory protection. A computer was to be designed in which operating system development was easy, in which ruggedness was produced by a much more fine-grained network of firewalls than was (or is) usual, and in which the full range of protection facilities was available to the writers of subsystems. Simplicity of mechanism was a very important goal, although some emphasis was placed on flexibility of protection policy.',221230,'SOSP \'77 Proceedings of the sixth ACM symposium on Operating systems principles',5),(603740,NULL,'1976',NULL,'An extendable approach to computer-aided software requirements engineering','The development of system requirements has been recognized as one of the major problems in the process of developing data processing system software. We have developed a computer-aided system for maintaining and analyzing such requirements. This system includes the Requirements Statement Language (RSL), a flow-oriented language for the expression of software requirements, and the Requirements Engineering and Validation System (REVS), a software package which includes a translator for RSL, a data base for maintaining the description of system requirements, and a collection of tools to analyze the information in the data base. The system emphasizes a balance between the use of the creativity of human thought processes and the rigor and thoroughness of computer analysis. To maintain this balance, two key design principles—extensibility and disciplined thinking—were followed throughout the system. Both the language and the software are easily user-extended, but adequate locks are placed on extensions, and limitations are imposed on use, so that discipline is augmented rather than decreased.',116537,'ICSE \'76 Proceedings of the 2nd international conference on Software engineering',5),(603748,NULL,'1976',NULL,'Essential elements of software engineering education','Software engineering involves the application of principles of computer science, management science, and other fields to the design and construction of software systems. Education in software engineering is fundamentally different from education in computer science, management science, or other constituent fields, even though it shares a large common area of concern. As we move toward the development of coordinated software engineering curricula, it is mandatory that we identify principles, not just random collections of techniques, on which to build them. Our research, teaching, and practical experience leads us to argue for five essential elements of any software engineering curriculum: computer science, management science, communication skills, problem solving, and design methodology. This paper will discuss these areas, illustrate their current application in courses, and indicate their implications for curriculum development.',116537,'ICSE \'76 Proceedings of the 2nd international conference on Software engineering',5),(604290,NULL,'1977',NULL,'Restricted data types, specification and enforcement of invariant properties of variables','When defining a data type, it is often useful to specify restrictions on the permitted values of that type. Pascal\'s subrange type declaration, a special case of this kind of constraint definition, has already proved itself to be quite useful. Restricted data types allow more complex constraints to be defined and checked; for example, a variable could be declared of type “odd integer” or the day field of a “date” type variable could be checked for consistency with the year and month fields. A simple mechanism is proposed, allowing the formulation of such constraints and their association with data types; the behaviour of a restricted type variable is described. The effects of the use of such a mechanism on program reliability, readability and efficiency are discussed.',189859,'Proceedings of an ACM conference on Language design for reliable software',2),(604730,NULL,'1983',NULL,'A linear-time algorithm for a special case of disjoint set union','This paper presents a linear-time algorithm for the special case of the disjoint set union problem in which the structure of the unions (defined by a “union tree”) is known in advance. The algorithm executes an intermixed sequence of m union and find operations on n elements in 0(m+n) time and 0(n) space. This is a slight but theoretically significant improvement over the fastest known algorithm for the general problem, which runs in 0(m&agr;(m+n, n)+n) time and 0(n) space, where &agr; is a functional inverse of Ackermann\'s function. Used as a subroutine, the algorithm gives similar improvements in the efficiency of algorithms for solving a number of other problems, including two-processor scheduling, the off-line min problem, matching on convex graphs, finding nearest common ancestors off-line, testing a flow graph for reducibility, and finding two disjoint directed spanning trees. The algorithm obtains its efficiency by combining a fast algorithm for the general problem with table look-up on small sets, and requires a random access machine for its implementation. The algorithm extends to the case in which single-node additions to the union tree are allowed. The extended algorithm is useful in finding maximum cardinality matchings on nonbipartite graphs.',225130,'STOC \'83 Proceedings of the fifteenth annual ACM symposium on Theory of computing',9),(605118,NULL,'1977',NULL,'PIRAMED project an integrated CAD/CAM system development','A study was conducted in 1976 by GTE Automatic Electric Laboratories to determine what is required to more effectively support product (e.g., Electronic Switching System) development and product manufacturing by GTE Automatic Electric. The results of the study was a project proposal (PIRAMED). The PIRAMED System is being developed from the basic perspective that Computer Aided Design and Computer Aided Manufacturing must not be separate, disjoint functions but, instead, integrated functions. Key design concepts upon which the system structure will rest are: an integrated database system, an interactive user access to the system, and a flexible system structure. The above discussion about the system only addresses the technical aspects of the total project. What is equally important is an environmental plan to deal with the present organization, such that the likelihood of a successive implementation is greatly enhanced. An analysis of GTE Automatic Electric environment was performed within the study and formed the basis for the environmental plan.',67530,'DAC \'77 Proceedings of the 14th Design Automation Conference',5),(610141,NULL,'1996',NULL,'Performance Characterization of the Alpha 21164 Microprocessor Using TP and SPEC Workloads','This paper compares the performance characteristics of the Alpha 21164 to the previous-generation 21064 microprocessor. Measurements on the 21164-based AlphaServer 8200 system are compared to the 21064-based DEC 7000 server using several commercial and technical workloads. The data analyzed includes cycles per instruction, multiple-issued instructions, branch predictions, stall components, cache misses, and instruction frequencies. The AlphaServer 8200 provides 2 to 3 times the performance of the DEC 7000 server based on the faster clock, larger on-chip cache, expanded multiple-issuing, and lower cache/memory latencies and higher bandwidth.',112915,'HPCA \'96 Proceedings of the 2nd IEEE Symposium on High-Performance Computer Architecture',4),(610746,NULL,'2000',NULL,'Synchronizing Network Probes to avoid Measurement Intrusiveness with the Network Weather Service','In this paper, we present a scalable protocol for conducting periodic probes of network performance in a way that minimizes collisions between separate probes. The goal of the protocol is to enable active performance monitoring of large-scale distributed computational systems and networks. We use the protocol to generate time series of measurement data that are then exposed to numerical forecasting models when a prediction of network performance is required. We present the protocol and demonstrate its effectiveness using the Network Weather Service - a tool for dynamically predicting network, CPU, memory, and storage performance.',112954,'HPDC \'00 Proceedings of the 9th IEEE International Symposium on High Performance Distributed Computing',3),(613826,NULL,'1999',NULL,'Using Physical and Simulated Fault Injection to Evaluate Error Detection Mechanisms','Effective error detection is paramount for building highly dependable computing systems. A new methodology, based on physical and simulated fault injection, is developed for evaluating error detection mechanisms. Our approach consists of two steps. First, transient faults are physically injected at the IC pin level of a prototype server. Experiments are carried out in a three dimensional space of events, the location, time of occurrence and duration of the fault being randomly selected. Improved detection circuitry is devised for decreasing signal sensitivity to transients. Second, simulated fault injection is performed to asses the effectiveness of the new detection mechanisms, without using expensive silicon implementations.Physical fault injection experiments, carried out on the server, and simulated fault injection, performed on a protocol checker, are presented in this paper. Detection effectiveness is measured by the error detection coverage, defined as the conditional probability that an error is detected, given that an error occurs. Fault injection reveals that coverage probability is a function of fault duration. The protocol checker significantly improves error detection. Although, further research is required to increase detection coverage of the errors induced by short transient faults.',187188,'PRDC \'99 Proceedings of the 1999 Pacific Rim International Symposium on Dependable Computing',6),(613938,NULL,'1997',NULL,'A New Quorum-Based Replica Control Protocol','Data replication is an important issue in distributed systems. Many protocols are designed to achieve high availability, but some of them have restrictions on N, the number of nodes in the system. This motivates us to design a protocol suitable for arbitrary N. In this paper, we present a new quorum-based replica control protocol whose quorum size is O(\\sqrt{N}), which is the same as that of the grid protocol. Moreover, the proposed protocol is almost symmetric, i.e., each node nearly bears the same responsibility. In particular, our protocol performs well in systems where read operations are requested more frequently than write ones.',187898,'PRFTS \'97 Proceedings of the 1997 Pacific Rim International Symposium on Fault-Tolerant Systems',3),(615572,NULL,'1997',NULL,'Exploiting skips in periodic tasks for enhancing aperiodic responsiveness','In certain real-time applications, ranging from multimedia to telecommunication systems, timing constraints can be more flexible than scheduling theory usually permits. For example, in video reception, missing a deadline is acceptable, provided that most deadlines are met. We deal with the problem of scheduling hybrid sets of tasks, consisting of firm periodic tasks (i.e., tasks with deadlines which can occasionally skip one instance) and soft aperiodic requests, which have to be served as soon as possible to minimize their average response time. We propose and analyze an algorithm, based on a variant of earliest deadline first scheduling, which exploits skips to enhance the response time of aperiodic requests. Schedulability bounds are also derived to perform off-line analysis.',208425,'RTSS \'97 Proceedings of the 18th IEEE Real-Time Systems Symposium',0),(615577,NULL,'1997',NULL,'Jitter concerns in periodic task systems','A model for periodic tasks is proposed that explicitly incorporates jitter-the uncertainty in the arrival times of individual frames. Feasibility-analysis of systems of such tasks is studied in the context of dynamic-priority, preemptive, uniprocessor scheduling. From a computational complexity perspective, the problem is shown to be no more difficult than feasibility analysis in systems of periodic tasks that do not exhibit jitter. Several feasibility analysis algorithms are presented and proven correct.',208425,'RTSS \'97 Proceedings of the 18th IEEE Real-Time Systems Symposium',0),(617124,NULL,'1996',NULL,'The Palindrome Network for Fault-Tolerant Interconnection','A new interconnection network composed of 3 by 3 switching elements is proposed in this paper. This new network is called Palindrome interconnection network (PIN) with hardware complexity identical to its counterparts. Compared with its counterparts, our proposed network is fault-tolerant, as totally disjoint paths exist in the network between any source/destination pair. In addition, for a given routing tag in the PIN, all the other equivalent tags which correspond to the same source/destination pair, can be derived easily. Computing an equivalent routing tag from a given routing tag requires the change of two bits of routing tag, irrespective of the network size, suggesting that rerouting logics can be incorporated into the constituent switching element comfortably without compromising performance. The proposed network exhibits higher terminal reliability than its counterparts.',221956,'SPDP \'96 Proceedings of the 8th IEEE Symposium on Parallel and Distributed Processing (SPDP \'96)',7),(617304,NULL,'1997',NULL,'Fault Detection Using Hints from the Socket Layer','This paper describes a fault detection mechanism that uses the error codes returned by the stream sockets to locate process failures. Since these errors are generated automatically when there is communication with a failed process, the mechanism does not incur in any failure-free overheads. However, for some types of faults, detection can only be attained if the surviving processes use certain communication operations. To assess the coverage and latency of the proposed mechanism, faults were injected during the execution of parallel applications. Our results show that in most cases, faults could be found using only the errors from the socket layer. Depending on the type of fault that was injected, detection occurred in an interval ranging from a few milliseconds to less than 9 minutes.',223451,'SRDS \'97 Proceedings of the 16th Symposium on Reliable Distributed Systems',6),(617451,NULL,'2000',NULL,'Semantically Reliable Multicast Protocols','Reliable multicast protocols can strongly simplify the design of distributed applications. However, it is hard to sustain a high-multicast throughput when groups are large and heterogeneous. In an attempt to overcome this limitation, previous work has focused on weakening reliability properties. In this paper, we introduce a novel reliability model that exploits semantic knowledge to decide in which specific condition messages can be purged without compromising application correctness. This model is based on the concept of message obsolescence: A message becomes obsolete when its content or purpose is overwritten by a subsequent message. We show that message obsolescence can be expressed in a generic way and can be used to configure the system to achieve higher multicast throughput.',223436,'SRDS \'00 Proceedings of the 19th IEEE Symposium on Reliable Distributed Systems',3),(620106,NULL,'1995',NULL,'An improved output compaction technique for built-in self-test in VLSI circuits','In this paper, we propose a space compression technique for digital circuits for minimizing the storage for the circuits under test while maintaining the fault coverage information. In this technique, a compaction tree is generated based on the circuit under test. The detectable error probability is calculated by using the Boolean Difference Method. The output modification is employed to minimize the number of faulty output data patterns which have the same compressed form as the fault-free patterns. The compressed outputs are then fed into a syndrome counter to derive the signature for the circuit. Simulations were performed on known combinational circuits and the results indicate that the loss in fault coverage caused by compression is in the range of 0-10% which is rather small.',261821,'VLSID \'95 Proceedings of the 8th International Conference on VLSI Design',11),(620124,NULL,'1995',NULL,'A genetic approach to test application time reduction for full scan and partial scan circuits','Full scan and partial scan are effective design-for-testability techniques for achieving high fault coverage. However, test application time can be high if long scan chains are used. Reductions in test application time can be made if flip-flop values are not scanned in and out before and after every test vector is applied. Previous research has used deterministic fault-oriented combinational and sequential circuit test generators in generating test vectors and sequences and in deciding when to scan the flip-flops. In this work we use genetic algorithms to generate compact test sets which limit the scan operations. Results for the ISCAS89 sequential benchmark circuits show that significant reductions in test application time can be achieved, especially for partial scan circuits.',261821,'VLSID \'95 Proceedings of the 8th International Conference on VLSI Design',11),(620275,NULL,'1996',NULL,'Sequential Circuits with combinational Test Generation Complexity','We propose a new design for testability (DFIJ technique which, like full scan design, guarantees that tests can be derived using a combinational test generator. Our DFT technique has two important advantages overfull scan design: (1) the area overhead incurred by our technique is significantly less and (2) the test application time for our technique is significantly lower. This DFT technique selects scan flip-flops and/or test points such that, in the test mode, the circuit will belong to a special class of sequential circuits that we call as strongly balanced structures. We give a simple characterization for strongly balanced structures and provide efficient methods to select scan Rip. flops and/or test points to reduce any sequential circuit to a strongly balanced structure. A complete test set can be obtained for a strongly balanced structure using a combinational test generator. Experimental results on ISCAS 89 benchmark circuits and production VLSI circuits show that both the DFT overhead and the test application time are substantially lower for our technique.',261822,'VLSID \'96 Proceedings of the 9th International Conference on VLSI Design: VLSI in Mobile Communication',11),(620330,NULL,'1997',NULL,'On the Detection of Reset Faults in Synchronous Sequential Circuits','We consider the problem of testing reset faults in synchronous sequential circuits with reset hardware. The reset hardware is assumed to consist of a reset input connected to all the flip-flops through a reset line. We propose a fault model that accommodates any routing of the reset line to the flip-flops. This is important since test generation is typically carried out without knowledge of the way in which the reset line is routed. We describe fault simulation procedures for the proposed reset fault model. The procedures use a given test sequence and generate appropriate reset sequences, if needed. It is shown that contrary to the common assumption that reset faults are easily detected by test sequences for other faults in the circuit some reset faults require special reset sequences and special test sequences. Thus, a complete test sequence must explicitly accommodate reset faults.',261823,'VLSID \'97 Proceedings of the Tenth International Conference on VLSI Design: VLSI in Multimedia Applications',11),(620466,NULL,'1998',NULL,'Partial Scan Selection Based on Dynamic Reachability and Observability Information','A partial scan selection strategy is proposed in which flip-flops are selected via newly proposed dynamic reachability and observability measures such that the remaining hard-to-detect faults are easily detected. This is done by taking advantage of the information available when a target fault is aborted by the test generator. A partial scan selection tool, IDROPS, has been developed which selects the best and smallest set of flip-flops to scan that will result in a high fault coverage. Results indicate that high fault coverage in hard-to-test circuits can be achieved using fewer scan flip-flops than in previous methods.',261824,'VLSID \'98 Proceedings of the Eleventh International Conference on VLSI Design: VLSI for Signal Processing',11),(620934,NULL,'2002',NULL,'Static and Dynamic Variable Voltage Scheduling Algorithms for Real-Time Heterogeneous Distributed Embedded Systems','This paper addresses the problem of static and dynamic variable voltage scheduling of multi-rate periodic task graphs (i.e., tasks with precedence relationships) and aperiodic tasks in heterogeneous distributed real-time embedded systems. Such an embedded system may contain general-purpose processors, field-programmable gate arrays (FPGAs) and application-specific integrated circuits (ASICs). Variable voltage scheduling is performed only on general-purpose processors. The static scheduling algorithm constructs a variable voltage schedule via heuristics based on critical path analysis and task execution order refinement. The algorithm redistributes the slack in the initial schedule and refines task execution order in an efficient manner. The variable voltage schedule guarantees all the hard deadlines and precedence relationships of periodic tasks. The dynamic scheduling algorithm is also based on an initially valid static schedule. The objective of the on-line scheduling algorithm is to provide best-effort service to soft aperiodic tasks, as well as to reduce the system power consumption by determining clock frequencies (and correspondingly supply voltages) for different tasks at run-time, while still guaranteeing the deadlines and precedence relationships of hard real-time periodic tasks.',34702,'ASP-DAC \'02 Proceedings of the 2002 Asia and South Pacific Design Automation Conference',0),(621624,NULL,'1996',NULL,'Automatic test generation using genetically-engineered distinguishing sequences','A fault-oriented sequential circuit test generator is described in which various types of distinguishing sequences are derived, both statically and dynamically, to aid the test generation process. A two-phase algorithm is used during test generation. The first phase activates the target fault, and the second phase propagates the fault effects (FE\'s) from the flip-flops with assistance from the distinguishing sequences. This strategy improves the propagation of FE\'s to the primary outputs, and the overall fault coverage is greatly increased. In our new test generator, DIGATE, genetic algorithms are used to derive both activating and distinguishing sequences during test generation. Our results show very high fault coverages for the ISCAS89 sequential benchmark circuits and several synthesized circuits.',262220,'VTS \'96 Proceedings of the 14th IEEE VLSI Test Symposium',11),(621690,NULL,'1997',NULL,'Methods to reduce test application time for accumulator-based self-test','Accumulators based on addition or subtraction can be used as test pattern generators. Some circuits, however, require long test lengths if the parameters of the accumulator are not properly adapted. This paper presents two different methods to minimize the test length without sacrificing fault coverage. The simulation-based reseeding method is suited to random pattern testable circuits and uses forward and reverse order simulation to skip ineffective patterns. The analytical method is appropriate for circuits with \"hard\" faults that are detected only by few test patterns. This method searches for an optimal input value of the accumulator and calculates the best seed analytically. The results show significant test length reductions. The proposed pattern generators can be implemented very efficiently in hardware using available blocks of a data path or in software using an embedded processor.',262221,'VTS \'97 Proceedings of the 15th IEEE VLSI Test Symposium',11),(621909,NULL,'2000',NULL,'Reducing Test Application Time for Built-in-Self-Test Test Pattern Generators','This paper presents a new technique, called C-compatibility, for reducing the test application time of the counter-based exhaustive Built-in-Self-Test (BIST) test pattern generators. This technique reduces the test application time by reducing the size of the binary counter used in the test pattern generators. We have incorporated the synthesis algorithm for synthesizing BIST test pattern generators using the C-compatibility technique into ATOM, an advanced ATPG system for combinational circuits. The experimental results showed that the test pattern generators synthesized using this technique for the ISCAS85 and full scan versions of the ISCAS89 benchmark circuits achieve 100% stuck-at fault coverage in much smaller test application time than the previously published counter-based exhaustive BIST test pattern generators.',262208,'VTS \'00 Proceedings of the 18th IEEE VLSI Test Symposium',11),(623193,NULL,'2003',NULL,'A Power-Aware, Best-Effort Real-Time Task Scheduling Algorithm','In this paper, we present a power-aware, best-effort real-time task scheduling algorithm calledPA-BTA that optimizes real-time performance andpower consumption. The algorithm considers atimeliness model where task timing constraints aredescribed using Jensen\'s benefit functions and asystem-level power model. We propose a metric called \"Energy and Real-Time PerformanceGrade\" (ERG) to measure real-time performanceand power consumption in a unified way. Since thescheduling problem is NP-hard, PA-BTA heuristically computes schedules to maximize ERG, incurring a worst-case computational cost of O(n2).Our simulation results indicate that the algorithmperforms close to the optimal algorithm and betterthan other algorithms considered in the study.',268398,'WSTFES \'03 Proceedings of the IEEE Workshop on Software Technologies for Future Embedded Systems',0),(625939,NULL,'1995',NULL,'Joint source/channel coding for multicast packet video','Current schemes for multicast packet video cope with congestion by adapting the transmission rate of a variable rate codec at the source. We propose a new approach, based on receiver- rather than source-adaptation, where a video source is encoded hierarchically with each layer of hierarchy distributed on a separate network channel. Each receiver can then dynamically adapt to local network capacity by adjusting the number of layers it receives. In order to deploy such a system, we must at the same time develop a layered codec tailored for this model. We present a prototype coder that has been designed specifically for our receiver-based congestion avoidance scheme. In order to evaluate the efficacy of our approach, we have implemented it in an existing Internet remote conferencing application and constrained the complexity of our design to run in real-time on standard workstations. Even with this constraint, our codec can generate a flexible range of layers while exhibiting reasonable compression performance.',116031,'ICIP \'95 Proceedings of the 1995 International Conference on Image Processing (Vol. 1)-Volume 1 - Volume 1',3),(631322,NULL,'1997',NULL,'Instruction prefetching using branch prediction information','Instruction prefetching can effectively reduce instruction cache misses, thus improving the performance. In this paper, we propose a prefetching scheme, which employs a branch predictor to run ahead of the execution unit and to prefetch potentially useful instructions. Branch prediction-based (BP-based) prefetching has a separate small fetching unit, allowing it to compute and predict targets autonomously. Our simulations show that a 4-issue machine with BP-based prefetching achieves higher performance than a plain cache 4 times the size. In addition, BP-based prefetching outperforms other hardware instruction fetching schemes, such as next-n line prefetching and wrong-path prefetching, by a factor of 17-44% in stall overhead.',115312,'ICCD \'97 Proceedings of the 1997 International Conference on Computer Design (ICCD \'97)',4),(631357,NULL,'1998',NULL,'Low-Power Radix-8 Divider','Using highly optimized, custom circuits and fast dynamic array control structures, a small team of designers at the IBM Austin Research Laboratory has developed a one gigahertz microprocessor. This paper describes the custom datapath circuit technology ...',115313,'ICCD \'98 Proceedings of the International Conference on Computer Design',10),(631432,NULL,'1998',NULL,'On Finding Undetectable and Redundant Faults in Synchronous Sequential Circuits','We describe a time-efficient procedure for identifying undetectable and redundant faults in a synchronous sequential circuit, without using a sequential circuit test pattern generator. The proposed procedure is based on the use of a limited length iterative logic array model of the circuit, and has two phases. In the first phase, faults that will not be proved to be undetectable are identified. In the second phase, undetectable faults are identified out of the remaining faults using a combinational circuit test generator. Sequential static learning on the fault-free circuit and a subset of unreachable states are used in the proposed procedure to increase the amount of information available when considering an iterative logic array model of limited length. An undetectable fault in a synchronizable circuit that leaves the faulty circuit synchronizable is identified as a redundant fault. Experimental results presented in this work demonstrate the effectiveness of the proposed techniques in finding undetectable and redundant faults. Larger numbers of undetectable and redundant faults are found compared to earlier works.Keywork : test generation, undetectable faults, redundant faults, synchronous sequential circuit',115313,'ICCD \'98 Proceedings of the International Conference on Computer Design',11),(631465,NULL,'1999',NULL,'Design and Evaluation of a Selective Compressed Memory System','This research explores any potential for an on-chip cache compression which can reduce not only cache miss ratio but also miss penalty, if main memory is also managed in compressed form. However, decompression time causes a critical effect on the memory access time and variable-sized compressed blocks tend to increase the design complexity of the compressed cache architecture. This paper suggests several techniques to reduce the decompression overhead and to manage the compressed blocks efficiently, which include selective compression, fixed space allocation for the compressed blocks, parallel decompression, the use of a decompression buffer, and so on. Moreover, a simple compressed cache architecture based on the above techniques and its management method are proposed. The results from trace-driven simulation show that this approach can provide around 35% decrease in the on-chip cache miss ratio as well as a 53% decrease in the data traffic over the conventional memory systems. Also, a large amount of the decompression overhead can be reduced, and thus the average memory access time can also be reduced by maximum 20% against the conventional memory systems.',115314,'ICCD \'99 Proceedings of the 1999 IEEE International Conference on Computer Design',4),(631703,NULL,'2002',NULL,'Dynamic Loop Caching Meets Preloaded Loop Caching \" A Hybrid Approach','Dynamically-loaded tagless loop caching reduces instruction fetch power for embedded software with small loops, but only supports simple loops without taken branches. Preloaded tagless loop caching supports complex loops with branches and thus can reduce power further, but has a limit on the total number of instructions cached. We show that each does well on particular benchmarks, but neither is best across all of those benchmarks. We present a new hybrid loop cache that only preloads the complex loops, while dynamically loading other loops, thus achieving the strengths of each approach. We demonstrate better power savings than either previous approach alone.',115302,'ICCD \'02 Proceedings of the 2002 IEEE International Conference on Computer Design: VLSI in Computers and Processors (ICCD\'02)',4),(636650,NULL,'1996',NULL,'Fault-tolerant routing strategy using routing capability in hypercube multicomputers','This paper addresses fault-tolerant routing which is concerned with finding feasible minimum paths in a faulty hypercube. The concept of routing capability, which is defined with respect to the entire spectrum of distance, is proposed to assist routing function. The amount of information that is useful for message routing is increased with our scheme. The proposed algorithm routes a message in an attempt to minimize derouting. In particular, it makes use of the information embedded in routing capabilities to establish a path for a message for which an upper bound on its length may be determined at the source. We then propose the notion of directed routing capability which captures more useful information for shortest path routing in comparison with undirected counterpart. Routing in hypercubes with link failures is also addressed.',116388,'ICPADS \'96 Proceedings of the 1996 International Conference on Parallel and Distributed Systems',7),(636905,NULL,'1999',NULL,'Optimization of Instruction Fetch for Decision Support Workloads','Instruction fetch bandwidth is feared to be a major limiting factor to the performance of future wide-issue aggressive superscalars.In this paper, we focus on Database applications running Decision Support workloads. We characterize the locality patterns of ia database kernel and find frequently executed paths. Using this information, we propose an algorithm to lay out the basic blocks for improved I-fetch.Our results show a miss reduction of 60-98% for realistic I-cache sizes and a doubling of the number of instructions executed between taken branches. As a consequence, we increase the fetch bandwith provided by an aggressive sequential fetch unit from 5.8 for the original code to 10.6 using our proposed layout. Our software scheme combines well with hardware schemes like a Trace Cache providing up to 12.1 instruction per cycle, suggesting that commercial workloads may be amenable to the aggressive I-fetch of future superscalars.',116426,'ICPP \'99 Proceedings of the 1999 International Conference on Parallel Processing',4),(638820,NULL,'1997',NULL,'Set-valued functions and regularity','In this paper, we focus on regularity and set-valued functions. The regularity was first introduced by S.C. Kleene (1952) into the propositional connectives of a ternary logic. Then, M. Mukaidono (1986) expanded the regularity of Kleene into n-variable ternary functions, and a ternary function which is regular is called a regular ternary logic function. Some studies expanded regular ternary logic functions into /spl tau/-valued functions, and studied properties of them. In this paper, we propose another extension of the concepts of the regularity in the sense of Kleene and Mukaidono. That is, we introduce regularity into r-valued set-valued functions. Further, we give properties of the set-valued functions with the regularity.',132247,'ISMVL \'97 Proceedings of the 27th International Symposium on Multiple-Valued Logic',1),(639717,NULL,'1996',NULL,'Error injection aimed at fault removal in fault tolerance mechanisms-criteria for error selection using field data on software faults','Fault injection allows a detailed study of complex interactions between faults and fault handling mechanisms. It can be a useful complement to analytical modeling and formal verification techniques in the testing of fault tolerant systems. However, work on fault injection has not matured adequately to provide industry with cost effective alternatives for the validation of fault tolerant systems. This study analyzes 408 customer discovered faults (defects) in a release of a large operating system product. We discuss methods to select the error types for an error injection experiment in the system test environment, aimed at fault removal. Using four levels of severity and a total of 24 error types as recorded in the customer defects records, we analyze the faults in terms of fault types and system test triggers as defined in ODC. Our work shows examples of criteria that can be used to select errors for injection that use the information from the field reported defects.',132530,'ISSRE \'96 Proceedings of the The Seventh International Symposium on Software Reliability Engineering',6),(647869,NULL,'1989',NULL,'Submicron Systems Architecture Project: Semiannual Technical Report','No abstract available.',226923,'Submicron Systems Architecture Project: Semiannual Technical Report',5),(652981,NULL,'1993',NULL,'Validating timing constraints in multiprocessor and distributed real-time systems','Analytical and efficient validation methods to determine whether all jobs always complete by their deadlines are not yet available for systems using modern dynamic scheduling strategies. Exhaustive methods are often infeasible or unreliable since the execution time and release time of each job may vary. This report presents several worst-case bounds and efficient algorithms for determining how late the completion times of independent jobs with arbitrary release times can be in a dynamic multiprocessor or distributed system. The special cases considered here are when the jobs are (1) preemptable and migratable, or (2) preemptable and nonmigratable, or (3) nonpreemptable.',258820,'Validating timing constraints in multiprocessor and distributed real-time systems',0),(653452,NULL,'2003',NULL,'A 225 MHz resonant clocked ASIC chip','We have recently designed, fabricated, and successfully tested an experimental chip that validates a novel method for reducing clock dissipation through energy recovery. Our approach includes a single-phase sinusoidal clock signal, an L-C resonant sinusoidal clock generator, and an energy recovering flip-flop. Our chip comprises a dual-mode ASIC with two independent clock systems, one conventional and one energy recovering, and was fabricated in a micron bulk CMOS process. The ASIC computes a pipelined discrete wavelet transform with self-test and contains over 3500 gates. We have verified correct functionality and obtained power measurements in both modes of operation for frequencies up to 225MHz. In the energy recovering mode, our power measurements account for all of the dissipation factors, including the operation of the integrated resonant clock generator, and show a net energy savings over the conventional mode of operation. For example, at 115MHz, measured dissipation is between 60% and 75% of the conventional mode, depending on primary input activity. To our knowledge, this is the first ever published account of a direct experimentally-measured comparison between a complete energy recovering ASIC chip and its conventional implementation correctly operating in silicon at frequencies exceeding 100MHz.',190940,'Proceedings of the 2003 international symposium on Low power electronics and design',10),(653462,NULL,'2003',NULL,'Design methodology for fine-grained leakage control in MTCMOS','Multi-threshold CMOS is a popular technique for reducing standby leakage power with low delay overhead. MTCMOS designs typically use large sleep devices to reduce standby leakage at the block level. We provide a formal examination of sneak leakage paths and a design methodology that enables gate-level insertion of sleep devices for sequential and combinational circuits. A fabricated 0.13 μm, dual V T testchip employs this methodology to implement a low-power FPGA core with gate-level sleep FETs and over 8X measured standby current reduction. The methodology allows local sleep regions that reduce leakage in active CLBs by up to 2.2X (measured) for some CLB configurations.',190940,'Proceedings of the 2003 international symposium on Low power electronics and design',10),(653520,NULL,'2003',NULL,'Exploiting program hotspots and code sequentiality for instruction cache leakage management','Leakage energy optimization for caches has been the target of much recent effort. In this work, we focus on instruction caches and tailor two techniques that exploit the two major factors that shape the instruction access behavior, namely, hotspot execution and sequentiality. First, we adopt a hotspot detection mechanism by profiling the branch behavior at runtime and utilize this to implement a HotSpot based Leakage Management (HSLM) mechanism. Second, we exploit code sequentiality in implementing a Just-In-Time Activation (JITA) that transitions cache lines to active mode just before they are accessed. We utilize the recently proposed drowsy cache that dynamically scales voltages for leakage reduction and implement various schemes that use different combinations of HSLM and JITA. Our experimental evaluation using the SPEC2000 benchmark suite shows that instruction cache leakage energy consumption can be reduced by 63%, 49% and 29%, on the average, as compared to an unoptimized cache, a recently proposed hardware optimized cache, and a cache optimized using compiler, respectively. Further, we observe that these energy savings can be obtained without a significant impact on performance.',190940,'Proceedings of the 2003 international symposium on Low power electronics and design',4),(653812,NULL,'2003',NULL,'Constant-time distributed dominating set approximation','Finding a small dominating set is one of the most fundamental problems of traditional graph theory. In this paper, we present a new fully distributed approximation algorithm based on LP relaxation techniques. For an arbitrary parameter k and maximum degree Δ, our algorithm computes a dominating set of expected size O(kΔ2/k log Δ|DSOPT|) in O(k2) rounds where each node has to send O(k2Δ) messages of size O(logΔ). This is the first algorithm which achieves a non-trivial approximation ratio in a constant number of rounds.',195233,'Proceedings of the twenty-second annual symposium on Principles of distributed computing',9),(654203,NULL,'2001',NULL,'Binary Multiplication Radix-32 and Radix-256','Abstract: Multipliers are used at many different places in microprocessor design. As the non-memory sub-blocks of the microprocessor with the largest size and delay, multipliers have a big impact on the cycle time of the microprocessor. Targeting deeper pipelines and higher clock frequencies, there is a growing demand for multiplier designs that can be split into shorter stages. For this purpose, the use of Booth recoding has been a popular method to cut down the number of partial products in a multiplier, to reduce the delay of the partial product accumulation and to simplify the partition of the multiplier into several shorter stages. The complexity to pre-compute an increasing number of digit multiples of the multiplicand within the multiplier unit limits the use of Booth recoding mainly to radices 4 and 8. We propose novel encoding schemes for the implementation of higher radix multiplication. In particular, we consider multiplication radix-32 and radix-256. In the high-radix representations each digit of the multiplier is represented in a secondary radix which is 7 in the case of radix-32 and which is 11 in the case of radix-256, so that the multiplier is represented by roughly 2p/5 resp. 3p/8 terms. All non zero digits of the secondary radix system are a power of two, simplifying partial product generation. The partial products depending on multiples of the radices 7 or 11 can be separately accumulated, with multiplication by the radix a pre- or post-computation option. These features provide more flexible multiplier designs that can be implemented in shorter pipeline stages. We compare the proposed designs with multipliers that use traditional Booth recoding.',33614,'ARITH \'01 Proceedings of the 15th IEEE Symposium on Computer Arithmetic',8),(655805,NULL,'2002',NULL,'New Techniques for Speeding-Up Fault-Injection Campaigns','Fault-tolerant circuits are currently required in severalmajor application sectors, and a new generation of CADtools is required to automate the insertion and validationof fault-tolerant mechanisms. This paper outlines thecharacteristics of a new fault-injection platform and itsevaluation in a real industrial environment. It also detailstechniques devised and implemented within the platform tospeed-up fault-injection campaigns. Experimental resultsare provided, showing the effects of the differenttechniques, and demonstrating that they are able to reducethe total time required by fault-injection campaigns by atleast one order of magnitude.',193651,'Proceedings of the conference on Design, automation and test in Europe',6),(656784,NULL,'2001',NULL,'Planar Graphs, Negative Weight Edges, Shortest Paths, and Near Linear Time','In this paper, we present an 0(n\\log ^3 n) time algorithm for finding shortest paths in a planar graph with real weights.This can be compared to the best previous strongly polynomial time algorithm developed by Lipton, Rose, and Tarjan in 1978 which ran in 0(n^{{3 \\mathord{\\left/ {\\vphantom {3 2}} \\right. \\kern-\\nulldelimiterspace} 2}} ) time, and the best polynomial algorithm developed by Henzinger, Klein, Subramanian, and Rao in 1994 which ran in \\widetilde0(n^{{4 \\mathord{\\left/ {\\vphantom {4 3}} \\right. \\kern-\\nulldelimiterspace} 3}} ) time.We also present significantly improved algorithms for query and dynamic versions of the shortest path problems.',98762,'FOCS \'01 Proceedings of the 42nd IEEE symposium on Foundations of Computer Science',9),(656981,NULL,'1996',NULL,'Delay Hazards in Complex Gate Based Speed Independent VLSI Circuits','Although speed independent VLSI circuit design is supported by rich theory at higher levels, it suffers from the lack of an area efficient robust transistor level implementation technique. In this paper we introduce safe cells based on which well-formed STGs can be implemented free of (delay) hazards with no unrealistic assumptions about physical gates. Although this technique still compromises chip area for the sake of preventing hazards, we show that it may achieve a significant area gain in comparison with the two-phase RS-implementation method, which is one of the few true speed independent implementation techniques that we are aware of so far. Delay hazards are then analysed in complex gate based speed independent circuits and hence theorems are developed to identify a subclass of delay hazards.',105578,'GLSVLSI \'96 Proceedings of the 6th Great Lakes Symposium on VLSI',10),(657730,NULL,'2003',NULL,'Priority-Driven Scheduling of Periodic Task Systems on Multiprocessors','The scheduling of systems of periodic tasks upon multiprocessor platforms is considered. Utilization-based conditions are derived for determining whether a periodic task system meets all deadlines when scheduled using the earliest deadline first scheduling algorithm (EDF) upon a given multiprocessor platform. A new priority-driven algorithm is proposed for scheduling periodic task systems upon multiprocessor platforms: this algorithm is shown to successfully schedule some task systems for which EDF may fail to meet all deadlines.',202759,'Real-Time Systems',0),(660120,NULL,'2001',NULL,'Minimal Subset Evaluation: Rapid Warm-Up for Simulated Hardware State','Abstract: A new asynchronous pipeline design is introduced for high-speed applications. The pipeline uses simple transparent latches in its datapath, and small latch controllers consisting of only a single gate per pipeline stage. This simple stage structure ...',115301,'ICCD \'01 Proceedings of the International Conference on Computer Design: VLSI in Computers & Processors',10),(660162,NULL,'2001',NULL,'Arithmetic Logic Circuits using Self-Timed Bit Level Dataflow and Early Evaluation','Abstract: A new asynchronous pipeline design is introduced for high-speed applications. The pipeline uses simple transparent latches in its datapath, and small latch controllers consisting of only a single gate per pipeline stage. This simple stage structure ...',115301,'ICCD \'01 Proceedings of the International Conference on Computer Design: VLSI in Computers & Processors',10),(660918,NULL,'1995',NULL,'Schedulability-oriented replication of periodic tasks in distributed real-time systems','Abstract: We consider the schedulability-oriented replication problem of a set of periodic real-time tasks where each task can be decomposed into several modules and intermodule communications. The objective is to find an allocation in which there exists a feasible schedule for the given task set. In this paper, we adopt a communication model where the replication of modules is not for the sake of fault tolerance but for increasing the degree of schedulability. To solve the problem, we develop a replication technique and embed the technique in a simulated annealing algorithm. Experimental results show that such replication may lead to a higher degree of schedulability and obtain a feasible solution.',115570,'ICDCS \'95 Proceedings of the 15th International Conference on Distributed Computing Systems',0),(664061,NULL,'2002',NULL,'Efficiently Maintaining Stock Portfolios Up-To-Date On The Web','Consider a continuous query where a user wants to be informed when the net worth of his/her stock portfolio changes by more than a specified threshold. In this paper we develop a data dissemination technique for the Web where (a) such queries access data from multiple sources and (b) the HTTP protocol - which is inherently pull based - is used for accessing the sources. Key challenges in supporting such queries - which also arise in a diverse set of contexts including monitoring of patients, network traffic, and experiments - lie in meeting users\' consistency requirements while minimizing network and server overheads, without the loss of fidelity in the responses provided to users. We also show the superior performance of our technique when compared to alternatives based on periodic independent polling of the sources.',206803,'RIDE \'02 Proceedings of the 12th International Workshop on Research Issues in Data Engineering: Engineering E-Commerce/E-Business Systems (RIDE\'02)',3),(664159,NULL,'2001',NULL,'Static-Priority Scheduling on Multiprocessors','The preemptive scheduling of systems of periodic taskson a platform comprised of several identical processors isconsidered.A scheduling algorithm is proposed for staticpriority scheduling of such systems; this algorithm isa simple extension of the unprocessed rate-monotonic scheduling algorithm.It is proven that this algorithm successfully schedules any periodic task system with aworst-case utilization no more than a third the capacity of the multiprocessor platform.It is also shown that no static-priority multiprocessor scheduling algorithm (partitioned or global) can guarantee schedulability ora periodic task set with a utilization higher than one halfthe capacity of the multiprocessor platform.',208410,'RTSS \'01 Proceedings of the 22nd IEEE Real-Time Systems Symposium',0),(664161,NULL,'2001',NULL,'On-Line Scheduling on Uniform Multiprocessors','Each processor in a uniform multiprocessor machine ischaracterized by a speed or computing capacity, with theinterpretation that a job executing on a processor with speeds for t time units completes (s X t) units of execution.Theon-line scheduling of hard-real-time systems, in which alljobs must complete by specified deadlines, on uniformmultiprocessor machines is considered.It is known that on-line algorithms tend to perform very poorly in schedulingsuch hard-real-time systems on multiprocessors; resource-augmentation techniques are presented here thatpermit on-line algorithms to perform better than may be expectedgiven the inherent limitations.Results derived here are appliedto the scheduling of periodic task systems on uniformmultiprocessor machines.',208410,'RTSS \'01 Proceedings of the 22nd IEEE Real-Time Systems Symposium',0),(664661,NULL,'1997',NULL,'Analysis of a Denial of Service Attack on TCP','This paper analyzes a network-based denial of service attack for IP (Internet Protocol) based networks. It is popularly called SYN flooding. It works by an attacker sending many TCP (Transmission Control Protocol) connection requests with spoofed source addresses to a victim\'s machine. Each request causes the targeted host to instantiate data structures out of a limited pool of resources. Once the target host\'s resources are exhausted, no more incoming TCP connections can be established, thus denying further legitimate access.The paper contributes a detailed analysis of the SYN flooding attack and a discussion of existing and proposed countermeasures. Furthermore, we introduce a new solution approach, explain its design, and evaluate its performance. Our approach offers protection against SYN flooding for all hosts connected to the same local area network, independent of their operating system or networking stack implementation. It is highly portable, configurable, extensible, and requires neither special hardware, nor modifications in routers or protected end systems.',221394,'SP \'97 Proceedings of the 1997 IEEE Symposium on Security and Privacy',3),(664670,NULL,'1997',NULL,'Execution monitoring of security-critical programs in distributed systems: a Specification-based approach','Abstract: We describe a specification-based approach to detect exploitations of vulnerabilities in security-critical programs. The approach utilizes security specifications that describe the intended behavior of programs and scans audit trails for operations that are in violation of the specifications. We developed a formal framework for specifying the security-relevant behavior of programs, on which we based the design and implementation of a real-time intrusion detection system for a distributed system. Also, we wrote security specifications for 15 Unix setuid root programs. Our system detects attacks caused by monitored programs, including security violations caused by improper synchronization in distributed programs. Our approach encompasses attacks that exploit previously unknown vulnerabilities in security-critical programs.',221394,'SP \'97 Proceedings of the 1997 IEEE Symposium on Security and Privacy',5),(665183,NULL,'1995',NULL,'Compact test generation for bridging faults under I/sub DDQ/ testing','Abstract: We propose a procedure to generate compact test sets for bridging faults under I/sub DDQ/ testing. Several techniques are employed to achieve compact test sets. Heuristics developed for stuck-at faults are shown to be effective in this context. The techniques especially designed for bridging faults are based on the observation that the yet-undetected faults can be represented using sets of lines and that a minimum test set size is obtained if the line sets representing yet-undetected faults are halved with every additional test vector. Logic blocks called bit-adders allow the partitioning of the line sets using a test generator for stuck-at faults, without having to determine in advance how the lines in a given set will be divided. Thus partitioning can be performed in a cost effective way for any line set size. Experimental results show that the test sets generated by the proposed procedure are smaller than those obtained by previously proposed procedures.',262219,'VTS \'95 Proceedings of the 13th IEEE VLSI Test Symposium',11),(668721,NULL,'1970',NULL,'COMPUTER RECOGNITION OF PRISMATIC SOLIDS','An investigation is made into the problem of constructing a model of the appearance to an optical input device of scenes consisting of plane-faced geometric solids. The goal is to study algorithms which find the real straight edges in the scenes, taking into account smooth variations in intensity over the faces of the solids, blurring of edges and noise. A general mathematical analysis is made of optimal methods of identifying the edge lines in figures, given a raster of intensities covering the entire field of view. There is given in addition a suboptimal statistical decision procedure, based on the model, for the identification of a line within a narrow band on the field of view given an array of intensities from within the band. A computer program has been written and extensively tested which implements this procedure and extracts lines from real scenes. Other programs were written which judge the completeness of extracted sets of lines, and propose and text for additional lines which had escaped initial detection. The performance of these programs is discussed in relation to the theory derived from the model, and with regard to their use of global information in detecting and proposing lines.',58205,'COMPUTER RECOGNITION OF PRISMATIC SOLIDS',2),(669858,NULL,'1980',NULL,'SCHEDULING TASK SYSTEMS WITH RESOURCES','Minimum execution time scheduling of task systems with resources has been the subject of several papers over the past few years. The model used for much of this work assumes that the resources associated with computer systems - readers, printers, disk drives - are not \"continuous\" resources. We present an alternative model of task systems with resources in which the resources are discrete. That is, there are a specific number of indivisible units of each resource and a task may require only integral numbers of those units. Several results involving the worst case performance of list scheduling and critical path scheduling with respect to this model are given. A new result on critical path scheduling of task systems with continuous resources is also given. Finally, a comparison will be made between corresponding bounds for the continuous and discrete models.',211475,'SCHEDULING TASK SYSTEMS WITH RESOURCES',0),(672335,NULL,'1984',NULL,'Parallelism and greedy algorithms','A number of greedy algorithms are examined and are shown to be probably inherently sequential. Greedy algorithms are presented for finding a maximal path, for finding a maximal set of disjoint paths in a layered dag, and for finding the largest induced subgraph of a graph that has all vertices of degree at least k. It is shown that for all of these algorithms, the problem of determining if a given node is in the solution set of the algorithm is P-complete. This means that it is unlikely that these sequential algorithms can be sped up significantly using parallelism.',179099,'Parallelism and greedy algorithms',9),(672745,NULL,'1979',NULL,'ADLIB user\'\'s manual','ADLIB (A Design Language for Indicating Behavior) is a new computer design language recently developed at Stanford. ADLIB is a superset of PASCAL with special facilities for concurrency and interprocess communication. It is normally used under the SABLE simulation system.',18208,'ADLIB user\'\'s manual',5),(674297,NULL,'1990',NULL,'SRP: A Resource Reservation Protocol for Guaranteed-Performance','This report describes the Session Reservation Protocol (SRP), SRP is defined in the DARPA Internet family of protocols. It allows communicating peer entities to reserve the resources, such as CPU and network bandwidth, necessary to achieve given performance objectives (delay and throughput). The immediate goal of SRP is to support \"continuous media\" (digital audio and video) in IP-based distributed systems. However, it is applicable to any application that requires guaranteed-performance network communication. The design goals of SRP include 1) independence from transport protocols (SRP can be used with standard protocols such as TCP or with new real-time protocols); 2) compatibility with IP (data packets are not modified); 3) a host implementing SRP can benefit from its use even when communicating with hosts not supporting SRP. SRP is based on a workload and scheduling model called the DASH resource model. This model defines a parameterization of client workload, an abstract interface for hardware resources, and an end-to-end algorithm for negotiated resources reservation based on cost minimization. SRP implements this end-to-end algorithm, handling those resources related to network communication.',223471,'SRP: A Resource Reservation Protocol for Guaranteed-Performance',3),(674545,NULL,'1984',NULL,'Development of a Control Process for the Berkeley UNIX Distributed','Conventional programs have a single stream of execution. Distributed programs expand on this notion, having multiple streams of execution that interact with each other. This expansion increases the complexity of a program\'\'s behavior. Existing program monitors do not provide enough information to deal with all of the problems of a distributed computing environment. This paper is concerned with the development of a programming tool for Berkeley UNIX whose goal is to characterize the performance of distributed programs. The Distributed Programs Monitor (DPM) monitors specifically the interactions between the processes of a distributed program and provides routines to analyze the resulting data. DPM is a tool composed of independent subtools that work together to monitor a distributed program. This paper will present an overview of DPM including a bit of its history and a bit of its experimental use, but it deals primarily with the development of a control process for the monitor. The design and the implementation of this process is described. A major issue of the design addresses how to provide distributed process management in a non- distributed processing environment.',75347,'Development of a Control Process for the Berkeley UNIX Distributed',5),(674686,NULL,'1995',NULL,'Performance Evaluation of Cache Prefetching Strategies','Prefetching into CPU cache has long been known to be effective in reducing the cache miss ratio, but implementations of prefetching have been unsuccessful in improving CPU performance. The reasons for this are that prefetches interfere with normal cache operation by making cache address and data ports busy, the memory bus busy and the memory banks busy, and by not necessarily being complete by the time that the prefetched data is actually referenced. In this paper, we present the results of a very detailed cycle by cycle trace driven stimulation of a uniprocessor memory system, in which we vary several relevant parameters in order to determine when and if prefetching is useful. We find that in order for prefetching to actually improve performance, the address array needs to be double ported, and the data array needs to either be double ported or fully buffered. It is also very helpful for the bus to be reasonably wide, bus transactions to be split and main memory to be interleaved. Under the best circumstances, i.e. with a significant investment in extra hardware, prefetching can significantly improve performances.',181712,'Performance Evaluation of Cache Prefetching Strategies',4),(680789,NULL,'1997',NULL,'Process Introspection: A Heterogeneous Checkpoint/Restart Mechanism Based on Automatic Code Modification','Process Introspection is a fundamentally new solution to the process checkpoint/restart problem suitable for use in high-performance heterogeneous distributed systems. A process checkpoint/restart mechanism for such an environment has the primary requirement that it must be platform-independent: process checkpoints produced on a computer system of one architecture or operating system platform must be restartable on a computer system of a different architecture or operating system platform. The central feature of the Process Introspection approach is automatic augmentation of program code to incorporate checkpoint and restart functionality. This program modification is performed at a platform-independent intermediate level of code representation, and preserves the original program semantics. This approach has attractive properties including portability, ease of use, customizability to application-specific requirements, and flexibility with respect to basic performance trade-offs. Our solution is novel in its true platform- and run-time system independence - no system support or non-portable code is required by our core mechanisms. Recent experimental results obtained using a prototype implementation of the Process Introspection system indicate the overheads introduced by the mechanisms are acceptable for computationally demanding applications.',195489,'Process Introspection: A Heterogeneous Checkpoint/Restart Mechanism Based on Automatic Code Modification',5),(718326,NULL,'2003',NULL,'ARC: an integrated admission and rate control framework for CDMA data networks based on non-cooperative games','The competition among wireless data service providers brings in an option for the customers to switch their providers, due to unsatisfactory service or otherwise. However, the existing resource management algorithms for wireless networks fail to fully capture the far-reaching impact of this competitiveness. From this perspective, we propose an integrated admission and rate control (ARC) framework for CDMA based wireless data networks. The admission control is at the session (macro) level while the rate control is at the link layer packet (micro) level. The ARC framework is based on a novel game theoretic formulation which defines non-cooperative games between the service providers and the customers. A user\'s decision to leave or join a provider is based on a finite set of strategies. A service provider can also construct its game strategy set so as to maximize the utility (revenue) yet attaining its target churn rate (the probability of users leaving the network). We show that the pure strategy Nash equilibrium can be established for both under-loaded and fully-loaded systems such that the providers have clearly defined admission criteria as outcome from this game. Users are categorized into multiple classes and offered differentiated services based on the price they pay and the service degradation they can tolerate. We show that the proposed ARC framework significantly increases the provider\'s revenue and also successfully offers differentiated QoS to the users.',193402,'Proceedings of the 9th annual international conference on Mobile computing and networking',3),(718765,NULL,'1999',NULL,'Addressable Test Ports An Approach to Testing Embedded Cores','Intellectual property (IP) core reuse is an emerging designstyle that will significantly accelerate the complexity of ICs. IPcores are predesigned circuit functions that can be selected froma library and integrated into system ICs to quickly providehighly complex silicon solutions. Low cost, efficient testing ofsystem ICs designed with IP cores will be challenging.',132990,'ITC \'99 Proceedings of the 1999 IEEE International Test Conference',10),(719116,NULL,'2003',NULL,'Challenges in Computer Architecture Evaluation','Reasoning about today\'s tremendously complex computer systems is difficult and developing them is expensive. Detailed software simulations are thus essential for evaluating computer architecture ideas. Industry uses simulation extensively during processor and system design as the easiest and least expensive way to explore design options.Unfortunately, constructing accurate models of modern computer systems is becoming harder and more time-consuming, while the effort required to develop high-fidelity simulation tools typically yields few academic rewards. Without funding and promising prospects for academic recognition, research and development in these areas will likely languish.',56357,'Computer',5),(719210,NULL,'2003',NULL,'Rate-Monotonic Scheduling on Uniform Multiprocessors','The rate-monotonic algorithm is arguably one of the most popular algorithms for scheduling systems of periodic real-time tasks. The rate-monotonic scheduling of systems of periodic tasks on uniform multiprocessor platforms is considered here. A simple, sufficient test is presented for determining whether a given periodic task system will be successfully scheduled by this algorithm upon a particular uniform multiprocessor platform驴this test generalizes earlier results concerning rate-monotonic scheduling upon identical multiprocessor platforms.',117351,'IEEE Transactions on Computers',0),(719433,NULL,'2003',NULL,'UNIX Network Programming, Vol. 1','From the Publisher:The leading book in its field, this guide focuses on the design, development and coding of network software under the UNIX operating system. Provides over 15,000 lines of C code with descriptions of how and why a given solution is achieved. For programmers seeking an indepth tutorial on sockets, transport level interface (TLI), interprocess communications (IPC) facilities under System V and BSD UNIX.',255651,'UNIX Network Programming, Vol. 1',5),(721119,NULL,'2003',NULL,'Comparison of Physical and Software-Implemented Fault Injection Techniques','This paper addresses the issue of characterizing the respective impact of fault injection techniques. Three physical techniques and one software-implemented technique that have been used to assess the fault tolerance features of the MARS fault-tolerant distributed real-time system are compared and analyzed. After a short summary of the fault tolerance features of the MARS architecture and especially of the error detection mechanisms that were used to compare the erroneous behaviors induced by the fault injection techniques considered, we describe the common distributed testbed and test scenario implemented to perform a coherent set of fault injection campaigns. The main features of the four fault injection techniques considered are then briefly described and the results obtained are finally presented and discussed. Emphasis is put on the analysis of the specific impact and merit of each injection technique.',117351,'IEEE Transactions on Computers',6),(721123,NULL,'2003',NULL,'Robustness Results Concerning EDF Scheduling upon Uniform Multiprocessors','Each processor in a uniform multiprocessor machine is characterized by a speed or computing capacity, with the interpretation that a job executing on a processor with speed s for t time units completes (s x t) units of execution. The earliest-deadline-first (EDF) scheduling of hard-real-time systems upon uniform multiprocessor machines is considered. It is known that online algorithms tend to perform very poorly in scheduling such hard-real-time systems on multiprocessors; resource-augmentation techniques are presented here that permit online algorithms in general (EDF in particular) to perform better than may be expected given these inherent limitations. It is shown that EDF scheduling upon uniform multiprocessors is robust with respect to both job execution requirements and processor computing capacity.',117351,'IEEE Transactions on Computers',0),(721126,NULL,'2003',NULL,'VLSI Implementation of a Low-Power Antilogarithmic Converter','This paper presents a VLSI implementation of a unique 32-bit antilogarithmic converter, which generates data for some digital-signal-processing (DSP) applications. Novel antilogarithm correcting algorithms are developed and implemented with low-power and hardware-efficient correcting circuits. The VLSI implementations of these algorithms are much smaller than other hardware intensive algorithms found in the literature. The converter is implemented using 0.6 \\mum CMOS technology, and its combinational logic implementation requires 1,500\\lambda\\times2,800\\lambda of chip area. The 32-bit antilogarithmic converter computes the antilogarithm in a single clock cycle and runs at 100 MHz and consumes 81 milliwatts.',117351,'IEEE Transactions on Computers',10),(721463,NULL,'2003',NULL,'RDSP: A RISC DSP based on Residue Number System','This paper is focused on low power programmable fastDigital Signal Processors (DSP) design based on a configurable5-stage RISC core architecture and on Residue Number Systems (RNS).Several innovative aspects are introducedat the control and datapath architecture levels, whichsupport both the binary system and the RNS.A new moduliset {2n-1, 2n+1} is also proposed for balancing theprocessing time in the different RNS channels.Experimentalresults, obtained trough RDSP implementation on FPGAand ASIC, show that not only a significant reduction in circuitarea and power consumption but also a speedup maybe achieved with RNS when compared with a binary DSP.',81022,'DSD \'03 Proceedings of the Euromicro Symposium on Digital Systems Design',10),(722702,NULL,'2003',NULL,'A codesigned on-chip logic minimizer','Boolean logic minimization is traditionally used in logic synthesis tools running on powerful desktop computers. However, logic minimization has recently been proposed for dynamic use in embedded systems, including network route table reduction, network access control list table reduction, and dynamic hardware/software partitioning. These new uses require logic minimization to run dynamically as part of an embedded system\'s active operation. Performing such dynamic logic minimization on-chip greatly reduces system complexity and security versus an approach that involves communication with a desktop logic minimizer. An on-chip minimizer must be exceptionally lean yet yield good enough results. Previous software-only on-chip minimizer results have been good, but we show that a codesigned minimizer can be much better, executing nearly 8 times faster and consuming nearly 60% less energy, while yielding identical results.',190728,'Proceedings of the 1st IEEE/ACM/IFIP international conference on Hardware/software codesign and system synthesis',10),(727642,NULL,'2003',NULL,'Enhanced Cluster k-Ary n-Cube, A Fault-Tolerant Multiprocessor','In this paper, we present a strongly fault-tolerant design for the k-ary n-cube multiprocessor and examine its reconfigurability. Our design augments the k-ary n-cube with {({\\frac{k}{j}})}^n spare nodes. Each set of j^n regular nodes is connected to a spare node and the spare nodes are interconnected as either a ({\\frac{k}{j}}){\\hbox{-}}{\\rm ary}n-cube if j \\ne {\\frac{k}{2}} or a hypercube of dimension n if j = {\\frac{k}{2}}. Our approach utilizes the capabilities of the wave-switching communication modules of the spare nodes to tolerate a large number of faulty nodes. Both theoretical and experimental results are examined. Compared with other proposed schemes, our approach can tolerate significantly more faulty nodes with a low overhead and no performance degradation.',117351,'IEEE Transactions on Computers',7),(729748,NULL,'2003',NULL,'Flexible Compiler-Managed L0 Buffers for Clustered VLIW Processors','Wire delays are a major concern for current and forthcoming processors.One approach to attack this problem is to divide the processorinto semi-independent units referred to as clusters. Acluster usually consists of a local register file and a subset of thefunctional units, while the data cache remains centralized. However,as technology evolves, the latency of such a centralizedcache will increase leading to an important performance impact.In this paper we propose to include flexible low-latency buffers ineach cluster in order to reduce the performance impact of highercache latencies. The reduced number of entries in each buffer permitsthe design of flexible ways to map data from L1 to these buffers.The proposed L0 buffers are managed by the compiler, whichis responsible to decide which memory instructions make use ofthem.Effective instruction scheduling techniques are proposed togenerate code that exploits these buffers. Results for the Media-benchbenchmark suite show that the performance of a clusteredVLIW processor with a unified L1 data cache is improved by 16%when such buffers are used. In addition, the proposed architecturealso shows significant advantages over both MultiVLIW processorsand a clustered processors with a word-interleaved cache,two state-of-the-art designs with a distributed L1 data cache.',192296,'Proceedings of the 36th annual IEEE/ACM International Symposium on Microarchitecture',4),(729780,NULL,'2003',NULL,'Scheduling Real-Time Dwells Using Tasks with Synthetic Periods','This paper addresses the problem of scheduling real-timedwells in multi-function phase array radar systems.To keep track of targets, a radar system must meet its timingand energy constraints. We propose a new task modelfor radar dwells to accurately characterize their timing parameters.We develop an algorithm of transforming everydwell task as a semi-period task so the dwell task can meetits timing constraint and the interarrival times of the taskmay not a constant. We also develop an enhanced template-basedscheduling algorithm to schedule such tasks to meetthe timing and energy constraints. Simulation results showthat this algorithm can significantly improve the resourceutilization.',208412,'RTSS \'03 Proceedings of the 24th IEEE International Real-Time Systems Symposium',0),(729808,NULL,'2003',NULL,'Multiprocessor EDF and Deadline Monotonic Schedulability Analysis','Schedulability tests are presented for preemptiveearliest-deadline-first and deadline-monotonic schedulingof periodic or sporadic real-time tasks on a single-queue-server system, in which the deadline of a task may be less than or equal to the task period. These results subsumeand generalize several known utilization-based multiprocessorschedulability tests, and are derived via anindependent proof.',208412,'RTSS \'03 Proceedings of the 24th IEEE International Real-Time Systems Symposium',0),(729810,NULL,'2003',NULL,'Multiple-Resource Periodic Scheduling Problem: how much fairness is necessary?','The Pfair algorithms are optimal for independent periodicreal-time tasks executing on a multiple-resource system,however, they incur a high scheduling overhead bymaking scheduling decisions in every time unit to enforceproportional progress for each task. In this paper, we willpropose a novel scheduling algorithm, boundary fair (),which makes scheduling decisions and enforces fairness totasks only at period boundaries. The BF algorithm is alsooptimal in the sense that it achieves 100% system utilization.Moreover, by making scheduling decisions at periodboundaries, BF effectively reduces the number of schedulingpoints. Theoretically, the BF algorithm has the samecomplexity as that of the Pfair algorithms. But, in practice,it could reduce the number of scheduling points dramatically(e.g., upto 75% in our experiments) and thus reducethe overall scheduling overhead, which is especially importantfor on-line scheduling.',208412,'RTSS \'03 Proceedings of the 24th IEEE International Real-Time Systems Symposium',0),(732499,NULL,'2003',NULL,'A BIST Pattern Generator Design for Near-Perfect Fault Coverage','Abstract--A new design methodology for a pattern generator is proposed, formulated in the context of on-chip BIST. The design methodology is circuit-specific and uses synthesis techniques to design BIST generators. The pattern generator consists of two components: a pseudorandom pattern generator (like an LFSR or, preferably, a GLFSR) and a combinational logic to map the outputs of the pseudorandom pattern generator. This combinational logic is synthesized to produce a given set of target patterns by mapping the outputs of the pseudorandom pattern generator. It is shown that, for a particular CUT, an area-efficient combinational logic block can be designed/synthesized to achieve 100 (or almost 100) percent single stuck-at fault coverage using a small number of test patterns. This method is significantly different from weighted pattern generation and can guarantee testing of all hard-to-detect faults without expensive test point insertion. Experimental results on common benchmark netlists demonstrate that the fault coverage of the proposed pattern generator is significantly higher compared to conventional pattern generation techniques. The design technique for the logic mapper is unique and can be used effectively to improve existing pattern generators for combinational logic and scan-based BIST structures.',117351,'IEEE Transactions on Computers',11),(732511,NULL,'2004',NULL,'DPR, LPR: Proactive Resource Allocation Algorithms for Asynchronous Real-Time Distributed Systems','Abstract-- We present two proactive resource allocation algorithms, called DPR and LPR, for satisfying the timeliness requirements of real-time tasks in asynchronous real-time distributed systems. The algorithms are proactive in the sense that they allow application-specified and user-triggered resource allocation by allowing anticipated task workloads to be specified for future time intervals. When proactively triggered, the algorithms allocate resources to maximize the aggregate deadline-satisfied ratio for the future time interval under the anticipated workload. While DPR uses the Earliest Deadline First scheduling algorithm as the underlying algorithm for process scheduling and packet scheduling, LPR uses a modified Least Laxity First scheduling algorithm. We show that LPR is computationally more expensive than DPR. Further, our experimental studies reveal that LPR yields a higher deadline-satisfied ratio than DPR.',117351,'IEEE Transactions on Computers',0),(733612,NULL,'2004',NULL,'Improving Scheduling of Tasks in a Heterogeneous Environment','Abstract--Optimal scheduling of parallel tasks with some precedence relationship, onto a parallel machine is known to be NP-complete. The complexity of the problem increases when task scheduling is to be done in a heterogeneous environment, where the processors in the network may not be identical and take different amounts of time to execute the same task. This paper introduces a Task duplication-based scheduling Algorithm for Network of Heterogeneous systems (TANH), with complexity $\\rm O(V^2)$, which provides optimal results for applications represented by Directed Acyclic Graphs (DAGs), provided a simple set of conditions on task computation and network communication time could be satisfied. The performance of the algorithm is illustrated by comparing the scheduling time with an existing 驴Best Imaginary Level scheduling (BIL)驴 scheme for heterogeneous systems. The scalability for a higher or lower number of processors, as per their availability is also discussed. This work is shown to provide substantial improvement over existing work on the Task Duplication-Based Scheduling Algorithm (TDS).',117407,'IEEE Transactions on Parallel and Distributed Systems',0),(735978,NULL,'2003',NULL,'Protocol design for scalable and reliable group rekeying','We present the design and specification of a protocol for scalable and reliable group rekeying together with performance evaluation results. The protocol is based upon the use of key trees for secure groups and periodic batch rekeying. At the beginning of each rekey interval, the key server sends a rekey message to all users consisting of encrypted new keys (encryptions, in short) carried in a sequence of packets. We present a scheme for identifying keys, encryptions, and users, and a key assignment algorithm that ensures that the encryptions needed by a user are in the same packet. Our protocol provides reliable delivery of new keys to all users eventually. It also attempts to deliver new keys to all users with a high probability by the end of the rekey interval. For each rekey message, the protocol runs in two steps: a multicast step followed by a unicast step. Proactive forward error correction (FEC) multicast is used to reduce delivery latency. Our experiments show that a small FEC block size can be used to reduce encoding time at the server without increasing server bandwidth overhead. Early transition to unicast, after at most two multicast rounds, further reduces the worst-case delivery latency as well as user bandwidth requirement. The key server adaptively adjusts the proactivity factor based upon past feedback information; our experiments show that the number of NACKs after a multicast round can be effectively controlled around a target number. Throughout the protocol design, we strive to minimize processing and bandwidth requirements for both the key server and users.',117498,'IEEE/ACM Transactions on Networking (TON)',3),(737553,NULL,'2004',NULL,'Highly pipelined asynchronous FPGAs','We present the design of a high-performance, highly pipelined asynchronous FPGA. We describe a very fine-grain pipelined logic block and routing interconnect architecture, and show how asynchronous logic can efficiently take advantage of this large amount of pipelining. Our FPGA, which does not use a clock to sequence computations, automatically self-pipelines\" its logic without the designer needing to be explicitly aware of all pipelining details. This property makes our FPGA ideal for throughput-intensive applications and we require minimal place and route support to achieve good performance. Benchmark circuits taken from both the asynchronous and clocked design communities yield throughputs in the neighborhood of 300--400 MHz in a TSMC 0.25m process and 500--700 MHz in a TSMC 0.18m process.',100301,'FPGA \'04 Proceedings of the 2004 ACM/SIGDA 12th international symposium on Field programmable gate arrays',10),(737950,NULL,'2004',NULL,'Assessing and Improving TCP Rate Shaping over Edge Gateways','Abstract--Computers installed with commercial/open-source software have been widely employed as organizational edge gateways to provide policy-based network management. Such gateways include firewalls for access control, and bandwidth managers for managing the narrow Internet access links. When managing the TCP traffic, pass-through TCP flows can introduce large buffer requirements, large latency, frequent buffer overflows, and unfairness among flows competing for the same queue. So, how to allocate the bandwidth for a TCP flow without the above drawbacks becomes an important issue. This study assesses and improves TCP rate shaping algorithms to solve the above problems through self-developed implementations in Linux, testbed emulations, live Internet measurements, computer simulations, modeling, and analysis. The widely deployed TCP Rate control (TCR) approach is found to be more vulnerable to Internet packet losses and less compatible to some TCP sending operating systems. The proposed PostACK approach can preserve TCR\'s advantages while avoiding TCR\'s drawbacks. PostACK emulates per-flow queuing, but relocates the queuing of data to the queuing of ACKs in the reverse direction, hence minimizing the buffer requirement up to 96 percent. PostACK also has 10 percent goodput improvement against TCR under lossy WAN environments. A further scalable design of PostACK can scale up to 750Mbps while seamlessly cooperating with the link-sharing architecture. Experimental results can be reproduced through our open sources: 1) tcp-masq: a modified Linux kernel, 2) wan-emu: a testbed for conducting switched LAN-to-WAN or WAN-to-LAN experiments with RTT/loss/jitter emulations.',117351,'IEEE Transactions on Computers',3),(737953,NULL,'2004',NULL,'A Utilization Bound for Aperiodic Tasks and Priority Driven Scheduling','Abstract--Real-time scheduling theory offers constant-time schedulability tests for periodic and sporadic tasks based on utilization bounds. Unfortunately, the periodicity or the minimal interarrival-time assumptions underlying these bounds make them inapplicable to a vast range of aperiodic workloads such as those seen by network routers, Web servers, and event-driven systems. This paper makes several important contributions toward real-time scheduling theory and schedulability analysis. We derive the first known bound for schedulability of aperiodic tasks. The bound is based on a utilization-like metric we call synthetic utilization, which allows implementing constant-time schedulability tests at admission control time. We prove that the synthetic utilization bound for deadline-monotonic scheduling of aperiodic tasks is {\\frac 1 {1 + \\sqrt {1/2}}}. We also show that no other time-independent scheduling policy can have a higher schedulability bound. Similarly, we show that EDF has a bound of 1 and that no dynamic-priority policy has a higher bound. We assess the performance of the derived bound and conclude that it is very efficient in hit-ratio maximization.',117351,'IEEE Transactions on Computers',0),(738143,NULL,'2004',NULL,'A Logic Level Design Methodology for a Secure DPA Resistant ASIC or FPGA Implementation','This paper describes a novel design methodology to implement a secure DPA resistant crypto processor. The methodology is suitable for integration in a common automated standard cell ASIC or FPGA design flow. The technique combines standard building blocks to make new\'compound standard cells, which have a close to constant power consumption. Experimental results indicate a 50 times reduction in the power consumption fluctuations.',193652,'Proceedings of the conference on Design, automation and test in Europe - Volume 1',10),(761665,NULL,'2003',NULL,'Efficient and flexible fair scheduling of real-time tasks on multiprocessors','Proportionate fair (Pfair) scheduling is the only known way to optimally schedule periodic real-time task systems on multiprocessors in an on-line manner. Under Pfair scheduling, the execution of each task is broken into a sequence of quantum length sub-tasks that must execute within intervals of approximately-equal lengths. This scheduling policy results in allocations that mimic those of an ideal “fluid” scheduler, and in periodic task systems, ensures that all deadlines are met. Though Pfair scheduling algorithms hold much promise, prior to our work, research on this topic was limited in that, only static systems consisting of synchronous periodic tasks were considered. My dissertation thesis is that the Pfair scheduling framework for the on-line scheduling of real-time tasks on multiprocessors can be made more flexible by allowing the underlying task model to be more general than the periodic model and by allowing dynamic task behaviors. Further, this flexibility can be efficiently achieved. Towards the goal of improving the efficiency of Pfair scheduling algorithms, we develop the PD2 Pfair algorithm, which is the most efficient optimal Pfair scheduling algorithm devised to date. Through a series of counterexamples, we show that it is unlikely that a more efficient optimal Pfair algorithm exists. We also introduce the concept of ERfair scheduling, which is a work-conserving extension of Pfair scheduling. In addition, we study the non-optimal earliest-pseudo-deadline-first (EPDF) Pfair algorithm, which is more efficient than PD2, and present several scenarios under which it is preferable to PD2. We address the flexibility issue by developing the intra-sporadic (IS) task model and by considering the scheduling of dynamic task systems. The well-known sporadic model generalizes the periodic model by allowing jobs to be released late. The IS model generalizes this notion further by allowing late as well as early subtask releases. Such a generalization is useful for modeling applications in which the instantaneous rate of releases differs greatly from the average rate of releases (e.g., an application that receives packets over a network). We prove that PD2 is optimal for scheduling static IS task systems on multiprocessors. In dynamic task systems, tasks are allowed to join and leave, i.e. , the set of tasks is allowed to change. (Abstract shortened by UMI.)',84583,'Efficient and flexible fair scheduling of real-time tasks on multiprocessors',0),(762649,NULL,'2004',NULL,'Resilient and Coherence Preserving Dissemination of Dynamic Data Using Cooperating Peers','The focus of our work is to design and build a dynamic data distribution system that is coherence-preserving, i.e., the delivered data must preserve associated coherence requirements (the user-specified bound on tolerable imprecision) and resilient to failures. To this end, we consider a system in which a set of repositories cooperate with each other and the sources, forming a peer-to-peer network. In this system, necessary changes are pushed to the users so that they are automatically informed about changes of interest. We present techniques 1) to determine when to push an update from one repository to another for coherence maintenance, 2) to construct an efficient dissemination tree for propagating changes from sources to cooperating repositories, and 3) to make the system resilient to failures. An experimental evaluation using real world traces of dynamically changing data demonstrates that 1) careful dissemination of updates through a network of cooperating repositories can substantially lower the cost of coherence maintenance, 2) unless designed carefully, even push-based systems experience considerable loss in fidelity due to message delays and processing costs, 3) the computational and communication cost of achieving resiliency can be made to be low, and 4) surprisingly, adding resiliency can actually improve fidelity even in the absence of failures.',117393,'IEEE Transactions on Knowledge and Data Engineering',3),(764826,NULL,'2004',NULL,'Microarchitecture Optimizations for Exploiting Memory-Level Parallelism','The performance of memory-bound commercial applicationssuch as databases is limited by increasing memory latencies. Inthis paper, we show that exploiting memory-level parallelism(MLP) is an effective approach for improving the performance ofthese applications and that microarchitecture has a profound impacton achievable MLP. Using the epoch model of MLP, we reasonhow traditional microarchitecture features such as out-of-orderissue and state-of-the-art microarchitecture techniques suchas runahead execution affect MLP. Simulation results show that amoderately aggressive out-of-order issue processor improvesMLP over an in-order issue processor by 12-30%, and that aggressivehandling of loads, branches and serializing instructionsis needed to attain the full benefits of large out-of-order instructionwindows. The results also show that a processor\'s issue windowand reorder buffer should be decoupled to exploit MLP more efficiently.In addition, we demonstrate that runahead execution ishighly effective in enhancing MLP, potentially improving the MLPof the database workload by 82% and its overall performance by60%. Finally, our limit study shows that there is considerableheadroom in improving MLP and overall performance by implementingeffective instruction prefetching, more accurate branchprediction and better value prediction in addition to runahead execution.',192250,'Proceedings of the 31st annual international symposium on Computer architecture',4),(764883,NULL,'2004',NULL,'Algorithms for Taylor Expansion Diagrams','This paper presents a multiple-valued multiple-rail encodingscheme for low-power asynchronous data transferbetween modules inside a VLSI chip. The use of multiplerailencoding makes it possible to reduce the dynamic rangein a single wire. If signal levels ...',132234,'ISMVL \'04 Proceedings of the 34th International Symposium on Multiple-Valued Logic',10),(766645,NULL,'2004',NULL,'Finding r-Dominating Sets and p-Centers of Trees in Parallel','Let T=(V, E) be an edge-weighted tree with |V|=n vertices embedded in the Euclidean plane. Let {\\hbox{\\rlap{I}\\kern 2.0pt{\\hbox{E}}}} denote the set of all points on the edges of T. Let X and Y be two subsets of {\\hbox{\\rlap{I}\\kern 2.0pt{\\hbox{E}}}} and let r be a positive real number. A subset D\\subseteq X is an X/Y/r{\\hbox{-}}dominating setif every point in Y is within distance r of a point in D. The X/Y/r{\\hbox{-}}dominating setproblem is to find an X/Y/r{\\hbox{-}}{\\rm{dominating}} set D^* with minimum cardinality. Let p\\ge 1 be an integer. The X/Y/p{\\hbox{-}}centerproblem is to find a subset C^*\\subseteq X of p points such that the maximum distance of any point in Y from C^* is minimized. Let X and Y be either V or {\\hbox{\\rlap{I}\\kern 2.0pt{\\hbox{E}}}}. In this paper, efficient parallel algorithms on the EREW PRAM are first presented for the X/Y/r{\\hbox{-}}{\\rm{dominating}} set problem. The presented algorithms require O(\\log^2n) time for all cases of X and Y. Parallel algorithms on the EREW PRAM are then developed for the X/Y/p{\\hbox{-}}{\\rm{center}} problem. The presented algorithms require O(\\log^3n) time for all cases of X and Y. Previously, sequential algorithms for these two problems had been extensively studied in the literature. However, parallel solutions with polylogarithmic time existed only for their special cases. The algorithms presented in this paper are obtained by using an interesting approach which we call the dependency-tree approach. Our results are examples of parallelizing sequential dynamic-programming algorithms by using the approach.',117407,'IEEE Transactions on Parallel and Distributed Systems',9),(768823,NULL,'1988',NULL,'Parallel encrypted array multipliers','An algorithm for direct two\'s-complement and sign-magnitude parallel multiplication is described. The partial product matrix representing the multiplication is converted to an equivalent matrix by encryption. Its reduction, producing the final result, needs no specialized adders and can be added with any parallel array addition technique. It contains no negative terms and no extra \"correction\" rows; in addition, it produces the multiplication with fewer than the minimal number of rows required for a direct multiplication process.',114539,'IBM Journal of Research and Development - Nanostructure technology',8),(770701,NULL,'2004',NULL,'Dynamic voltage scaling of periodic and aperiodic tasks in priority-driven systems','We describe dynamic voltage scaling (DVS) algorithms for real-time systems with both periodic and aperiodic tasks. Although many DVS algorithms have been developed for real-time systems with periodic tasks, none of them can be used for the system with both periodic and aperiodic tasks because of arbitrary temporal behaviors of aperiodic tasks. We propose an off-line DVS algorithm and on-line DVS algorithms that are based on existing DVS algorithms. The proposed algorithms utilize the execution behaviors of scheduling server for aperiodic tasks. Experimental results show that the proposed algorithms reduce the energy consumption by 12% and 32% under the RM scheduling policy and the EDF scheduling policy, respectively.',190960,'Proceedings of the 2004 Asia and South Pacific Design Automation Conference',0),(777175,NULL,'2003',NULL,'Architecture generation of customized reconfigurable hardware','Reconfigurable hardware is ideal for use in systems-on-a-chip (SoCs), achieving hardware speeds but also flexibility not available with more traditional custom circuitry. Traditional FPGA structures can be used in an SoC, but they suffer from significant overhead due to their generic nature. Alternatively, for cases when the application domain of the SoC is known, the reconfigurable hardware can be optimized for that domain. The Totem Project focuses on the automatic creation of customized reconfigurable architectures, including high-level design, VLSI layout, and associated custom place and route tools. This thesis focuses on the high-level design phase, or “Architecture Generation”. Two distinct categories of reconfigurable architectures can be created: highly optimized near-ASIC designs with a very low degree of reconfigurability, and flexible architectures with a one-dimensional segmented routing structure. Each of these design methods shows significant improvements through tailoring the architectures to the given application area. The cASIC designs are on average up to 12.3x smaller than an FPGA solution with embedded multipliers and 2.2x smaller than a standard cell implementation. The more flexible architectures, able to support a wider variety of circuits, are on average up to 5.5x smaller than the FPGA solution, and close in area to standard cells.',33359,'Architecture generation of customized reconfigurable hardware',10),(784470,NULL,'2005',NULL,'A Hardware Algorithm for Modular Multiplication/Division','A mixed radix-4/2 algorithm for modular multiplication/division suitable for VLSI implementation is proposed. The algorithm is based on Montgomery method for modular multiplication and on the extended Binary GCD algorithm for modular division. Both algorithms are modified and combined into the proposed algorithm so that almost all the hardware components are shared. The new algorithm carries out both calculations using simple operations such as shifts, additions, and subtractions. The radix-2 signed-digit representation is used to avoid carry propagation in all additions and subtractions. A modular multiplier/divider based on the algorithm performs an n{\\hbox{-}}{\\rm bit} modular multiplication/division in O(n) clock cycles where the length of the clock cycle is constant and independent of n. The modular multiplier/divider has a linear array structure with a bit-slice feature and can be implemented with much smaller hardware than that necessary to implement both multiplier and divider separately.',117351,'IEEE Transactions on Computers',8),(784471,NULL,'2005',NULL,'Software Trace Cache','This paper explores the use of compiler optimizations which optimize the layout of instructions in memory. The target is to enable the code to make better use of the underlying hardware resources regardless of the specific details of the processor/architecture in order to increase fetch performance. The Software Trace Cache (STC) is a code layout algorithm with a broader target than previous layout optimizations. We target not only an improvement in the instruction cache hit rate, but also an increase in the effective fetch width of the fetch engine. The STC algorithm organizes basic blocks into chains trying to make sequentially executed basic blocks reside in consecutive memory positions, then maps the basic block chains in memory to minimize conflict misses in the important sections of the program. We evaluate and analyze in detail the impact of the STC, and code layout optimizations in general, on the three main aspects of fetch performance: the instruction cache hit rate, the effective fetch width, and the branch prediction accuracy. Our results show that layout optimized codes have some special characteristics that make them more amenable for high-performance instruction fetch: They have a very high rate of not-taken branches and execute long chains of sequential instructions; also, they make very effective use of instruction cache lines, mapping only useful instructions which will execute close in time, increasing both spatial and temporal locality.',117351,'IEEE Transactions on Computers',4),(784478,NULL,'2004',NULL,'A Combined Approach to High-Level Synthesis for Dynamically Reconfigurable Systems','The increase in complexity of programmable hardware platforms results in the need to develop efficient high-level synthesis tools since that allows more efficient exploration of the design space while predicting the effects of technology specific tools on the design space. Much of the previous work, however, neglects the delay of interconnects (e.g. multiplexers) which can heavily influence the overall performance of the design. In addition, in the case of dynamic reconfigurable logic circuits, unless an appropriate design methodology is followed, an unnecessarily large number of configurable logic blocks may end up being used for communication between contexts, rather than for implementing function units. The aim of this paper is to present a new technique to perform interconnect-sensitive synthesis, targeting dynamic reconfigurable circuits. Further, the proposed technique exploits multiple hardware contexts to achieve efficient designs. Experimental results on several benchmarks, which have been done on our DRL LSI circuit [10], [12], demonstrate that, by jointly optimizing the interconnect, communication, and function unit cost, we can achieve higher quality designs than is possible with such previous techniques as Force-Directed Scheduling.',117351,'IEEE Transactions on Computers',10),(784483,NULL,'2004',NULL,'Diagnosability of t-Connected Networks and Product Networks under the Comparison Diagnosis Model','Diagnosability is an important factor in measuring the reliability of an interconnection network, while the (node) connectivity is used to measure the fault tolerance of an interconnection network. We observe that there is a close relationship between the connectivity and the diagnosability. According to our results, a t-regular and t-connected network with at least 2t + 3 nodes is t--diagnosable. Furthermore, the diagnosability of the product networks is also investigated in this work. The product networks, including hypercube, mesh, and tori, comprise very important classes of interconnection networks. Herein, different combinations of t--diagnosable and t-connected are employed to study the diagnosability of the product networks.',117351,'IEEE Transactions on Computers',7),(787176,NULL,'2004',NULL,'Spatial queries in wireless broadcast systems','Owing to the advent of wireless networking and personal digital devices, information systems in the era of mobile computing are expected to be able to handle a tremendous amount of traffic and service requests from the users. Wireless data broadcast, thanks to its high scalability, is particularly suitable for meeting such a challenge. Indexing techniques have been developed for wireless data broadcast systems in order to conserve the scarce power resources in mobile clients. However, most of the previous studies do not take into account the impact of location information of users. In this paper, we address the issues of supporting spatial queries (including window queries and kNN queries) of location-dependent information via wireless data broadcast. A linear index structure based on the Hilbert curve and corresponding search algorithms are proposed to answer spatial queries on air. Experiments are conducted to evaluate the performance of the proposed indexing technique. Results show that the proposed index and its enhancement outperform existing algorithms significantly.',266383,'Wireless Networks - Special issue: Pervasive computing and communications',3),(788989,NULL,'2005',NULL,'Preventing Session Table Explosion in Packet Inspection Computers','In this paper, we first show that various network attacks can cause fatal inflation of dynamic memory usage on packet processing computers. Considering Transmission Control Protocol (TCP) is utilized by most of these attacks as well as legitimate traffic, we propose a parsimonious memory management guideline based on the design of the TCP and the analysis of real-life Internet traces. In particular, we demonstrate that, for all practical purposes, one should not allocate memory for an embryonic TCP connection with roughly more than 10 seconds of inactivity.',117351,'IEEE Transactions on Computers',3),(791376,NULL,'2005',NULL,'Searching Worst Cases of a One-Variable Function Using Lattice Reduction','We propose a new algorithm to find worst cases for the correct rounding of a mathematical function of one variable. We first reduce this problem to the real small value problem驴i.e., for polynomials with real coefficients. Then, we show that this second problem can be solved efficiently by extending Coppersmith\'s work on the integer small value problem驴for polynomials with integer coefficients驴using lattice reduction. For floating-point numbers with a mantissa less than N and a polynomial approximation of degree d, our algorithm finds all worst cases at distance less than N^{\\frac{-d^2}{2d+1}} from a machine number in time O(N^{{\\frac{d+1}{2d+1}}+\\varepsilon}). For d=2, a detailed study improves on the O(N^{2/3+\\varepsilon}) complexity from Lefèvre\'s algorithm to O(N^{4/7+\\varepsilon}). For larger d, our algorithm can be used to check that there exist no worst cases at distance less than N^{-k} in time O(N^{1/2+\\varepsilon}).',117351,'IEEE Transactions on Computers',9),(791379,NULL,'2005',NULL,'A Class of Unidirectional Bit Serial Systolic Architectures for Multiplicative Inversion and Division over {\\rm GF}(2^m)','A class of universal unidirectional bit serial systolic architectures for multiplicative inversion and division over Galois field {\\rm GF}(2^m) is presented. The field elements are represented with polynomial (standard) basis. These systolic architectures have no carry propagation structures and are suitable for hardware implementations where the dimension of the field is large and may vary. This is the typical case for cryptographic applications. These architectures are independent of any defining irreducible polynomial of a given degree as well. The time complexity is constant and area complexity is linear (w.r.t. field dimension) and these measures are equivalent to or exceed similar proposed designs.',117351,'IEEE Transactions on Computers',8),(796464,NULL,'2005',NULL,'Energy-Efficient, Utility Accrual Real-Time Scheduling Under the Unimodal Arbitrary Arrival Model','We present an energy-efficient real-time scheduling algorithm called EUA*, for the unimodal arbitrary arrival model (or UAM). UAM embodies a \"stronger\" adversary than most arrival models. The algorithm considers application activities that are subject to time/utility function time constraints, UAM, and the multi-criteria scheduling objective of probabilistically satisfying utility lower bounds, and maximizing system-level energy efficiency. Since the scheduling problem is intractable, EUA* allocates CPU cycles, scales clock frequency, and heuristically computes schedules using statistical estimates of cycle demands, in polynomial-time. We establish that EUA* achieves optimal timeliness during under-loads, and identify the conditions under which timeliness assurances hold. Our simulation experiments illustrate EUA*\'s superiority.',NULL,'Proceedings of the conference on Design, Automation and Test in Europe - Volume 1',0),(799549,NULL,'2005',NULL,'An analysis of TCP reset behaviour on the internet','This paper presents a one-year study of Internet packet traffic from a large campus network, showing that 15-25% of TCP connections have at least one TCP RST (reset). Similar results have also been observed from measurements of other Internet links. The results in this paper show that reset connections arise from local events such as network outages, attacks, or reconfigurations, as well as from global trends in TCP usage. In particular, we identify application-level Web behaviour as the primary contributor to the global trend in reset TCP connections. The most prevalent anomaly is the absence of the normal connection termination handshake. Instead, connections are often reset by the client. We believe that particular implementations of HTTP/TCP connection management cause this global trend.',15853,'ACM SIGCOMM Computer Communication Review',3),(801319,NULL,'2004',NULL,'The Liberty Simulation Environment, version 1.0','High-level hardware modeling via simulation is an essential step in hardware systems design and research. Despite the importance of simulation, current model creation methods are error prone and are unnecessarily time consuming. To address these problems, we have publicly released the Liberty Simulation Environment (LSE), Version 1.0, consisting of a simulator builder and automatic visualizer based on a shared hardware description language. LSE\'s design was motivated by a careful analysis of the strengths and weaknesses of existing systems. This has resulted in a system in which models are easier to understand, faster to develop, and have performance on par with other systems. LSE is capable of modeling any synchronous hardware system. To date, LSE has been used to simulate and convey ideas about a diverse set of complex systems including a chip multiprocessor out-of-order IA-64 machine and a multiprocessor system with detailed device models.',16066,'ACM SIGMETRICS Performance Evaluation Review - Special issue on tools for computer architecture research',5),(801349,NULL,'2004',NULL,'Understanding the effects of wrong-path memory references on processor performance','High-performance out-of-order processors spend a significant portion of their execution time on the incorrect program path even though they employ aggressive branch prediction algorithms. Although memory references generated on the wrong path do not change the architectural state of the processor, they can affect the arrangement of data in the memory hierarchy. This paper examines the effects of wrong-path memory references on processor performance. It is shown that these references significantly affect the IPC (Instructions Per Cycle) performance of a processor. Not modeling them can lead to errors of up to 10% in IPC estimates for the SPEC2000 integer benchmarks; 7 out of 12 benchmarks experience an error of greater than 2% in IPC estimates. In general, the error in the IPC increases with increasing memory latency and instruction window size.We find that wrong-path references are usually beneficial for performance, because they prefetch data that will be used by later correct-path references. L2 cache pollution is found to be the most significant negative effect of wrong-path references. Code examples are shown to provide insights into how wrong-path references affect performance.',266668,'WMPI \'04 Proceedings of the 3rd workshop on Memory performance issues: in conjunction with the 31st international symposium on computer architecture',4),(802330,NULL,'2005',NULL,'Defending against distributed denial-of-service attacks with max-min fair server-centric router throttles','Our work targets a network architecture and accompanying algorithms for countering distributed denial-of-service (DDoS) attacks directed at an Internet server. The basic mechanism is for a server under stress to install a router throttle at selected upstream routers. The throttle can be the leaky-bucket rate at which a router can forward packets destined for the server. Hence, before aggressive packets can converge to overwhelm the server, participating routers proactively regulate the contributing packet rates to more moderate levels, thus forestalling an impending attack. In allocating the server capacity among the routers, we propose a notion of level-k max-min fairness. We first present a control-theoretic model to evaluate algorithm convergence under a varitey of system parameters. In addition, we present packet network simulation results using a realistic global network topology, and various models of good user and attacker distributions and behavior. Using a generator model of web requests parameterized by empirical data, we also evaluate the impact of throttling in protecting user access to a web server. First, for aggressive attackers, the throttle mechanism is highly effective in preferentially dropping attacker traffic over good user traffic. In particular, level-k max-min fairness gives better good-user protection than recursive pushback of max-min fair rate limits proposed in the literature. Second, throttling can regulate the experienced server load to below its design limit - in the presence of user dynamics - so that the server can remain operational during a DDoS attack. Lastly, we present implementation results of our prototype on a Pentium III/866 MHz machine. The results show that router throttling has low deployment overhead in time and memory.',117498,'IEEE/ACM Transactions on Networking (TON)',3),(803751,NULL,'2005',NULL,'A Scalable Peer-to-Peer Architecture for Distributed Information Monitoring Applications','We present PeerCQ, a decentralized architecture for Internet scale information monitoring using a network of heterogeneous peer nodes. PeerCQ uses Continual Queries (CQs) as its primitives to express information-monitoring requests. The PeerCQ development has three unique characteristics. First, we develop a systematic and serverless approach to large scale information monitoring, aiming at providing a fully distributed, highly scalable, and self-configurable architecture for scalable and reliable processing of a large number of CQs over a network of loosely coupled, heterogeneous, and possibly unreliable nodes (peers). Second, we introduce an effective service partitioning scheme at the P2P protocol layer to distribute the processing of CQs over a peer-to-peer information monitoring overlay network while maintaining a good balance between system utilization and load balance in the presence of peer joins, departures, and failures. A unique feature of our service partitioning scheme is its ability to incorporate strategies for handling hot spot monitoring requests and peer heterogeneity into the load balancing scheme in PeerCQ. Third, but not least, we develop a dynamic passive replication scheme to enable reliable processing of long-running information monitoring requests in an environment of inherently unreliable peers, including an analytical model to discuss its fault tolerance properties. We report a set of experiments demonstrating the feasibility and the effectiveness of the PeerCQ approach to large-scale peer-to-peer information monitoring.',117351,'IEEE Transactions on Computers',3),(813914,NULL,'2005',NULL,'Enhancing Memory-Level Parallelism via Recovery-Free Value Prediction','The ever-increasing computational power of contemporary microprocessors reduces the execution time spent on arithmetic computations (i.e., the computations not involving slow memory operations such as cache misses) significantly. Therefore, for memory-intensive workloads, it becomes more important to overlap multiple cache misses than to overlap slow memory operations with other computations. In this paper, we propose a novel technique to parallelize sequential cache misses, thereby increasing memory-level parallelism (MLP). Our idea is based on value prediction, which was proposed originally as an instruction-level parallelism (ILP) optimization to break true data dependencies. In this paper, we advocate value prediction in its capability to enhance MLP instead of ILP. We propose using value prediction and value-speculative execution only for prefetching so that not only the complex prediction validation and misprediction recovery mechanisms are avoided, but better performance can also be achieved for memory-intensive workloads. The minor hardware modifications that are required also enable aggressive memory disambiguation for prefetching. The experimental results show that our technique enhances MLP effectively and achieves significant speedups, even with a simple stride value predictor.',117351,'IEEE Transactions on Computers',4),(820001,NULL,'2005',NULL,'Decimal Multiplication with Efficient Partial Product Generation','Decimal multiplication is important in many commercial applications including financial analysis, banking, tax calculation, currency conversion, insurance, and accounting. This paper presents a novel design for fixed-point decimal multiplication that utilizes a simple recoding scheme to produce signed-magnitude representations of the operands thereby greatly simplifying the process of generating partial products for each multiplier digit. The partial products are generated using a digit-by-digit multiplier on aword-by-digit basis, first in a signed-digit form with two digits per position, and then combined via a combinational circuit. As the signed-digit partial products are developed one at a time while traversing the recoded multiplier operand from the least significant digit to the most significant digit, each partial product is added along with the accumulated sum of previous partial products via a signed-digit adder. This work is significantly different from other work employing digit-by-digit multipliers due to the efficiency gained by restricting the range of digits throughout the multiplication process.',33616,'ARITH \'05 Proceedings of the 17th IEEE Symposium on Computer Arithmetic',8),(824917,NULL,'2005',NULL,'Some Optimizations of Hardware Multiplication by Constant Matrices','This paper presents some improvements on the optimization of hardware multiplication by constant matrices. We focus on the automatic generation of circuits that involve constant matrix multiplication, i.e., multiplication of a vector by a constant matrix. The proposed method, based on number recoding and dedicated common subexpression factorization algorithms, was implemented in a VHDL generator. Our algorithms and generator have been extended to the case of some digital filters based on multiplication by a constant matrix and delay operations. The obtained results on several applications have been implemented on FPGAs and compared to previous solutions. Up to 40 percent area and speed savings are achieved.',117351,'IEEE Transactions on Computers',8),(824944,NULL,'2005',NULL,'Performance Study of Link Layer and MAC Layer Protocols to Support TCP in 3G CDMA Systems','Providing support for TCP with good quality link connection is a key issue for future wireless networks in which Internet access is going to be one of the most important data services. A number of schemes have been proposed in literature to improve the TCP performance over wireless links. In this paper, we study the performance of a particular combination of link layer protocol (e.g., Radio Link Protocol or RLP) and MAC retransmissions to support the TCP connections over third generation (3G) wireless CDMA networks. We specifically investigate two metrics驴the packet error rate and the delay provided by RLP and MAC retransmissions驴both of which are important for TCP performance. For independent and identically distributed (i.i.d) error channels, we propose an analytical model for RLP performance with MAC retransmission. The segmentation of TCP/IP packets into smaller RLP frames, as well as the RLP buffering process, are modeled using a Markov chain. For correlated fading channels, we introduce an analytical metric called RLP retransmission efficiency. We show that: 1) the RLP frame size has significant impact on the overall 3G system performance, 2) MAC layer retransmissions significantly improve the TCP performance, and 3) the RLP retransmission scheme (1,1,1,1,1,1) performs better in highly correlated channels, while scheme (1,2,3) performs better in low correlated channels. Simulation results also confirm these conclusions.',117395,'IEEE Transactions on Mobile Computing',3),(832532,NULL,'2005',NULL,'An Exact Stochastic Analysis of Priority-Driven Periodic Real-Time Systems and Its Approximations','This paper describes a stochastic analysis framework which computes the response time distribution and the deadline miss probability of individual tasks, even for systems with a maximum utilization greater than one. The framework is uniformly applied to fixed-priority and dynamic-priority systems and can handle tasks with arbitrary relative deadlines and execution time distributions.',117351,'IEEE Transactions on Computers',0),(835561,NULL,'2005',NULL,'A dynamic and reliability-driven scheduling algorithm for parallel real-time jobs executing on heterogeneous clusters','In this paper, a heuristic dynamic scheduling scheme for parallel real-time jobs executing on a heterogeneous cluster is presented. In our system model, parallel real-time jobs, which are modeled by directed acyclic graphs, arrive at a heterogeneous cluster following a Poisson process. A job is said to be feasible if all its tasks meet their respective deadlines. The scheduling algorithm proposed in this paper takes reliability measures into account, thereby enhancing the reliability of heterogeneous clusters without any additional hardware cost. To make scheduling results more realistic and precise, we incorporate scheduling and dispatching times into the proposed scheduling approach. An admission control mechanism is in place so that parallel real-time jobs whose deadlines cannot be guaranteed are rejected by the system. For experimental performance study, we have considered a real world application as well as synthetic workloads. Simulation results show that compared with existing scheduling algorithms in the literature, our scheduling algorithm reduces reliability cost by up to 71.4% (with an average of 63.7%) while improving schedulability over a spectrum of workload and system parameters. Furthermore, results suggest that shortening scheduling times leads to a higher guarantee ratio. Hence, if parallel scheduling algorithms are applied to shorten scheduling times, the performance of heterogeneous clusters will be further enhanced.',135651,'Journal of Parallel and Distributed Computing',0),(835765,NULL,'1978',NULL,'Microprogrammable microprocessor survey','The Motorola M10800 LSI processor family consists of a sequencer, referred to as a Microprogram Control Function (MCF) - MC10801, and a processing element, referred to as a 4-bit ALU Slice - MC10800 (Not to be confused with the processor family number M10800). Undoubtedly, the most interesting feature of the M10800 processor family is the ECL technology used to produce it. The M10800 processor family is completely MECL 10,000 compatible and exhibits the ultra-high speed performance of ECL logic.',16069,'ACM SIGMICRO Newsletter',10),(837581,NULL,'2005',NULL,'The Granularity Metric for Fine-Grain Real-Time Scheduling','This paper investigates the task scheduling problem for real-time systems that provide rate of progress guarantees on task execution. A parameterized task system model, called the (r,g) task system, is introduced that allows rate of progress requirements to be specified in terms of two simple parameters: an execution rate r and a granularity g. The granularity parameter is a new metric that allows the specification of \"fine-grain驴 timing constraints on the task\'s execution and is a generalization of the stretch metric used in recent research on task scheduling. It is shown that the product r \\lg(1/g) is an important determiner of the existence of good online scheduling algorithms. Specifically, there is an upper bound on this product above which there are no good online algorithms, but below which an online algorithm with logarithmic competitive ratio exists. This paper also demonstrates a fundamental difference between two contrasting strategies for admission control: greedy versus nongreedy. It is shown that \"greed does not pay驴: there is a scheduling algorithm with a nongreedy admission policy that provably outperforms the well-known Greedy EDF scheduling algorithm.',117351,'IEEE Transactions on Computers',0),(837592,NULL,'2005',NULL,'Low Complexity Bit-Parallel Multiplier for GF(2^m) Defined by All-One Polynomials Using Redundant Representation','This paper presents a new bit-parallel multiplier for the finite field GF(2^m) defined by an irreducible all-one polynomial. In order to reduce the complexity of the multiplier, we introduce a redundant representation and use the well-known multiplication method proposed by Karatsuba. The main idea is to combine the redundant representation and the Karatsuba method to design an efficient bit-parallel multiplier. As a result, the proposed multiplier requires about 25 percent fewer AND/XOR gates than the previously proposed multipliers using an all-one polynomial, while it has almost the same time delay as the previously proposed ones.',117351,'IEEE Transactions on Computers',8),(838884,NULL,'2005',NULL,'Incorporating Error Detection and Online Reconfiguration into a Regular Architecture for the Advanced Encryption Standard','Fault injection based attacks on cryptographic devices aim at recovering the secret keys by inducing an error in the computation process. They are now considered a real threat and countermeasures against them must be taken. In this paper, we describe an extension to an existing AES architecture proposed by Mangard et al. [13], which provides error detection and fault tolerance by exploiting the high regularity of the architecture. The proposed design is capable of performing online error detection and reconfiguring internal data paths to protect against faults occurring in the computation process. We also describe how different redundancy levels provide protection against different numbers of errors. The presented design incorporating fault detection and tolerance has the same throughput as the base architecture but incurs a non-negligible area overhead. This overhead is about 40% for the fault detection circuitry and 134% for the entire fault detection and tolerance (through reconfiguration). Although quite high, this overhead is still lower than for reference solutions such as duplication (providing detection) and triple modular redundancy (providing fault masking).',75851,'DFT \'05 Proceedings of the 20th IEEE International Symposium on Defect and Fault Tolerance in VLSI Systems',6),(839314,NULL,'2005',NULL,'Address-Value Delta (AVD) Prediction: Increasing the Effectiveness of Runahead Execution by Exploiting Regular Memory Allocation Patterns','While runahead execution is effective at parallelizing independent long-latency cache misses, it is unable to parallelize dependent long-latency cache misses. To overcome this limitation, this paper proposes a novel technique, address-value delta (AVD) prediction. An AVD predictor keeps track of the address (pointer) load instructions for which the arithmetic difference (i.e., delta) between the effective address and the data value is stable. If such a load instruction incurs a long-latency cache miss during runahead execution, its data value is predicted by subtracting the stable delta from its effective address. This prediction enables the pre-execution of dependent instructions, including load instructions that incur long-latency cache misses. We describe how, why, and for what kind of loads AVD prediction works and evaluate the design tradeoffs in an implementable AVD predictor. Our analysis shows that stable AVDs exist because of patterns in the way data structures are allocated in memory. Our results show that augmenting a runahead processor with a simple, 16-entry AVD predictor improves the average execution time of a set of pointer-intensive applications by 12.1%.',192314,'Proceedings of the 38th annual IEEE/ACM International Symposium on Microarchitecture',4),(843866,NULL,'2006',NULL,'Efficient Algorithms and Architectures for Field Multiplication Using Gaussian Normal Bases','Recently, implementations of normal basis multiplication over the extended binary field GF(2^{m}) have received considerable attention. A class of low complexity normal bases called Gaussian normal bases has been included in a number of standards, such as IEEE [1] and NIST [2] for an elliptic curve digital signature algorithm. The multiplication algorithms presented there are slow in software since they rely on bit-wise inner product operations. In this paper, we present two vector-level software algorithms which essentially eliminate such bit-wise operations for Gaussian normal bases. Our analysis and timing results show that the software implementation of the proposed algorithm is faster than previously reported normal basis multiplication algorithms. The proposed algorithm is also more memory efficient compared with its look-up table-based counterpart. Moreover, two new digit-level multiplier architectures are proposed and it is shown that they outperform the existing normal basis multiplier structures. As compared with similar digit-level normal basis multipliers, the proposed multiplier with serial output requires the fewest number of XOR gates and the one with parallel output is the fastest multiplier.',117351,'IEEE Transactions on Computers',8),(844602,NULL,'2005',NULL,'Energy-Aware Modeling and Scheduling of Real-Time Tasks for Dynamic Voltage Scaling','Dynamic voltage scaling (DVS) is a promising technique for battery-powered systems to conserve energy consumption. Most existing DVS algorithms assume information about task periodicity or a priori knowledge about the task set to be scheduled. This paper presents an analytical model of general tasks for DVS assuming job timing information is known only after a task release. It models the voltage scaling process as a transfer function-based filter system, which facilitates the design of two efficient scaling algorithms. The first is a time-invariant scaling policy based on a voltage scaling function independent of input jobs over time. It is proved to be a generalization of several existing DVS algorithms. A more energy efficient policy is a time-variant scaling algorithm. The algorithm turns out to be a water-filling process of information theory with a low time complexity. It can not only be applied to scheduling based on worst case execution times, but also to online slack distribution when jobs complete earlier. We further establish two relationships between computation capacity and deadline misses. The relationships make it possible to the provisioning of statistical real-time guarantees.',208414,'RTSS \'05 Proceedings of the 26th IEEE International Real-Time Systems Symposium',0),(849800,NULL,'2006',NULL,'Diversity Analysis in the Presence of Delay Faults Affecting Duplex Systems','This paper analyzes the problem of timing related common mode failures in redundant systems. The specific case of duplex systems in the presence of delay faults is analyzed by providing a probabilistic characterization of undetectable errors. PDF simulation was used to evaluate the probability of undetectable errors in conventional duplex systems and in duplex systems making use of a simple kind of data diversity.',117351,'IEEE Transactions on Computers',6),(852646,NULL,'2004',NULL,'Logic BISTWith Scan Chain Segmentation','This paper presents a novel BIST (Built-In Self Test) scheme with scan chain segmentation. In the scheme, a combination of pseudo random patterns and single-weight patterns have been applied to CUT (Circuit Under Test). Scan chain is partitioned into multiple segments delimited by inverters. When a single weighted pattern is applied to a segmented scan chain, successive segments receive bit patterns with complementary weights. Several segment configurations may be required to achieve full fault coverage. In this scheme the control logic is inside the scan path and built-in self test can be implemented without compromising timing performance of CUT. Experiments show that our scheme can obtain very good fault coverage. Hardware implementation is simple and straightforward.',132985,'ITC \'04 Proceedings of the International Test Conference on International Test Conference',11),(863356,NULL,'2006',NULL,'A Utility Accrual Scheduling Algorithm for Real-Time Activities with Mutual Exclusion Resource Constraints','This paper presents a uni-processor real-time scheduling algorithm called the Generic Utility Scheduling algorithm (which we will refer to simply as GUS). GUS solves a previously open real-time scheduling problem—scheduling application activities that have time constraints specified using arbitrarily shaped time/utility functions and have mutual exclusion resource constraints. A time/utility function is a time constraint specification that describes an activity\'s utility to the system as a function of that activity\'s completion time. Given such time and resource constraints, we consider the scheduling objective of maximizing the total utility that is accrued by the completion of all activities. Since this problem is {\\cal N}{\\cal P}{\\hbox{-}}{\\rm hard}, GUS heuristically computes schedules with a polynomial-time cost of O(n^3) at each scheduling event, where n is the number of activities in the ready queue. We evaluate the performance of GUS through simulation and by an actual implementation on a real-time POSIX operating system. Our simulation studies and implementation measurements reveal that GUS performs close to, if not better than, the existing algorithms for the cases that they apply. Furthermore, we analytically establish several properties of GUS.',117351,'IEEE Transactions on Computers',0),(884341,NULL,'2006',NULL,'Optimal deadline assignment for periodic real-time tasks in dynamic priority systems','Real-time systems are often designed using a set of periodic task. Task periods are usually set by the system requirements, but deadlines and computation times can be modified in order to improve system performance. Sensitivity analysis in real-time systems has focused on changes in task computation times, using fixed priority analysis. Only a few studies deal with the modification of deadlines in dynamic-priority scheduling. The aim of this work is to provide a sensitivity analysis for task deadlines in the context of dynamic-priority, pre-emptive, uniprocessor scheduling. In this paper, we present a deadline minimisation method that achieves the maximum reduction. As undertaken in other studies concerning computation times, we also define and calculate the critical scaling factor for task deadlines. Our proposal is evaluated and compared with other works in terms of jitter: The deadline minimisation can be used to strongly reduce jitter of control tasks, in a real-time control application.',83391,'ECRTS \'06 Proceedings of the 18th Euromicro Conference on Real-Time Systems',0),(884455,NULL,'2006',NULL,'A Locality-Aware Cooperative Cache Management Protocol to Improve Network File System Performance','The Java messaging service (JMS) is a means to organize communication among distributed applications according to the publish/subscribe principle. If the subscribers install filter rules on the JMS server, JMS can be used as a message routing platform, ...',115561,'ICDCS \'06 Proceedings of the 26th IEEE International Conference on Distributed Computing Systems',3),(884952,NULL,'2003',NULL,'Memory reference reuse latency: Accelerated warmup for sampled microarchitecture simulation','This paper proposes to speedup sampled microprocessor simulations by reducing warmup times without sacrificing simulation accuracy. It exploiting the observation that of the memory references that precede a sample cluster, references that occur nearest to the cluster are more likely to be germane to the execution of the cluster itself. Hence, while modeling all cache and branch predictor interactions that precede a sample cluster would reliably establish their state, this is overkill and leads to long-running simulations. Instead, accurately establishing simulated cache and branch predictor state can be accomplished quickly by only modeling a subset of the memory references and control-flow instructions immediately preceding a sample cluster. Our technique measures memory reference reuse latencies (MRRLs) - the number of completed instructions between consecutive references to each unique memory location - and uses these data to choose a point prior to each cluster to engage cache hierarchy and branch predictor modeling. By starting cache and branch predictor modeling late in the pre-cluster instruction stream, we were able to reduce overall simulation running times by an average of 90.62% of the maximum potential speedup (accomplished by performing no pre-cluster warmup at all), while generating an average error in IPC of less than 1%, both relative to the IPC generated by warming up all pre-cluster cache and branch predictor interactions.',132415,'ISPASS \'03 Proceedings of the 2003 IEEE International Symposium on Performance Analysis of Systems and Software',4),(889025,NULL,'2006',NULL,'Efficient Bit-Parallel Multiplier for Irreducible Pentanomials Using a Shifted Polynomial Basis','In this paper, we present a bit-parallel multiplier for GF(2^m) defined by an irreducible pentanomial x^m+x^{k_3}+x^{k_2}+x^{k_1}+1, where 1\\leq k_1',117351,'IEEE Transactions on Computers',8),(889035,NULL,'2006',NULL,'Optimum Digit Serial GF(2^m) Multipliers for Curve-Based Cryptography','Digit Serial Multipliers are used extensively in hardware implementations of elliptic and hyperelliptic curve cryptography. This contribution shows different architectural enhancements in Least Significant Digit (LSD) multiplier for binary fields GF(2^m). We propose two different architectures, the Double Accumulator Multiplier (DAM) and N-Accumulator Multiplier (NAM), which are both faster compared to traditional LSD multipliers. Our evaluation of the multipliers for different digit sizes gives optimum choices and shows that currently used digit sizes are the worst possible choices. Hence, one of the most important results of this contribution is that digit sizes of the form 2^l-1, where l is an integer, are preferable for the digit multipliers. Furthermore, one should always use the NAM architecture to get the best timings. Considering the time area product DAM or NAM gives the best performance depending on the digit size.',117351,'IEEE Transactions on Computers',8),(889047,NULL,'2006',NULL,'Location-Aware Combinatorial Key Management Scheme for Clustered Sensor Networks','Recent advances in wireless sensor networks (WSNs) are fueling the interest in their application in a wide variety of sensitive settings such as battlefield surveillance, border control, and infrastructure protection. Data confidentiality and authenticity are critical in these settings. However, the wireless connectivity, the absence of physical protection, the close interaction between WSNs and their physical environment, and the unattended deployment of WSNs make them highly vulnerable to node capture as well as a wide range of network-level attacks. Moreover, the constrained energy, memory, and computational capabilities of the employed sensor nodes limit the adoption of security solutions designed for wire-line and wireless networks. In this paper, we focus on the management of encryption keys in large-scale clustered WSNs. We propose a novel distributed key management scheme based on Exclusion Basis Systems (EBS); a combinatorial formulation of the group key management problem. Our scheme is termed SHELL because it is Scalable, Hierarchical, Efficient, Location-aware, and Light-weight. Unlike most existing key management schemes for WSNs, SHELL supports rekeying and, thus, enhances network security and survivability against node capture. SHELL distributes key management functionality among multiple nodes and minimizes the memory and energy consumption through trading off the number of keys and rekeying messages. In addition, SHELL employs a novel key assignment scheme that reduces the potential of collusion among compromised sensor nodes by factoring the geographic location of nodes in key assignment. Simulation results demonstrate that SHELL significantly boosts the network resilience to attacks while conservatively consuming nodes\' resources.',117407,'IEEE Transactions on Parallel and Distributed Systems',3),(910751,NULL,'2006',NULL,'Address-Value Delta (AVD) Prediction: A Hardware Technique for Efficiently Parallelizing Dependent Cache Misses','While runahead execution is effective at parallelizing independent long-latency cache misses, it is unable to parallelize dependent long-latency cache misses. To overcome this limitation, this paper proposes a novel hardware technique, address-value delta (AVD) prediction. An AVD predictor keeps track of the address (pointer) load instructions for which the arithmetic difference (i.e., delta) between the effective address and the data value is stable. If such a load instruction incurs a long-latency cache miss during runahead execution, its data value is predicted by subtracting the stable delta from its effective address. This prediction enables the preexecution of dependent instructions, including load instructions that incur long-latency cache misses. We analyze why and for what kind of loads AVD prediction works and describe the design of an implementable AVD predictor. We also describe simple hardware and software optimizations that can significantly improve the benefits of AVD prediction and analyze the interaction of AVD prediction with runahead efficiency techniques and stream-based data prefetching. Our analysis shows that AVD prediction is complementary to these techniques. Our results show that augmenting a runahead processor with a simple, 16-entry AVD predictor improves the average execution time of a set of pointer-intensive applications by 14.3 percent (7.5 percent excluding benchmark health).',117351,'IEEE Transactions on Computers',4),(937513,NULL,'1976',NULL,'Operating system enhancement through microprogramming','Microprogramming support for the enhancement of operating system design is described briefly, organization structure of real-time operating systems are examined, and criteria proposed for determining which functions are best candidates for implementation in firmware. A suitable microprogrammed computer is selected and avenues for additional research are recommended.',16069,'ACM SIGMICRO Newsletter',5),(949795,NULL,'2006',NULL,'System-wide energy minimization for real-time tasks: lower bound and approximation','We present a dynamic voltage scaling (DVS) technique that minimizes system-wide energy consumption for both periodic and sporadic tasks. It is known that a system consists of processors and a number of other components. Energy-aware processors can be run in different speed levels; components like memory and I/O subsystems and network interface cards can be in a standby state when they are active but idle. Processor energy optimization solutions are not necessarily efficient from the perspective of systems. Current system-wide energy optimization studies are often limited to periodic tasks with heuristics in getting approximated solutions. In this paper, we develop an exact dynamic programming algorithm for periodic tasks on processors with practical discrete speed levels. The algorithm determines the lower bound of energy expenditure in pseudo-polynomial time. An approximation algorithm is proposed to provide performance guarantee with a given bound in polynomial running time. Because of their time efficiency, both the optimization and approximation algorithms can be adapted for online scheduling of sporadic tasks with irregular task releases. We prove that system-wide energy optimization for sporadic tasks is NP-hard in the strong sense. We develop (pseudo-) polynomialtime solutions by exploiting its inherent properties.',191076,'Proceedings of the 2006 IEEE/ACM international conference on Computer-aided design',0),(963463,NULL,'2003',NULL,'TCP offload is a dumb idea whose time has come','Network interface implementors have repeatedly attempted to offload TCP processing from the host CPU. These efforts met with little success, because they were based on faulty premises. TCP offload per se is neither of much overall benefit nor free from significant costs and risks. But TCP offload in the service of very specific goals might actually be useful. In the context of the replacement of storage-specific interconnect via commoditized network hardware, TCP offload (and more generally, offloading the transport protocol) appropriately solves an important problem.',111750,'HOTOS\'03 Proceedings of the 9th conference on Hot Topics in Operating Systems - Volume 9',3),(963651,NULL,'2000',NULL,'Interposed request routing for scalable network storage','This paper explores interposed request routing in Slice, a new storage system architecture for high-speed networks incorporating network-attached block storage. Slice interposes a request switching filter - called a µproxy - along each client\'s network path to the storage service (e.g., in a network adapter or switch). The µproxy intercepts request traffic and distributes it across a server ensemble. We propose request routing schemes for I/O and file service traffic, and explore their effect on service structure. The Slice prototype uses a packet filter µproxy to virtualize the standard Network File System (NFS) protocol, presenting to NFS clients a unified shared file volume with scalable bandwidth and capacity. Experimental results from the industry-standard SPECsfs97 workload demonstrate that the architecture enables construction of powerful network-attached storage services by aggregating cost-effective components on a switched Gigabit Ethernet LAN.',176950,'OSDI\'00 Proceedings of the 4th conference on Symposium on Operating System Design & Implementation - Volume 4',3),(972698,NULL,'1993',NULL,'Fixed and Adaptive Sequential Prefetching in Shared Memory Multiprocessors','To offset the effect of read miss penalties on processor utilization in shared-memory multiprocessors, several software- and hardware-based data prefetching schemes have been proposed. A major advantage of hardware tech niques is that they need no support from the programmer or compiler. Sequential prefetching is a simple hardware-controlled prefetching technique which relies on the automatic prefetch of consecutive blocks following the block that misses in the cache. In its simplest form, the number of prefetched blocks on each miss is fixed throughout the exe cution. However, since the prefetching efficiency varies during the execution of a program, we propose to adapt the number of pref etched blocks according to a dynamic measure of prefetching effectiveness. Simulations of this adaptive scheme show significant reductions of the read penalty and of the overall execution time.',116416,'ICPP \'93 Proceedings of the 1993 International Conference on Parallel Processing - Volume 01',4),(974861,NULL,'2007',NULL,'Energy-Aware Modeling and Scheduling for Dynamic Voltage Scaling with Statistical Real-Time Guarantee','Dynamic voltage scaling (DVS) is a promising technique for battery-powered systems to conserve energy consumption. Most existing DVS algorithms assume information about task periodicity or a priori knowledge about the task set to be scheduled. This paper presents an analytical model of general tasks for DVS assuming job timing information is known only after a task release. It models the voltage scaling process as a transfer function-based filtering system, which facilitates the design of two efficient scaling algorithms. The first is a time-invariant scaling policy and it is proved to be a generalization of several popular DVS algorithms for periodic, sporadic, and aperiodic tasks. A more energy efficient policy is a time-variant scaling algorithm for aperiodic tasks. It is optimal in the sense that it is online without assumed information about future task releases. The algorithm turns out to be a water-filling process with a linear time complexity. It can be applied to scheduling based on worst-case execution times as well as online slack distribution when jobs complete earlier. We further establish two relationships between computation capacity and deadline misses to provide a statistical real-time guarantee with reduced capacity.',117351,'IEEE Transactions on Computers',0),(974952,NULL,'2007',NULL,'A Radix-10 Digit-Recurrence Division Unit: Algorithm and Architecture','In this work, we present a radix-10 division unit that is based on the digit-recurrence algorithm. The previous decimal division designs do not include recent developments in the theory and practice of this type of algorithm, which were developed for {\\rm radix}{\\hbox{-}}2^{k} dividers. In addition to the adaptation of these features, the radix-10 quotient digit is decomposed into a radix-2 digit and a radix-5 digit in such a way that only five and two times the divisor are required in the recurrence. Moreover, the most significant slice of the recurrence, which includes the selection function, is implemented in radix-2, avoiding the additional delay introduced by the radix--10 carry-save additions and allowing the balancing of the paths to reduce the cycle delay. The results of the implementation of the proposed radix-10 division unit show that its latency is close to that of radix-16 division units (comparable dynamic range of significands) and it has a shorter latency than a radix-10 unit based on the Newton-Raphson approximation.',117351,'IEEE Transactions on Computers',8),(982717,NULL,'2007',NULL,'Comb Architectures for Finite Field Multiplication in F(2^m)','Two high-speed bit-serial word-parallel or comb-style finite field multipliers are proposed in this paper. The first proposal utilizes a redundant representation for any binary field and the other uses a reordered normal basis for the binary field where a type-II optimal normal basis exists. The proposed redundant representation architecture has a smaller critical path delay compared to the previous methods while the complexities remain about the same. The proposed reordered normal basis multiplier has a significantly smaller critical path delay compared to the previous methods using the same basis or normal basis. Field-programmable gate array (FPGA) implementation results of the proposed multipliers are compared to those of the previous methods using the same basis, which confirms that the proposed multipliers allow a much higher clock rate.',117351,'IEEE Transactions on Computers',8),(1010502,NULL,'1969',NULL,'Binary Multiplication with Overlapped Addition Cycles','With a suitable adder organization it is possible to overlap the adder operation during a binary multiplication and significantly decrease the overall multiplication time. The method is explained and a prototype multiplier described. The new technique provides a very economical method of obtaining a reasonably fast multiplier.',117351,'IEEE Transactions on Computers',8),(1010515,NULL,'1980',NULL,'Tours of Graphs, Digraphs, and Sequential Machines','A tour of a graph (digraph or sequential machine) is a sequence of nodes from the graph such that each node appears at least once and two nodes are adjacent in the sequence only if they are adjacent in the graph. Finding the shortest tour of a graph is known to be an NP-complete problem. Several theorems are given that show that there are classes ofgraphs in which the shortest tour can be found easily. For more general graphs, we present approximating algorithms for finding short tours. For undirected graphs, the approximating algorithms give tours at worst a constant times the length of the shortest tour. For directed graphs, the size of the calculated tour is bounded by the size of the digraph times the shortest tour. Not only are the bounds worse for the directed case, but the running times of the approximating algorithms are also larger than those for the undirected case.',117351,'IEEE Transactions on Computers',9),(1010655,NULL,'1971',NULL,'Modular LSI Control Logic Design with Error Detection','This paper describes a modular approach of implementing the control circuitry. It is achieved by the use of multifunctional binary decoder circuits in conjunction with a transistor array high-speed READ-ONLY memory.',117351,'IEEE Transactions on Computers',10),(1010676,NULL,'1969',NULL,'Unateness Test of a Boolean Function and Two General Synthesis Methods Using Threshold Logic Elements','This paper presents an elaboration of the unateness test of a Boolean function based on the definition of the so-called simple zero or unit transition of arguments of the given Boolean function. It is proved that the unateness of the given Boolean function is subject to the necessary and sufficient condition that there must be no occurrence of both simple unit transition and zero transition at the same time. Whether this condition is satisfied or not can easily be seen from the truth table of the given function.',117351,'IEEE Transactions on Computers',1),(1010686,NULL,'1975',NULL,'Typicality, Diversity, and Feature Pattern of an Ensemble','In this paper, issues concerning feature patterns in terms of both feature composition and feature interdependence are discussed, and the concepts of typicality and diversity of an ensemble are formulated. The features of the specimens investigated are organized in a two-dimensional array, called an observation matrix, with each row vector representing the ordered set of features of a specimen. An algorithm (based upon the proposed measures and statistical screening) is implemented for extracting feature patterns. In the algorithm, schemes for feature patterns and specimen reweighting are proposed to optimize the utilization of available information in the array, and to minimize possible bias caused by the uneven sampling of the ensemble. Two sets of real world data in the environmental and molecular biology areas are used to exemplify the physical meaning of the proposed measures as well as to demonstrate the operational feasibility and significance of this methodology in analyzing homologous ensemble which is subject to variable degrees of diversity.',117351,'IEEE Transactions on Computers',2),(1010704,NULL,'1970',NULL,'Minimization of Exclusive or and Logical Equivalence Switching Circuits','This paper is an attempt to develop minimization algorithms for switching circuits based on Reed-Muller canonic forms. In particular, algorithms are presented for obtaining minimal modulo 2 or complement modulo 2 sum-of- products (or sums) expressions of any arbitrary single-output or multiple-output switching function with fixed polarities of the input variables.',117351,'IEEE Transactions on Computers',1),(1010819,NULL,'1969',NULL,'Loop-Free Threshold Element Structures','The paper deals with the problem of \"compound\" and \"cascade threshold\" element \"synthesis\" of an arbitrary Boolean function from the \"multithreshold weight threshold vector\" (MTWTV). The above synthesis procedure is presented to reveal a unique feature of the multithreshold weight threshold vector, from which several realizations of threshold element nets can be obtained from one of the multithreshold weight threshold vectors.',117351,'IEEE Transactions on Computers',1),(1010855,NULL,'1982',NULL,'Architecture for VLSI Design of Reed-Solomon Encoders','In this correspondence the logic structure of a universal VLSI chip called the symbol-slice Reed-Solomon (RS) encoder chip is presented. An RS encoder can be constructed by cascading and properly interconnecting a group of such VLSI chips. As a design example, it is shown that a (255, 223) RS encoder requiring around 40 discrete CMOS IC\'s may be replaced by an RS encoder consisting of four identical interconnected VLSI RS encoder chips. Besides the size advantage, the VLSI RS encoder also has the potential advantages of requiring less power and having a higher reliability.',117351,'IEEE Transactions on Computers',10),(1010903,NULL,'1982',NULL,'Counting and Reporting Intersections of d-Ranges','We present a solution to the problem of reporting and counting all pairs of intersecting or overlapping d-ranges in a given set of n d-ranges. A d-range is a rectilinearly oriented rectangular d-dimensional box or brick. The three-dimensional case is presented in some detail and an 0(n log22n + k) time and 0(n log22n) space algorithm is developed for the',117351,'IEEE Transactions on Computers',9),(1010917,NULL,'1975',NULL,'Two-Level Emitter-Function Logic Structures for Logic-in-Memory Computers','Logic-in-memory (LIM) organization allows central processor functions of computing systems to be combined with memory in regular arrays. The cells of these arrays can themselves be constructed regularly and economically using similar two-level emitter-function logic (EFL) structures. The structure is a development of current-mode logic and it permits the large-scale integration (LSI) realization of three levels of logic with similar silicon area and power dissipation to a single conventional emitter-coupled logic (ECL) gate. It also permits the realization of a D latch in a single structure. The capability of the structure is demonstrated in examples of LIM data transfer and sorting arrays.',117351,'IEEE Transactions on Computers',10),(1010953,NULL,'1977',NULL,'Cross-Section Reconstruction with a Fan-Beam Scanning Geometry','A simple approach for cross-section reconstruction with a fan-beam scanning geometry is discussed. The scanner and the radiation beams are set up in such a way that the projection data can be rearranged and considered as if they were generated from the conventional parallel-beam configuration. The convolution algorithm is used for the cross-section reconstruction of synthetic patterns. Combarisons are made between the fan-beam and parallel-beam geometries.',117351,'IEEE Transactions on Computers',2),(1010954,NULL,'1977',NULL,'Recognition of Waveforms Using Autoregressive Feature Extraction','It is proposed that the autoregressive coefficients (as computed from the estimated autocorrelation function) in an autoregressive spectral estimation scheme may be used for feature extraction purposes. An example is presented where such extractors are used to describe seismic wave traces originating from shallow earthquakes and underground nuclear explosions.',117351,'IEEE Transactions on Computers',2),(1010979,NULL,'1978',NULL,'A Theory of Galois Switching Functions','Galois switching functions (GSF\'s) are generalizations of binary functions in that the input and output variables can assume values over any finite field. The concepts of minterms, k-cubes and minimal complexity realizations as related to GSF are introduced.',117351,'IEEE Transactions on Computers',1),(1011000,NULL,'1981',NULL,'Synthesis of Finite State Algorithms in a Galois Field GF[pn]','This correspondence describes a method for achieving synthesis of finite state algorithms by the use of a set of logic elements that execute field operations from the Galois field GF[pn]. The method begins with a definition of the algorithm to be synthesized in a completely specified finite state flow table form. A polynomial expansion of this flow table function is derived. A canonical sequential circuit corresponding to this polynomial expansion is defined. Subsequently, the given algorithm is synthesized using the canonical circuit by specification of a number of arbitrary constants in the canonical circuit. A mechanical method for deriving constants used in the canonical circuits is given. Finally, some estimates on complexity of the given circuit structure are stated assuming the most fundamental logic element structures.',117351,'IEEE Transactions on Computers',1),(1011007,NULL,'1980',NULL,'An Experimental Delay Test Generator for LSI Logic','Delay testing is a test procedure to verify the timing performance of manufactured logic networks. When a level-sensitive scan design (LSSD) discipline is used, all networks are combinational. Appropriate test patterns are selected on the basis of certain theoretical criteria. These criteria are embodied in an experimental test generation program. The program has successfully produced sets of delay tests for large logic networks. The average coverage achieved by these tests faDs within 95.8 percent to 99.9 percent of optimal.',117351,'IEEE Transactions on Computers',11),(1011083,NULL,'1971',NULL,'On Detecting Total or Partial Symmetry of Switching Functions','This note presents a method for identifying total or partial symmetry of switching functions based on the application of the principle of residue test by numerical methods. The invariance of a switching function under a single interchange of two variables can be readily detected from the equality of some of the residues of expansion about these two variables. This procedure of detecting invariance is directly applied for the identification of total or partial symmetry of a switching function whose variables of symmetry may be either all unprimed (or all primed), mixed, or of multiform nature.',117351,'IEEE Transactions on Computers',1),(1011085,NULL,'1971',NULL,'BCD and Radi x-2 Conversions','An algorithm for the conversion of BCD numbers to numbers in the -2 radix system is presented. This algorithm consists of simple arithmetic operations such as division by two and addition of the constants one and five. These operations may easily be performed in a -2 radix computer by shifting a number right for division by two and a table look-up procedure for the additions. The reverse conversion is also briefly discussed',117351,'IEEE Transactions on Computers',8),(1011097,NULL,'1986',NULL,'Correction to \"Fault-Tolerant Multiprocessor Link and Bus Architectures\"','In the above paper1 the following typographical errors should be corrected.',117351,'IEEE Transactions on Computers',6),(1011174,NULL,'1969',NULL,'Experiments with \"Characteristic Loci\" for Recognition of Handprinted Characters','Recognition experiments have been performed on handprinted characters using a set of features which has not been applied previously to handprinting. Glucksman\'s \"characteristic loci\" were utilized in experiments with the well-known Highleyman data, as well as samples generated at Stanford Research Institute and Honeywell. Two recognition algorithms were tested. Results on numeric samples compare favorably with those of other investigators despite the small dimensionality of the feature vector. On the constrained Honeywell samples, recognition rates exceeding 98 percent were achieved using the simpler algorithm. With alphabetic samples, some problems remain in resolving persistent ambiguities, and methods for attacking these problems are considered.',117351,'IEEE Transactions on Computers',2),(1011223,NULL,'1971',NULL,'On the Design of Universal Boolean Functions','A Boolean function U( z1,...,zm) is universal for given n=1 and a set I of variables if it realizes all Boolean functions f(x1,..., xn) by substituting for each zj a variable of I. Designs of universal Boolean functions for various specifications of I are considered for the practical cases of n',117351,'IEEE Transactions on Computers',1),(1011232,NULL,'1971',NULL,'On the Delay Required to Realize Boolean Functions','Using as logic modules two-input one-output arbitrary logic gates, this note considers the problem of the longest chain (number of levels) in a tree-type interconnection realizing a Boolean function of n variables. Specifically, we are interested in the minimum number of levels L(n) by which we can constructively realize all Boolean functions of n variables. It was previously shown that L(n)=n for n=3, 4 and it was so conjectured for n= 5; in this note we are able to show that this holds for n=5, 6, 7, 8.',117351,'IEEE Transactions on Computers',1),(1011243,NULL,'1970',NULL,'Application of the Karhunen-Loève Expansion to Feature Selection and Ordering','The Karhunen-Lo6ve expansion has been used previously to extract important features for representing samples taken from a given distribution. A method is developed herein to use the Karhunen-Loeve expansion to extract features relevant to classification of a sample taken from one of two pattern classes. Numerical examples are presented to illustrate the technique.',117351,'IEEE Transactions on Computers',2),(1011260,NULL,'1983',NULL,'Finite Precision Rational Arithmetic: An Arithmetic Unit','The foundations of an arithmetic unit performing the add, subtract, multiply, and divide operations on rational operands are developed. The unit uses the classical Euclidean algorithm as one unified algorithm for all the arithmetic operations, including rounding. Binary implementations are discussed, based on techniques known from SRT division, and utilizing ripple-free borrow-save and carry-save addition. Average time behavior is investigated.',117351,'IEEE Transactions on Computers',8),(1011323,NULL,'1978',NULL,'Sequential Hierarchical Scene Matching','The general approach to matching two scenes by a digital computer is usually costly in computations. A match is determined by selecting the position of maximum cross correlation between the window and each possible shift position of the search region. A new approach which is logarithmically efficient is presented in this paper. Its logarithmic efficiency and computational savings will be demonstrated both theoretically and in practical examples. Experimental results are presented for matching an image region corrupted by noise and for matching images from optical and radar sensors. The significance of this approach is that scene matching can be accomplished by the use of a computer even in cases which are difficult for humans or standard correlation techniques, and can be accomplished with greatly reduced computations.',117351,'IEEE Transactions on Computers',2),(1011327,NULL,'1973',NULL,'The Quasi-Serial Multiplier','A novel technique for digital multiplication is presented that represents a considerable departure from conventional (i.e., add and shift or fully parallel) multiplication algorithms. The quasi-serial multiplier generates the bits of the product sequentially from least significant to most significant. Each bit is computed by \"counting\" the number of ones in the corresponding column of the bit-product matrix and adding the previous carrys. This single operation yields both the product bit and the carrys for the next column. The quasi-serial multiplier requires 2n of these count and add operations to determine the product of two n-bit numbers.',117351,'IEEE Transactions on Computers',8),(1011330,NULL,'1973',NULL,'The Counting Recursive Digital Filter','Analysis of the bit-level operations involved in the convolutions realizing recursive digital filters leads to hardware designs of such filters based on the operation of counting. Various designs realizing both the canonic and \"direct\" forms are presented with particular emphasis on low-cost low-speed high-flexibility machines.',117351,'IEEE Transactions on Computers',10),(1011374,NULL,'1970',NULL,'The Synthesis of Redundant Threshold- Logic Elements','This paper is devoted to the development of procedures for synthesizing threshold-logic elements which can be employed in redundant networks based on the von Neumann multiplexing principle. A major result is the statement of rules for synthesizing elements whose outputs are correct despite the presence of e or fewer errors on the input lines. Several theorems are proved and used as the basis for establishing synthesis rules for four canonical redundant threshold element specifications. Some of these canonical elements are optimum in the sense that they require the minimum sum of weights or minimum sum of input bundle sizes.',117351,'IEEE Transactions on Computers',1),(1011423,NULL,'1975',NULL,'Fault-Tolerant Computing: An Introduction and a Perspective','FAULT-TOLERANT computing has been defined as \"the ability to execute specified algorithms correctly regardless of hardware failures, total system flaws, or program fallacies\" [1]. To the extent that a system falls short of meeting the requirements of this definition, it can be labeled a partially fault-tolerant system [2]. Thus the definition of fault-tolerant computing provides a standard against which to measure all systems having a degree of fault tolerance. In particular, one can classify systems according to: 1), the amount of manual intervention required in performing three basic functions, and 2) the class of faults covered by three basic functions involved in fault tolerance: system validation, fault diagnosis, and fault masking or recovery. The word \"fault\" here is used to inclusively describe \"failures, flaws, and fallacies\" in the original definition. The first function is involved in the design and production of the system hardware and software, while the last two functions are embodied in the system itself. Likewise, the first function is directed to handling faults arising from design and production errors, whereas the last two functions are aimed at faults due to random hardware failures.',117351,'IEEE Transactions on Computers',6),(1011425,NULL,'1975',NULL,'On-Line Diagnosis of Unrestricted Faults','A formal model for the study of on-line diagnosis is introduced and used to investigate the diagnosis of unrestricted faults. Within this model a fault of a system S is considered to be a transformation of S into another system S\' at some time r. The resulting faulty system is taken to be the system which looks like S up to time r and like S\' thereafter. Notions of fault tolerance and error are defined in terms of the resulting system being able to mimic some desired behavior as specified by a system S. A notion of on-line diagnosis is formulated which involves an external detector and a maximum time delay within which every error caused by a fault in a prescribed set must be detected.',117351,'IEEE Transactions on Computers',6),(1011433,NULL,'1975',NULL,'Reliability Modeling of Compensating Module Failures in Majority Voted Redundancy','The classical reliability model for N-modular redundancy (NMR) assumes the network to be failed when a majority of modules which drive the same voter fail. It has long been known that this model is pessimistic since there are instances, termed compensating module failures, where a majority of the modules fail but the network is nonfailed. A different module reliability model based on lead reliability is proposed which has the classical NMR reliability model as a special case. Recent results from the area of test generation are employed to simplify the module reliability calculation under the lead reliability model. First a fault equivalent technique, based on functional equivalence of faults, is developed to determine the effect of compensating module failures on system reliability. This technique can increase the predicted mission time (the time the system is to operate at or above a given reliability) by at least 40 percent over the classical model prediction for simple networks. Since the fault equivalent technique is too complex for modeling of large circuits a second, computational simpler technique, based on fault dominance, is derived. It is then shown to yield results comparable to the fault equivalent technique. A more complex example circuit analyzed by the fault dominance model shows at least a 75 percent improvement in mission time due to modeling compensating module failures. A commercialy available 31 gate integrated circuit chip is also modeled to demonstrate the applicability of the technique to large circuits.',117351,'IEEE Transactions on Computers',6),(1011494,NULL,'1982',NULL,'The Extra Stage Cube: A Fault-Tolerant Interconnection Network for Supersystems','The Extra Stage Cube (ESC) interconnection network, a fault-tolerant structure, is proposed for use in large-scale parallel and distributed supercomputer systems. It has all of the interconnecting capabilities of the multistage cube-type networks that have been proposed for many supersystems. The ESC is derived from the Generalized Cube network by the addition of one stage of interchange boxes and a bypass capability for two stages. It is shown that the ESC provides fault tolerance for any single failure. Further, the network can be controlled even when it has a failure, using a simple modification of a routing tag scheme proposed for the Generalized Cube. Both one-to-one and broadcast connections under routing tag control are performable by the faulted ESC. The ability of the ESC to operate with multiple faults is examined. The ways in which the ESC can be partitioned and permute data are described.',117351,'IEEE Transactions on Computers',7),(1011520,NULL,'1981',NULL,'The Universality of the Shuffle-Exchange Network','This paper has focused on the realization of every arbitrary permutation with the shuffle-exchange network. Permutation properties of shuffle-exchange networks are studied and are used to demonstrate several universal networks. It is concluded that 3(log2 N) -1 passes through a single-stage regular shuffle exchange network are sufficient to realize every arbitrary permutation where N is network size. A routing algorithm is also developed to calculate control settings of the shuffle-exchange switches for the permutation realization. Three optimal universal networks, namely, expanded direct- connection shuffle-exchange network, multiple-pass omega network, and modified shuffle-exchange network are then exploited for better interconnection purposes. In addition, this work specifies the inherent relationship between the shuffle-exchange network and the Benes binary network so that designers can have a broad prospect.',117351,'IEEE Transactions on Computers',7),(1011560,NULL,'1978',NULL,'Recognition of Spoken Words and Phrases in Multitalker Environment Using Syntactic Methods','We describe a method of recognizing isolated words and phrases from a given vocabulary spoken by any member in a given group of speakers, the identity of the speaker being unknown to the system. The word utterance is divided into 20-30 nearly equal frames, frame boundaries being aligned with glottal pulses for voiced speech. A constant number of pitch periods are included in each frame. Statistical decision rules are used to determine the phoneme in each frame. Using the string of phonemes from all the frames of the utterance, a word decision is obtained using (phonological) syntactic rules. The syntactic rules used here are of 2 types, namely, 1) those obtained from the theory of word construction from phonemes in English as applied to our vocabulary, 2) those used to correct possible errors in phonemic decisions obtained earlier based on the decisions of neighboring segments. In our experiment, the vocabulary had 40 words, consisting of many pairs of words which are phonemically close to each other. The number of speakers was 6. The identity of the speaker is not known to the system. In testing 400 words utterances, the recognition rate was about 80 percent for phonemes (for 11 phonemes) but the word recognition was 98.1 percent correct. Phonological-syntactic rules played an important role in upgrading the word recognition rate over the phoneme recognition rate.',117351,'IEEE Transactions on Computers',2),(1011576,NULL,'1977',NULL,'A Method to Generate the Prime Cascades of an Arbitrary Switching Function','A method is described to generate all prime cascades of the restricted Maitra type with fixed input order, of an arbitrary switching function. The method allows the optimal synthesis of cutpoint cellular arrays.',117351,'IEEE Transactions on Computers',1),(1011584,NULL,'1969',NULL,'Realization of Sequential Machines with Threshold Elements','The state assignment problem for finite-state sequential machines is examined in the context of threshold logic. An algorithm is developed for assigning binary codes to the states, inputs, and outputs so that the state variable and output variable functions satisfy the necessary condition of 2-assumability that they be linearly separable. The algorithm deals with 2-block partitions by which the assignments are made. First those partitions which cannot be used are computed. For each of the remaining partitions, a list is compiled of those partitions of which one must be used if the given partition is used. Finally, a method is given for constructing the sets of partitions satisfying these constraints and having zero product. Code assignments made by these partition sets will induce 2-asummable functions.',117351,'IEEE Transactions on Computers',1),(1011586,NULL,'1969',NULL,'Multigate Synthesis of General Boolean Functions by Threshold Logic Elements','A decomposition and reconstruction approach for synthesizing an arbitrary Boolean function with a minimum number of threshold logic elements connected by feedforward paths only is presented. Attention is mainly focused on cascade-type realizations. The approach has the advantage that near-minimal solutions are readily derived. An estimate of how closely the minimality has been approached is obtainable in this method. The method has been successfully applied by the authors to Boolean functions of 5 and 6 variables.',117351,'IEEE Transactions on Computers',1),(1011596,NULL,'1974',NULL,'Linearity Testing and Realization of Sequential Machines','The necessary and sufficient conditions for the linear realizability of a sequential machine are stated. The discussion considers the state transition and output functions separately and thus machines with nonlinear output function can also be tested for linearity of the state transition function. It then goes on to determine the characterizing matrices and corresponding secondary state assignment when such realization exists. The method can be applied to both permutation and nonpermutation machines.',117351,'IEEE Transactions on Computers',1),(1011600,NULL,'1974',NULL,'An Experimental Procedure for Handwritten Character Recognition','Most of the work done so far on recognition of handwritten script uses curve tracing techniques. The present paper describes a recognition scheme which is independent of the dynamics of writing and is suitable both for on-line and off-line systems. New recognition criteria, namely, the distribution of intensity of marking along and perpendicular to the direction of writing are used. Methods for correcting the inclination of script, determination of zonal limits and segmentation of the continuous script into a set of curve elements have been developed. The method of correlation is used for classification of the curve-elements. A simple grammer for the reconstruction of letters from classified segments is presented. A significant recognition has been found.',117351,'IEEE Transactions on Computers',2),(1011610,NULL,'1974',NULL,'A Computer Algorithm for the Synthesis of Memoryless Logic Circuits','A method has been investigated for the synthesis of memoryless logical networks using a restricted repertoire of functional modules. The method is based on the reduced general solution to a generalized system of Boolean equations (BE) as applied to the decomposition of Boolean functions. The aim of the synthesis is to obtain the most constrained circuit having at most two levels of gating. The constraints take the form of single input variables or constant logic levels applied to the inputs of the first level gate. This is achieved by assembling a set of constraint equations which are then a part of the generalized system of BE. The method is then tested on some synthesis examples of single and multiple output functions in terms of the NAND and (WOS) modules.',117351,'IEEE Transactions on Computers',1),(1011655,NULL,'1978',NULL,'Efficiency of Random Compact Testing','Random compact testing uses random inputs to test digital circuits. Fault detection can be achieved by comparing some statistic of the circuit under test, e.g., the frequency of logic ones at an output, with the value of that statistic previously determined for the fault-free circuit. In this paper, we show that random compact testing can efficiently detect failures in both combinational and sequential circuits. Although this testing method cannot guarantee detection of all faults, it provides a simple way to detect the vast majority of failures in most circuits. The effects of failures inside combinational circuits are modeled in relation to the statistical property measured by the test and a general evaluation of the testing efficiency is obtained. The probability of detection is shown to increase with the test length and to be dependent upon test parameters such as the statistics of the input sequence. For sequential circuits, the uncertainty of the initial state necessitates an initialization step, which is a long sequence of random inputs. The length of such an initialization sequence is circuit dependent, but for most circuits, proper initialization can be achieved in a few seconds. Most failures inside the memory elements are easily detected, even with short tests. Random compact testing can also detect most of the failures inside the excitation logic and the output circuitry. There, as for combinational circuits, its efficiency is largely dependent upon the test length. Some of the requirements and tradeoffs to achieve efficient detection are presented.',117351,'IEEE Transactions on Computers',11),(1011667,NULL,'1983',NULL,'Abstractions for Node Level Passive Fault Detection in Distributed Systems','We introduce a scheme for passive node-level fault detection in a distributed system. With each system node associate a low-cost, low-complexity observer which monitors the pattern of incoming and outgoing messages and compares it against an abstracted model of the node\'s behavior. We develop a fault detection procedure, which is probabilistic because of nondeterminism in the simplified node model. Abstraction reduces model complexity, but renders some errors undetectable by the observer. In the paper we characterize these undetectable errors. Succeeding studies show how to select model abstractions to lower the number of undetectable errors.',117351,'IEEE Transactions on Computers',6),(1011673,NULL,'1983',NULL,'Sign Detection and Implicit-Explicit Conversion of Numbers in Residue Arithmetic','A new method of sign detection is proposed. The advantage of this method is a possibility of simultaneous execution of two operations: residue to mixed-radix conversion of the number magnitude and sign detection in one and the same circuit (implicit-explicit conversion).',117351,'IEEE Transactions on Computers',8),(1011674,NULL,'1983',NULL,'A Simple Random Test Procedure for Detection of Single Intermittent Fault in Combinational Circuits','This paper suggests a method for near-optimal selection of input vector probabilities for random testing of intermittent faults in combinational circuits. The assignment of input vector probabilities is obtained by equalizing the quality factors of all test vectors in a simple way. It is shown that the degree of fault detection is comparable with that of Savir [2].',117351,'IEEE Transactions on Computers',11),(1011680,NULL,'1984',NULL,'Error Detection Process Model, Design, and Its Impact on Computer Performance','Conventionally, reliability analyses either assume that a fault/error is detected immediately as it occurs, or ignore damage caused by imperfect detection mechanisms and error latency, namely, the time interval between the occurrence of an error and the detection of that error.',117351,'IEEE Transactions on Computers',6),(1011700,NULL,'1980',NULL,'Syndrome-Testable Design of Combinational Circuits','Classical testing of combinational circuits requires a list of the fault-free response of the circuit to the test set. For most practical circuits implemented today the large storage requirement for such a list makes such a test procedure very expensive. Moreover, the computational cost to generate the test set increases exponentially with the circuit size.',117351,'IEEE Transactions on Computers',11),(1011702,NULL,'1980',NULL,'Diagnosis Without Repair for Hybrid Fault Situations','For a new class of fault situations called hybrid fault situations, we consider the fault diagnosing capabilities of systems which for testing/monitoring purposes can be viewed as being composed of independent units. The classical Preparata, Metze, and Chien model (PMC model) is used to specify the various testing assignments among the units. Hybrid fault situations are described as explicitly bounded combinations of permanently and intermittently faulty units. This new concept of a hybrid fault situation includes as special cases the all permanent fault case and the unrestricted intermittent fault case which have both been previously considered with PMC models. A general characterization of the so-called connection assignment of a PMC model is established for a diagnosing capability which is referred to as hybrid fault diagnosability without repair. This diagnosing capability is compatible with the well-known permanent fault diagnosability without repair concept, and the quality of this diagnosing capability is shown to be correct, but sometimes an incomplete diagnosis where the incompleteness is solely a consequence of intermittently faulty units. The characterization for hybrid fault diagnosability without repair is seen to encompass as extreme cases the previously known special characterizations for permanent and unrestricted intermittent diagnosability without repair. Fundamental interrelationships are determined among parameter bounds used to describe the hybrid fault cases which can be diagnosed for a given PMC model.',117351,'IEEE Transactions on Computers',6),(1011720,NULL,'1982',NULL,'Exception Handling and Software Fault Tolerance','Some basic concepts underlying the issue of fault-tolerant software design are investigated. Relying on these concepts, a unified point of view on programmed exception handling and default exception handling based on automatic backward recovery is constructed. The cause-effect relationship between software design faults and failure occurrences is explored and a class of faults for which default exception handling can provide effective fault tolerance is characterized. It is also shown that there exists a second class of design faults which cannot be tolerated by using default exception handling. The role that software verification methods can play in avoiding the production of such faults is discussed.',117351,'IEEE Transactions on Computers',6),(1011772,NULL,'1969',NULL,'Transformation of Ternary Switching Functions to Completely Symmetric Ternary Switching Functions','It is shown that any ternary switching function can be transformed to a completely symmetric ternary switching function in which some of the variables are repeated. The partial symmetry information inherent in a given function plays a key role in this transformation.',117351,'IEEE Transactions on Computers',1),(1011787,NULL,'1976',NULL,'A Nine-Valued Circuit Model for Test Generation','A nine-valued circuit model for test generation is introduced which takes care of multiple and repeated effects of a fault in sequential circuits. Using this model test sequences can be determined which allow multiple and repeated effects of faults on the internal state of a sequential circuit. Thus valid test sequences are derived where other known procedures, like the D-algorithm, do not find any test although one exists.',117351,'IEEE Transactions on Computers',11),(1011814,NULL,'1971',NULL,'Realization of Nonlinearly Separable Switching Functions','In this paper CEP functions are applied to realize nonlinearly separable switching functions. It is proved that the introduction of these functions may realize all the 22nswitching functions of n binary variables. A model of a nonlinear threshold component is proposed to realize the switching functions.',117351,'IEEE Transactions on Computers',1),(1011864,NULL,'1975',NULL,'An Algorithm for the Generation of Test Sets for Combinational Logic Networks','An algorithm is developed for generating a single-fault detection test set to be used in a combinational logic network. This algorithm has two unique characteristics. 1) When a test is generated, a list of faults detected by this test is available. Fault simulation, therefore, is not required after the test has been generated. 2) It generates a test set rather than a single test. Each test, with the exception of the first one, is based on a previous test. Repetition of effort and overlapped coverage of faults for different test generations are thus reduced.',117351,'IEEE Transactions on Computers',11),(1011882,NULL,'1980',NULL,'Polylogic Realization of Switching Functions','The properties of polynomic functions on Rn are reviewed. Such functions are then used to establish a general synthesis procedure for arbitrary multivalued switching functions. The relationship with threshold logic and Boolean logic is also explored.',117351,'IEEE Transactions on Computers',1),(1011894,NULL,'1982',NULL,'The Containment Set Approach to Upsets in Digital Systems','Fault analysis of digital systems is highly dependent upon the fault model employed. Much previous work utilizes fault models known to contain inaccuracies in order to permit mathematically tractable analysis. In this correspondence a new approach is taken which combines faults, hardware, and software together into one overall model. This new model is shown to be useful for the consideration of intermittent/transient faults. It supports a new method, based on the novel concept of a containment set, for realizing transient fault tolerance without massive redundancy. It also allows for a new approach to system fault tolerance evaluation and validation which uses a transition matrix which is defined in terms of the containment set.',117351,'IEEE Transactions on Computers',6),(1011917,NULL,'1978',NULL,'Fault-Tolerant Asynchronous Networks Using Read-Only Memories','In this paper, we present techniques for the construction of certain redundant state assignments suitable for fault-tolerant asynchronous network design. These assignments have certain characteristics that make them well-suited for error-control in asynchronous networks, using read-only memories, and are as follows: only a small number of additional state variables are required for incorporating fault-tolerance properties; the assignments form the codewords of an error-correcting code; and are systematic. An example is also provided to clarify some misconceptions that have arisen over the fault-tolerant design proposed earlier by the authors',117351,'IEEE Transactions on Computers',6),(1011920,NULL,'1977',NULL,'On-Line Algorithms for Division and Multiplication','In this paper, on-line algorithms for division and multiplication are developed. It is assumed that the operands as well as the result flow through the arithmetic unit in a digit-by-digit, most significant digit first fashion. The use of a redundant digit set, at least for the digits of the result, is required.',117351,'IEEE Transactions on Computers',8),(1011931,NULL,'1969',NULL,'R69-21 Hybrid Assumed Mode Solution of Non-Linear Partial Differential Equations','Readers of this paper are likely to come away with too narrow a view of the little-known and little-appreciated art of assumed mode methods for solving distributed parameter system problems. The fact that only three references to these methods are cited is evidence of the wide scattering of the many papers which have appeared concerning these methods. The papers cited at the end of this review have been carefully selected to enable the reader to enter the most recent literature on a broader front. There the reader will find many more assumed mode methods than just the Galerkin method (which is the only one mentioned by Newman and Strauss). The reader will also find, contrary to an assertion by the authors, that assumed mode methods do not offer equal accuracy with fewer equations, as compared with nodal methods [1], although this possibility exists when a problem involves only a restricted class of system behaviors [2]. The literature also contains examples of nonlinear problems which have been solved on analog computers by assumed mode methods, e.g., [2], which the authors apparently had not encountered.',117351,'IEEE Transactions on Computers',2),(1011934,NULL,'1969',NULL,'The Characterization and Properties of Cascade Realizable Switching Functions','This paper is a study of switching functions realizable by a single cascaded switching network composed of two-input, one-output elements where a distinct variable is applied to one input and the other receives the output of the previous element. A special type of Boolean formula called a standard cascade form is introduced with the property that all cascade realizable functions, and only these, can be written in this form. This characterization leads to a strong necessary condition on such functions: there is no consensus for any pair of its prime implicants which are, therefore, all core. It also leads to a new, efficient procedure for testing an arbitrary function f for cascade realizability. The test operates on the prime implicants of f, and yields a realization employing a particular complete set of cell types from which it is especially easy to derive any other realization using any other complete set.',117351,'IEEE Transactions on Computers',1),(1011949,NULL,'1976',NULL,'A Numerical Expansion Technique and Its Application to Minimal Multiplexer Logic Circuits','A method of realizing arbitrary combinational switching functions with multiplexers is derived. These circuits are divided in two classes where the first allows only uncomplemented variables as control inputs and the second has unrestricted inputs. The selected inputs to each multiplexer in the first class of circuits (tree circuits) are shown to be residue functions of the output function. Using this fact, it is demonstrated that many functional properties simplify realization procedures.',117351,'IEEE Transactions on Computers',1),(1011977,NULL,'1970',NULL,'A Transform Approach to Logic Design','This paper describes a new approach to the design of combinational logic using large-scale integrated (LSI) circuit technology. A simple \"prototype\" logic function of n binary variables is imbedded within an array of at most (n+1) rows and columns. The cells of this array contain two-input EXCLUSIVE-OR gates, and its rows are fed by the input variables and logical \"1.\" Its column outputs are first-degree polynomial functions of the input variables. These functions supply inputs to, and modify the output of, the prototype in order to realize the desired function. These transformations form a group; specifically, the largest subgroup of the (n+1)-dimensional affine group such that input variable encodings are not affected by feedback from the function\'s output.',117351,'IEEE Transactions on Computers',1),(1012004,NULL,'1972',NULL,'The Application of Filtered Transforms to the General Classification Problem','An image classification model based on nearest prototypes in filtered Fourier and Walsh transform domains is presented. A computer simulation of the model applied to handwritten English letters, Russian letters, numerals, and electromagnetic signals is also presented. Experiments to date fail to refute the working hypothesis that generalized harmonic analysis can be used to reliably classify alphabet characters, time-varying signals, and other images.',117351,'IEEE Transactions on Computers',2),(1012075,NULL,'1969',NULL,'Optimal Synthesis of Arbitrary Switching Functions with Regular Arrays of 2-Input 1-Output Switching Elements','Most switching functions are not realizable by a single cascade of 2-input 1-output switching elements, even if repeated inputs are allowed. However, arrays of such cascades feeding a single collector cascade of AND or OR cells can be used to synthesize any function. This paper is concerned with optimal array realizations of this form.',117351,'IEEE Transactions on Computers',1),(1012083,NULL,'1981',NULL,'The Prospects for Multivalued Logic: A Technology and Applications View','Advances in multiple-valued logic (MVL) have been inspired, in large part, by advances in integrated circuit technology. Multiple-valued logic has matured to the point where four-valued logic is now part of commercially available VLSI IC\'s. Besides reduction in chip area, MVL offers other benefits such as the potential for circuit test. This paper describes the historical and technical background of MVL, and areas of present and future application. It is intended, as well, to serve as a tutorial for the nonspecialist.',117351,'IEEE Transactions on Computers',10),(1012098,NULL,'1980',NULL,'The Reverse-Exchange Interconnection Network','Properties of the reverse-exchange interconnection network are used to develop a reconfiguration scheme and a two-pass structure for enhancing the efficiency of a class of multistage interconnection networks. Functional relationships among a class of multistage interconnection networks are first derived. According to the functional relationships, we propose a reconfiguration scheme which enables a network to accomplish various interconnection functions of other networks. Then the admissible permutations along with related recursive control algorithms of the reverse-exchange interconnection network are specified through a set of theorems. Using the reverse-exchange property, we also prove that the algorithms actually work. Finally, we prove that arbitrary permutations can be realized in two passes (or 2 · 1og2N switching steps where N is the network size). By taking advantage of Benes network control algorithms, a way to control the two-pass structure is also developed.',117351,'IEEE Transactions on Computers',7),(1012140,NULL,'1976',NULL,'Logic Design Using EFL Structures','The emitter function iogic (EFL) structure is a bipolar integratable circuit suitable for large-scale-integration (LSI). The structure is a development of current-mode logic and it permits the LSI realization of complex logic functions with similar silicon area and power dissipation to a single conventional ECL gate. In order to use the structure in logic design, some of its properties are developed and a logic design technique is given with examples.',117351,'IEEE Transactions on Computers',10),(1012211,NULL,'1975',NULL,'Reliability Analysis of Systems with Concurrent Error Detection','There is an increasing use of error detectors and correctors in computer subsystems, such as parity detectors in memory modules and residue checkers in arithmetic units. Their fault tolerant characteristics are studied through the model of detector redundant systems. Their reliabilities and availabilities are analyzed and compared with those which do not have any such error detectors. The design of fault isolating and reconfiguring networks used in the implementation of such systems are developed.',117351,'IEEE Transactions on Computers',6),(1012212,NULL,'1975',NULL,'An Experimental System for Computer Controlled Mechanical Assembly','This paper describes the software and hardware architecture of a system designed as a research tool for experiments on programming the computer controlled assembly of mechanical objects. The software consists of a real-time control level and a background level in which an on-line interpreter permits interactive programming. The hardware consists of a manipulator with sensory feedback coupled to an IBM System/7. Additional facilities are available through a link to an IBM System/370 Model 145. The application of the system to sample assemblies is also discussed.',117351,'IEEE Transactions on Computers',5),(1012304,NULL,'1978',NULL,'Computer Description of Bodies Bounded by Quadric Surfaces from a Set of Imperfect Projections','This paper describes a computer program for constructing a description of solid bodies from a set of pictures taken from different vantage points. The bodies are assumed to be bounded by faces which are planar or quadric, and to have vertices formed by exactly three faces It is assumed that a preprocessor provides the program with line and junction information which it has extracted from the pictures. The preprocessor is expected to make mistakes, such as losing features or providing misinformation about their nature. A technique is presented for validating doubtful features as well as for matching corresponding features extracted from the different pictures. New grammar rules are developed for linedrawing projections of curved and planar bodies and are used as a tool in the scene analysis process. Each picture\'s data analysis is supported dynamically by the results obtained thus far in the other pictures\' analysis. The analyzed data from all pictures are grouped into sets, each corresponding to a single face (flat or curved), whose nature is also determined. The sets are grouped again to correspond to the different bodies in the scene. The program written in PL/I has been tested successfully on several scenes.',117351,'IEEE Transactions on Computers',2),(1012314,NULL,'1983',NULL,'A Two\'s Complement Array Multiplier Using True Values of the Operands','A new algorithm for implementing the two\'s complement multiplication of an m 脳 n bit number is described. By interpreting certain positive partial product bits as negative, a parallel array is developed which has the advantage of using only one type of adder cell. A comparison with the Pezaris and Baugh-Wooley arrays is presented, showing that the new array is as fast as the Pezaris array and uses less hardware than the Baugh-Wooley implementation.',117351,'IEEE Transactions on Computers',8),(1012317,NULL,'1983',NULL,'A Simplified Method to Calculate Failure Times in Fault-Tolerant Systems','A simplified method is presented to calculate moments of failure time and residual lifetime of a fault-tolerant system. The method is based on recent results in queueing theory. Its effectiveness is illustrated by considering a dual repairable system from the literature.',117351,'IEEE Transactions on Computers',6),(1012330,NULL,'2007',NULL,'PRESENCE: A Human-Inspired Architecture for Speech-Based Human-Machine Interaction','Recent years have seen steady improvements in the quality and performance of speech-based human-machine interaction driven by a significant convergence in the methods and techniques employed. However, the quantity of training data required to improve state-of-the-art systems seems to be growing exponentially, and performance appears to be asymptoting to a level that may be inadequate for many real-world applications. This suggests that there may be a fundamental flaw in the underlying architecture of contemporary systems, as well as a failure to capitalize on the combinatorial properties of human spoken language. This paper addresses these issues and presents a novel architecture for speech-based human-machine interaction inspired by recent findings in the neurobiology of living systems. Called PRESENCE \'PREdictive SENsorimotor Control and Emulation\' - this new architecture blurs the distinction between the core components of a traditional spoken language dialogue system and, instead, focuses on a recursive hierarchical feedback control structure. Cooperative and communicative behavior emerges as a by-product of an architecture that is founded on a model of interaction in which the system has in mind the needs and intentions of a user, and a user has in mind the needs and intentions of the system.',117351,'IEEE Transactions on Computers',5),(1012347,NULL,'1981',NULL,'Reliability Measure of Hardware Redundancy Fault-Tolerant Digital Systems with Intermittent Faults','While significant results are available which allow estimation of reliability measure for systems with permanent faults, no generally applicable results are available for intermittent (transient) faults. Methods are presented here which allow reliability evaluation for systems with both intermittent and permanent faults. Two reliability measures, instantaneous and durational reliabilities, are defined and methods to compute them are given. Computed results for the durational reliability for various redundancy schemes are compared.',117351,'IEEE Transactions on Computers',6),(1012352,NULL,'1981',NULL,'A Layout System for the Random Logic Portion of an MOS LSI Chip','The random logic portion of an MOS LSI chip intended mainly for a calculator is constructed of an array of MOS complex gates, each composed of an MOS ratioless circuit with a multiphase clocking system, and occupies ordinarily a considerable part of chip area. In this paper a layout system for this portion of an LSI chip is described, which is constructed on the basis of heuristics for a set of interrelated optimization problems. Implementation results of the layout system are also shown to reveal that the random logic portion can be realized in such an areas as comparable to one done by manual layout.',117351,'IEEE Transactions on Computers',10),(1012362,NULL,'1976',NULL,'Introduction Tutorial on Resolution','Automated theorem proving involves the programming of computers to perform logical (mathematical) deduction. This should not be confused with numerical calculation, in which operations that need to be performed can be exactly specified ahead of time as, for example, in Gaussian elimination. Rather, theorem provers search for proofs of statements given axioms describing the basic assumptions such as would occur in a modern algebra text on group theory. There are many theorem-proving programs that are based on ad hoc data representations and manipulations;many such techniques are derived by the programmer analyzing how he himself proves theorems. However, the most widely studied and best understood general method is based on the resolution principle for first-order logic of Robinson [9]. Indeed, all the papers in this issue have resolution as a starting point.',117351,'IEEE Transactions on Computers',2),(1012375,NULL,'1970',NULL,'Enumeration of Threshold Functions of Eight Variables','The number of threshold functions of eight variables is counted by ILLIAC II, the computer of the University of Illinois. Sets of optimum weights of majority elements realizing these functions also are investigated. Actually, canonical positive self-dual threshold functions of nine variables are investigated instead of directly investigating threshold functions of eight variables because it is easier to deal with them. The number and optimum weights of threshold functions of eight variables are easily obtained from these functions of nine variables and their realization.',117351,'IEEE Transactions on Computers',1),(1012411,NULL,'1975',NULL,'Multiple-Output Optimization with Mosaics of Boolean Functions','An algorithmic procedure is presented which produces an optimal two-level multiple-output network. In agreement with procedures suggested by other authors, the set of given (incompletely specified) Boolean functions of variables xi is transformed into a single (mosaic) function f( y,x) by using additional variables yi, related to the structure of the resulting network. The agreement ends here, however. It is shown that the function f(y, x) can be composed in such a way that the following statement can be proven as true.',117351,'IEEE Transactions on Computers',1),(1012425,NULL,'1975',NULL,'Delayed Universal Logic Modules and Sequential Machine Synthesis','A universal logic module of n variables (ULM-n) can be considered as a 2n-in-1 multiplexer (inverse binary tree) whose code is determined by the presentation of external variables. This interpretation permits synthesis of sequential machines. Starting from the state transition map, any sequential machine can be realized by delayed ULM\'s and OR gates. The decomposition concept of Weiner and Hopcroft is generalized to form a uniform synthesis of n-variable sequential machines.',117351,'IEEE Transactions on Computers',1),(1012436,NULL,'1970',NULL,'Binary Logic for Residue Arithmetic Using Magnitude Index','We consider a residue number system using n pairwise relatively prime moduli m1,?,mnto represent any integer X in the range M/ 2=XM/2, when M = ?mi. The moduli miare chosen to be of the 2-1 type, in order that the residue arithmetic can be implemented by means of binary registers and binary logic. Further, for each residue number X, a magnitude index Pxis maintained for all arithmetic operations. We investigate the properties of such a system and derive the addition, subtraction, multiplication, sign determination, and overflow detection algorithms. The proposed organization is found to improve the operation times for sign detection and overflow detection operations, while rendering multiplication to be a difficult operation.',117351,'IEEE Transactions on Computers',8),(1012445,NULL,'1980',NULL,'On a Class of Multistage Interconnection Networks','A baseline network and a configuration concept are introduced to evaluate relationships among some proposed multistage interconnection networks. It is proven that the data manipulator (modified version), flip network, omega network, indirect binary n-cube network, and regular SW banyan network (S = F = 2) are topologically equivalent. The configuration concept facilitates developing a homogeneous routing algorithm which allows one-to-one and one- to-many connections from an arbitrary side of a network to the other side. This routing algorithm is extended to full communication which allows connections between terminals on the same side of a network. A conflict resolution scheme is also included. Some practical implications of our results are presented for further research.',117351,'IEEE Transactions on Computers',7),(1012543,NULL,'1972',NULL,'Minimum Two-Level Threslold Gate Realizations','This paper deals with the synthesis of two-level networks of threshold gate logic elements for realization of nonlinearly separable switching functions. The realization obtained contains the minimum number of threshold logic elements possible for a two-level realization. An algorithm based on the tree procedure of Coates and Lewis is developed which can be used to obtain the desired network realization for a given switching function. The function may be incompletely specified.',117351,'IEEE Transactions on Computers',1),(1012577,NULL,'1976',NULL,'A Realization of the RAM Digital Filter','The digital filtering algorithm of W. D. Little, which employs a large RAM to obtain high speed, is implemented in a simple hardware configuration.',117351,'IEEE Transactions on Computers',10),(1012606,NULL,'1969',NULL,'Simplification of Incompletely Specified Flow Tables with the Help of Prime Closed Sets','In this short note, an attempt has been made to arrive at a general algorithm for minimizing the number of internal states in an incompletely specified flow table. The closure property of the compatibility classes which cover a given flow table leads us to the derivation of a particular class of closed sets defined as irredundant prime closed sets. It has been shown that these prime closed sets in sequential circuit synthesis play an analogous role to that of the prime implicants in combinational circuit synthesis. A method has been described for determining all the irredundant prime closed sets and finding the minimal row flow table by suitably choosing one or a collection of those sets.',117351,'IEEE Transactions on Computers',1),(1012617,NULL,'1973',NULL,'Optimal and Near-Optimal Universal Logic Modules with Interconnected External Terminals','A universal logic module (ULM) with interconnected external terminals contains m input terminals and s auxiliary function terminals. The module implements the function U( Z1, Z2,...,Zm) such that every Boolean function of n variables f( x1, x2,* .* * X,n) can be realized by an appropriate substitution of an element of 5= { xl',117351,'IEEE Transactions on Computers',1),(1012644,NULL,'1977',NULL,'New Concepts for Three-Dimensional Shape Analysis','A new approach to machine representation and analysis of three-dimensional objects is presented. The representation, based on the notion of \"skeleton\" of an object leads to a scheme for comparing two given object views for shape relations. The objects are composed of long, thin, rectangular prisms joined at their ends. The input picture to the program is the digitized line drawing portraying the three-dimensional object. To compare two object views, two characteristic vertices called \"cardinal point\" and \"end-cardinal point,\" occurring consistently at the bends and open ends of the object are detected. The skeletons are then obtained as a connected path passing through these points. The shape relationships between the objects are then obtained from the matching characteristics of their skeletons. The method explores the possibility of a more detailed and finer analysis leading to detection of features like symmetry, asymmetry and other shape properties of an object.',117351,'IEEE Transactions on Computers',2),(1012678,NULL,'1980',NULL,'Merged Arithmetic','The concept of merged arithmetic is introduced and demonstrated in the context of multiterm multiplication/addition. The merged approach involves synthesizing a composite arithmetic function (such as an inner product) directly instead of decomposing the function into discrete multiplication and addition operations. This approach provides equivalent arithmetic throughput with lower implementation complexity than conventional fast multipliers and carry look-ahead adder trees.',117351,'IEEE Transactions on Computers',8),(1012687,NULL,'1983',NULL,'Greedy Diagnosis as the Basis of an Intermittent-Fault/ Transient-Upset Tolerant System Design','Multiple-unit computer systems which are to be tolerant of intermittently faulty units or transiently upset units are considered in this paper. Designs for such systems, which exploit a new so-called greedy diagnosis theory, are developed. Using greedy diagnosis, assessments on the condition of a unit (intermittent-fault case) or the integrity of data (transient-upset case) can be made on the basis of syndromes formed from comparisons of the results of jobs performed by pairs of units. Greedy diagnosis avoids the requirement that for such syndromes to be useful, they must be interpretable from a permanent-fault/continuous-upset perspective.',117351,'IEEE Transactions on Computers',6),(1012699,NULL,'1975',NULL,'Computer Analysis of Moving Polygonal Images','A general mathematical model is developed as an idealization of the problem of determining cloud motions from satellite pictures. The model consists of superimposed planes of rigid moving polygons. The problem is to determine from a sequence of scenes the linear and angular velocities of the figures, and to decompose the scene into its component figures. Study of the model reveals a number of fundamental relations that form the basis for an analysis program. In particular, a systematic anaylsis is given of the topological changes that can occur when overlapping figures move together or apart. A computer program based on these results is described, and experimental results are presented.',117351,'IEEE Transactions on Computers',2),(1012703,NULL,'1975',NULL,'A Note on Automatic Detection of Texture Gradients','The rate and direction of maximum change of texture coarseness across a surface are important cues to the orientation of the surface relative to the observer. This note describes a simple method of automatically detecting these \"texture gradients.\"',117351,'IEEE Transactions on Computers',2),(1012707,NULL,'1975',NULL,'An Algorithm for Finding Nearest Neighbors','An algorithm that finds the k nearest neighbors of a point, from a sample of size N in a d-dimensional space, with an expected number of distance calculations is described, its properties examined, and the validity of the estimate verified with simulated data.',117351,'IEEE Transactions on Computers',9),(1012725,NULL,'2007',NULL,'Enlarging Instruction Streams','The stream fetch engine is a high-performance fetch architecture based on the concept of instruction stream. We call stream to a sequence of instructions from the target of a taken branch to the next taken branch, potentially containing multiple basic blocks. The long size of instruction streams makes it possible for the stream fetch engine to provide high fetch bandwidth and to hide the branch predictor access latency, leading to performance results close to a trace cache at lower implementation cost and complexity. Therefore, enlarging instruction streams is an excellent way for improving the stream fetch engine. In this paper, we present several hardware and software mechanisms focused on enlarging those streams that finalize at particular branch types. However, our results point out that focusing on particular branch types is not a good strategy due to Amdahl\'s law. Consequently, we propose the multiple stream predictor, a novel mechanism that deals with all branch types by combining single streams into long virtual streams. This proposal tolerates the prediction table access latency without requiring the complexity caused by additional hardware mechanisms like prediction overriding. Moreover, it provides high performance results, which are comparable to state-of-the-art fetch architectures, but with a simpler design that consumes less energy.',117351,'IEEE Transactions on Computers',4),(1012726,NULL,'2007',NULL,'Utility Accrual Real-Time Scheduling Under the Unimodal Arbitrary Arrival Model with Energy Bounds','In this paper, we consider timeliness and energy optimization in battery-powered, dynamic, embedded real-time systems, which must remain functional during an operation/mission with a bounded energy budget. We consider application activities that are subject to time/utility function time constraints, statistical assurance requirements on timeliness behavior, and an energy budget, which cannot be exceeded at run-time. To account for the inevitable variability in activity arrivals in dynamic systems, we describe arrival behaviors using the unimodal arbitrary arrival model (or UAM) [15]. For such a model, we present a DVS (dynamic voltage scaling)-based, CPU scheduling algorithm called Energy-Bounded Utility Accrual Algorithm (or EBUA). Since the scheduling problem is intractable, EBUA allocates CPU cycles, scales clock frequency, and heuristically computes schedules using statistical estimates of cycle demands, in polynomial-time. We analytically establish EBUA\'s properties including satisfaction of energy bounds, statistical assurances on individual activity timeliness behavior, optimal timeliness during under-loads, and bounded time for mutually exclusively accessing shared non-CPU resources. Our simulation experiments validate our analytical results and illustrate the algorithm\'s effectiveness and superiority over past algorithms.',117351,'IEEE Transactions on Computers',0),(1012807,NULL,'1978',NULL,'Computer-Aided Logic Design of Two-Level MOS Combinational Networks with Statistical Results','Metal-oxide-semiconductor (MOS) logic elements offer advantages over bipolar logic elements, such as smaller size, complexity, and power consumption, as well as more flexibility and versatility. Since MOS is playing a major role in large-scale integration (LSI), synthesis of MOS networks with a large number of variables is very important.',117351,'IEEE Transactions on Computers',10),(1012905,NULL,'1980',NULL,'Synthesis of Combinational Logic Using Decomposition and Probability','A new algorithm for the synthesis of single-output two-valued combinational logic using decomposition and probability is described. Two probabilistic quantities, the probability of existence of all pertinent decomposition classes, and the probable cost of an N variable function are defined. Results for completely specified functions are derived and tabulated. The extension to incompletely specified functions is discussed. A randomly chosen function is assumed throughout the paper.',117351,'IEEE Transactions on Computers',1),(1013035,NULL,'1977',NULL,'Adaptive Techniques for Photomosaicking','A previous report described techniques for creating photomosaics: first, the two overlapping images are brought into register; second, a seam from the top of the overlap region to its bottom is tracked one row at a time; finally, the resultant artificial edge introduced by the seam is locally smoothed by a ramp function. This correspondence describes improved methods for gray-scale registration and for choosing a best seam path with specified endpoints using dynamic programming.',117351,'IEEE Transactions on Computers',2),(1013114,NULL,'1975',NULL,'System Fault Diagnosis: Closure and Diagnosability with Repair','Determination of the detectability and diagnosability of a digital system containing at most t faulty system components is considered. The model employed is to an extent independent of the means used to implement diagnostic procedures, i.e., whether the tests are accomplished via hardware, software, or combinations thereof. A parameter, called the closure index, is defined which characterizes the capability for executing valid tests in the presence of faults. The closure index can be thought of as the size of the smallest potentially undetectable multiple-fault in the system as modeled. On the basis of this parameter, results are presented which permit the determination of t-fault detectability and t-fault diagnosability with repair for the system. Examples are presented to illustrate the application of the model for systems close to those encountered in actual practice.',117351,'IEEE Transactions on Computers',6),(1013119,NULL,'1975',NULL,'On the Minimization of Tree-Type Universal Logic Circuits','The realization of arbitrary switching functions using the universal logic modules of Yau and Tang [1], [2] has the disadvantage that, in general, large trees of modules result. A decomposition theorem and an algorithm are given for reducing the size of such trees, using what is called a chain decomposition.',117351,'IEEE Transactions on Computers',1),(1013145,NULL,'2007',NULL,'Study of the Effects of SEU-Induced Faults on a Pipeline Protected Microprocessor','This paper presents a detailed analysis of the behavior of a novel, fault-tolerant, 32-bit embedded CPU when compared to a default (non fault-tolerant) implementation of the same processor, during a fault injection campaign of single and double faults. The fault-tolerant processor tested is characterized by per-cycle voting of microarchitectural and the flop-based architectural states, redundancy at the pipeline level and a distributed voting scheme. Its fault-tolerant behavior is characterized for three different workloads from the automotive application domain. The study proposes statistical methods for both the single and dual fault injection campaigns and demonstrates the fault-tolerant capability of both processors in terms of fault latencies, probability of fault manifestation and the behavior of latent faults.',117351,'IEEE Transactions on Computers',6),(1013148,NULL,'2007',NULL,'Using Weighted Scan Enable Signals to Improve Test Effectiveness of Scan-Based BIST','The conventional test-per-scan built-in self-test (BIST) scheme needs a number of shift cycles followed by one capture cycle. Fault effects received by the scan flip-flops are shifted out while shifting in the next test vector like scan testing. Unlike deterministic testing, it is unnecessary to apply a complete test vector to the scan chains. A new scan-based BIST scheme is proposed by properly controlling the scan enable signals of the scan chains. Different weighted values are assigned to the scan enable signals of scan flip-flops in separate scan chains. Capture cycles can be inserted at any clock cycle if necessary. A new testability estimation procedure according to the proposed testing scheme is presented. A greedy procedure is proposed to select a weight for each scan chain. Experimental results show that the proposed method can improve test effectiveness of scanbased BIST greatly, and most circuits can obtain complete fault coverage or very close to complete fault coverage.',117351,'IEEE Transactions on Computers',11),(1013158,NULL,'1976',NULL,'Computer Science and Engineering Education','This paper provides a brief overview of significant developments in computer science and engineering education with discussions on curriculum development, continuing education, and certification. Forecasts of industry needs in the 1980\'s for computer professionals are included, together with the trends in curriculum developments that may auger.',117351,'IEEE Transactions on Computers',5),(1013198,NULL,'1972',NULL,'On Determining Optimum Simple Golay Marking Transformations for Binary Image Processing','A computer program has been written which is capable of evaluating all possible Golay marking transformations related to a particular pattern recognition task using an exhaustive search technique. Such evaluations are reported for separating two image data sets: one taken from biomedical microscopy; the other, aerial reconnaissance.',117351,'IEEE Transactions on Computers',2),(1013211,NULL,'1975',NULL,'System Fault Diagnosis: Masking, Exposure, and Diagnosability Without Repair','Diagnosability without fault repair of a digital system containing at most t faults is considered. A system-level diagnostic model defined in an earlier paper [1] is employed. The model is to an extent independent of the means used to implement diagnostic procedures, i.e., whether the tests are accomplished via hardware, software, or combinations thereof. Two parameters, the masking and exposure indices, are defined. Conjoined with the previously defined closure index, the parameters fundamentally characterize the capability for executing valid tests in a multiple-fault environment. Necessary and sufficient conditions for a system to be t-fault diagnosable without repair are derived in terms of these parameters. Examples are presented to illustrate the application of the model for systems close to those encountered in actual practice.',117351,'IEEE Transactions on Computers',6),(1013222,NULL,'1975',NULL,'Picture Segmentation by Texture Discrimination','This correspondence describes a method of dividing a picture into differently textured regions by thresholding the values of a suitable local picture property. The approach used is a generalization to natural textures of a technique recently proposed by Tsuji. The examples given involve textures that differ in coarseness; a method of estimating texture coarseness by analysis of local property values is also described.',117351,'IEEE Transactions on Computers',2),(1013223,NULL,'1975',NULL,'The Two\'s Complement Quasi-Serial Multiplier','This correspondence develops a multiplier for two\'s complement numbers, similar to the quasi-serial multiplier which operates on sign magnitude numbers. The method offers an alternative to add shift techniques for low cost two\'s complement multiplication.',117351,'IEEE Transactions on Computers',8),(1013233,NULL,'1982',NULL,'An Optimal Illumination Region Algorithm for Convex Polygons','For the convex polygon P having n vertices entirely contained in a convex polygon K having m vertices, an optimal algorithm with running time O(n + m) is presented to compute and name regions in the boundary of K from which it is possible to illuminate the exterior of P. It is also shown that this illumination region algorithm can be used to improve the worst case O(nm) running time of a related two dimensional simplex coverability algorithm so that it too has running time O(n + m), and is thus optimal to within a constant factor.',117351,'IEEE Transactions on Computers',9),(1013291,NULL,'1973',NULL,'Boundary Points of Threshold Functions','Boundary points of a threshold function fare those vertices of the n-cube that produce a minimal irredundant set of inequalities for the realization of f. In this paper they are shown to be those vertices that can be contained in separating hyperplanes. With this theorem it is shown that knowledge of canonical boundary points of self-dual canonical threshold functions allows complete determination of boundary points of equivalent functions. This provides a compact and geometrically interesting characterization of threshold functions.',117351,'IEEE Transactions on Computers',1),(1013295,NULL,'1981',NULL,'Delay Analysis of Broadcast Routing in Packet-Switching Networks','Broadcast addressing is the capability to send a packet from a source node to all other nodes in the network. Store-and-forward, packet-switching networks are not inherently designed to carry broadcast packets, and broadcasting has to be implemented by some sort of routing algorithm. In this paper, the source based forwarding algorithm is considered. With this algorithm, a spanning tree is defined for each node, and broadcast packets are sent along the branches of these trees. Approximation methods are presented to obtain a lower bound and estimates of the mean broadcast time. The accuracy of these methods is evaluated by comparison with simulation.',117351,'IEEE Transactions on Computers',7),(1013432,NULL,'1977',NULL,'Synthesis of Multiple-Valued Logic Networks Based on Tree-Type Universal Logic Module','This correspondence presents a theoretical study on the synthesis of multiple-valued logic networks based on tree-type universal logic modules (T-ULM\'s). The mathematical notation of T-ULM is introduced. On the basis of the mathematical properties, an algorithm for synthesizing an arbitrary logic function of n variables with a smaller number of modules is presented. In this algorithm, only true and constant inputs are allowed at input terminals. The algorithm consists of two parts. The first one is a functional decomposition, and the other one is the proper order of the expansion which is the problem of zinding the most incomplete tree structure. Thus it is established that the systematic and nonexhaustive procedure of the algorithm gives a suboptimal solution for the reduction of the number of T-ULM\'s.',117351,'IEEE Transactions on Computers',1),(1034658,NULL,'2008',NULL,'Minimum Deadline Calculation for Periodic Real-Time Tasks in Dynamic Priority Systems','Real-time systems are often designed using a set of periodic tasks. Task periods are usually set by the system requirements, but deadlines and computation times can be modified in order to improve system performance. Sensitivity analysis in real-time systems has focused on changes in task computation times, using fixed priority analysis. Only a few studies deal with the modification of deadlines in dynamic-priority scheduling. The aim of this work is to provide a sensitivity analysis for task deadlines in the context of dynamic-priority, pre-emptive, uniprocessor scheduling. In this paper, we present a deadline minimisation method that computes the shortest deadline of a periodic task. As undertaken in other studies concerning computation times, we also define and calculate the critical scaling factor for task deadlines. Our proposal is evaluated and compared with other works. The deadline minimisation proposed strongly reduces jitter and response time of control tasks, which can lead to a significant improvement of the system performance.',117351,'IEEE Transactions on Computers',0),(1080852,NULL,'1988',NULL,'A fast planar partition algorithm. I','A fast randomized algorithm is given for finding a partition of the plane induced by a given set of linear segments. The algorithm is ideally suited for a practical use because it is extremely simple and robust, as well as optimal; its expected running time is O(m+n log n) where n is the number of input segments and m is the number of points of intersection. The storage requirement is O(m+n). Though the algorithm itself is simple, the global evolution of the partition is complex, which makes the analysis of the algorithm theoretically interesting in its own right.',215353,'SFCS \'88 Proceedings of the 29th Annual Symposium on Foundations of Computer Science',9),(1115241,NULL,'1968',NULL,'Optimal synthesis of arbitrary switching functions with regular arrays of 2-input, 1-output switching elements','Not all switching functions are realizable by a single cascade of 2- input, 1-output switching elements, even if repeated inputs are allowed. However, arrays of such cascades feeding a single collector cascade of AND or OR cells can be used to synthesize any function. This paper is concerned with optimal array realizations of this form. A procedure is given which allows one to generate rather efficiently all prime cascade realizable functions which imply a given switching function. These prime functions may be restricted to those realizable by a single cascade either with or without repeated inputs. To solve problems of optimal single output array synthesis, allowing incompletely specified functions, it is necessary only to select a minimum cost cover from the set of prime cascade realizable functions. Each function used in the cover may be realized very easily, and the resulting cascades supply the inputs to the collector cascade. Extensions are made to the simultaneous realization of several switching functions, each the output of a separate collector cascade fed from the same array.',228172,'SWAT \'68 Proceedings of the 9th Annual Symposium on Switching and Automata Theory (swat 1968)',1),(1115242,NULL,'1968',NULL,'A transform approach to logic design','This paper describes a new approach to the design of combinational logic using large-scale-integrated (LSI) circuit technology. A simple \"prototype\" logic function of n binary variables is imbedded within an array of at most (n+1) rows and columns. The cells of this array contain 2-input exclusive-OR gates, and its rows are fed by the input variables and logical \"1\". Its column outputs are first degree polynomial functions of the input variables. These functions supply inputs to, and modify the output of, the prototype in order to realize the desired function. These transformations form a group, specifically, the largest subgroup of the (n+1)- dimensional affine group such that input variable encodings are not affected by feedback from the function\'s output.',228172,'SWAT \'68 Proceedings of the 9th Annual Symposium on Switching and Automata Theory (swat 1968)',1),(1120937,NULL,'1971',NULL,'Universal base functions and modules for realizing arbitrary switching functions','Universal base functions (UBF\'s) are defined which are generalizations of universal logic functions. An (n, m, r)-UBF can be implemented as a single module (UBM) with n+m inputs and 1 output. An arbitrary n-variable switching function fn (X) is then realized on the fixed UBM by realizing a suitable set of m r-variable functions with which to drive m inputs of the UBM, the remaining n inputs being driven by X. The fan-in of a UBM for r ≥ 2 is shown to be considerably less than that of a Universal Logic Module (a special case corresponding to r = 1). Specific UBF\'s are proposed for r = 2 in which m is on the order of 60% of the value obtained by using the UBF defined by the familiar Shannon decomposition formula. This is close to the theoretical lower bound on m. The use of UBM\'s provides a new way to realize an arbitrary function or set of functions, completely specified or otherwise, by assembling a small number of circuits selected from a small set of standard logic modules of limited fan-in. For the case r = 2, the number of 2-input devices required to drive the m inputs of the UBM is often considerably less than m. DON\'T CARES can be used to advantage to reduce this number still further. Realizations based on a UBM (n=6, m=12, r=2) were computed for over 1000 randomly generated completely specified functions grouped, exclusively, into sets of 1,2,3 and 4 output functions. Most (86%) single output functions required no more than 8 2-input devices to drive the 12 control inputs of the UBM, while most (79%) simultaneous realizations of sets of 4 functions required an average of no more than 6 such devices per output function. More detailed results are provided. Array type realizations of UBM\'s are proposed such that each can itself be built from multiple copies of a rather limited number of submodule types, each with n+2 inputs.',228175,'SWAT \'71 Proceedings of the 12th Annual Symposium on Switching and Automata Theory (swat 1971)',1),(1130785,NULL,'1958',NULL,'Design criteria for autosynchronous circuits','Synopsis: The circuits and organization of present computers are such that possible operating speeds are lower than the capabilities of the components. The speed limitations of such synchronous computers will be described, and design criteria for higher speed operation set forth. Examples will be given for a logic and circuit organization which results in both faster operation and improved performance to cost ratios. In particular, circuits which are free of transient logical malfunctions, sometimes called \"spikes,\" will be developed and a typical autosynchronous system will be shown.',22103,'AIEE-ACM-IRE \'58 (Eastern) Papers and discussions presented at the December 3-5, 1958, eastern joint computer conference: Modern computers: objectives, designs, applications',10),(1133613,NULL,'1962',NULL,'D825 - a multiple-computer system for command & control','The D825 Modular Data Processing System is the result of a Burroughs study, initiated several years ago, of the data processing requirements for command and control systems. The D825 has been developed for operation in the military environment. The initial system,. constructed for the Naval Research Laboratory with the designation AN/GYK-3(V), has been completed and tested. This paper reviews the design criteria analysis and design rationale that led to the system structure of the D825. The implementation and operation of the system are also described. Of particular interest is the role that developed for an operating system program in coordinating the system components.',21388,'AFIPS \'62 (Fall) Proceedings of the December 4-6, 1962, fall joint computer conference',5),(1135847,NULL,'1966',NULL,'Parameter optimization by random search using hybrid computer techniques','Optimum selection of the parameter values for a complex dynamic system usually consists of three distinct phases: (1) a proposed system configuration is selected, in which only parameter values remain as unknowns; (2) one or more performance or cost criteria for evaluation of the system are selected; and (3) a computer technique or algorithm is chosen for adjusting the system parameters until an optimum value of the criterion function is achieved. Typical algorithms are those based on relaxation or steep descent methods. However, both of these methods are primarily suited to optimization of criterion functions with unique minima or maxima. Furthermore, they may fail to converge or may converge only very slowly if the criterion function---parameter space exhibits \"ridges\" or if the criterion function is only piecewise differentiable or piecewise continuous. Both of these difficulties are likely to arise in connection with nonlinear systems. This paper presents an approach to finding a global optimum by means of a modified sequential random perturbation technique implemented on a hybrid computer.',21396,'AFIPS \'66 (Fall) Proceedings of the November 7-10, 1966, fall joint computer conference',2),(1135852,NULL,'1966',NULL,'Automated logic design techniques applicable to integrated circuitry technology','Rapidly advancing integrated circuit technology has placed many new and often unforeseen demands on logic packaging techniques and, hence, is also impacting traditional computer design concepts. For instance, one of the most pertinent and immediate requirements is the optimum utilization of input/output (I/O) connections since the package size is strongly dependent on such connections. The package efficiency is measured in part by the I/O pin-to-circuit ratio, assuming the circuits in a package are connected in a way to provide an optimum logic function. Another potential problem to be considered is power dissipation, since integrated circuits may be contained in extremely small areas.',21396,'AFIPS \'66 (Fall) Proceedings of the November 7-10, 1966, fall joint computer conference',10),(1135857,NULL,'1966',NULL,'A high-speed integrated circuit scratchpad memory','Computer systems are presently being designed and fabricated using one- to two-nanosecond current-mode logic gates. High-speed scratchpad memories are required in order to utilize this circuit speed effectively.',21396,'AFIPS \'66 (Fall) Proceedings of the November 7-10, 1966, fall joint computer conference',10),(1136999,NULL,'1967',NULL,'System architecture for large-scale integration','The developing capability of the semiconductor industry to fabricate and interconnect a hundred or more logic gates on a single silicon chip promises to have a substantial impact upon the performance and reliability of today\'s computers. In just a decade, computer fabrication techniques have progressed from a single vacuum tube gate occupying many cubic inches in volume, to second generation discrete transistor circuitry, and to integrated circuit flat-packs in the third generation machines. Each successive generation has offered more computing power through faster circuitry and increased packing densities. Approximately 99% of the volume, even in densely packaged third generation computers, represents packaging and circuit interconnection material, and this separation between computer components still represents a severe speed bottleneck. It is not uncommon for 75% of the machine delay to occur in interconnection wiring with only 25% of the delay inherent in the flat-packs. Large-scale integration of logic gates on a single silicon chip offers promise of breaking this speed bottleneck in the larger and faster fourth generation machines.',21398,'AFIPS \'67 (Fall) Proceedings of the November 14-16, 1967, fall joint computer conference',10),(1147064,NULL,'1968',NULL,'Program composition and editing with an on-line display','An Interactive Programming Support System (IPSS) has been under development at System Development Corporation since 1965. The purpose of the system is to permit all of the programming processes---composition (in a procedure-oriented language), editing, execution, testing, and documentation---to be carried out as parts of a single, coordinated activity centered around an interactive compiler. IPSS attempts to unify techniques that are usually embodied in separate functional programs, so that the programmer need not know which particular program is performing a specific task. The system is intended for a time-sharing environment, with user interaction via a small tabular display or typewriter-like terminal.',21401,'AFIPS \'68 (Fall, part II) Proceedings of the December 9-11, 1968, fall joint computer conference, part II',5),(1147219,NULL,'1970',NULL,'A study of user-microprogrammable computers','The user microprogrammable computer as the fourth generation computer is investigated from the user\'s point of view. In the first part of the paper microprogramming and its concept as well as the problems and requirements incurring its use in various applications are discussed. The current status of the microprogrammed computer is also studied to indicate the differences of philosophy in microprogramming. A number of suggestions are made for the design of fourth generation user-microprogrammable computers.',21406,'AFIPS \'70 (Spring) Proceedings of the May 5-7, 1970, spring joint computer conference',5),(1148507,NULL,'1970',NULL,'A method of test generation for fault location in combinational logic','The Path Generating Method is a simple procedure to obtain, from a directed graph, an irredundant set of paths that is sufficient to detect and isolate all distinguishable failures. It was developed as a tool for diagnostic generation at the system level, e.g., to test data paths and register loading and to test a sequence of transfer instructions. But it has been found to be a powerful tool for test generation for combinational logic networks as well.',21405,'AFIPS \'70 (Fall) Proceedings of the November 17-19, 1970, fall joint computer conference',11),(1148814,NULL,'1971',NULL,'The design of a meta-system','The design and implementation of multiprogrammed, time-sharing computer systems continues long after the system is put to use. A tool is needed that will measure and evaluate the computer system while it is in operation, as an aid to further development or optimization for a particular usage. Research into the possibility of developing this tool was undertaken at the University of Pennsylvania\'s Moore School of Electrical Engineering. The research led to the design of the tool, which is presented in this report. It is called the Meta-system.',21408,'AFIPS \'71 (Spring) Proceedings of the May 18-20, 1971, spring joint computer conference',5),(1148817,NULL,'1971',NULL,'Coding techniques for failure recovery in a distributive modular memory organization','This paper considers various coding techniques which could be applied to a memory organization to achieve detection and correction of failures. In this discussion, we define a fault as a malfunction of a systems component and a failure as a manifestation of a fault. Notice that a single fault can result in multiple failures. Thus, techniques such as error-detecting and correction codes, when used alone, are limited in that they operate on failures---not faults.',21408,'AFIPS \'71 (Spring) Proceedings of the May 18-20, 1971, spring joint computer conference',6),(1148977,NULL,'1972',NULL,'A universal cyclic division circuit','Recent innovations in circuit technology have allowed design alternatives that previously would have been economically unsound. LSI technology permits the use of generalized systems containing more logic than the specialized systems used in the past, implemented in unit logic, and at even lower cost. Five years ago an engineer would not have even considered using a cyclic redundancy checking circuit in the manner described here.',21407,'AFIPS \'71 (Fall) Proceedings of the November 16-18, 1971, fall joint computer conference',10),(1149013,NULL,'1972',NULL,'The ECL programming system','ECL is a programming language system currently being implemented as a research project at Harvard University. Its goal is an environment which will significantly facilitate the production of programs. In this paper, we describe the motivation for this project, present the approach taken in its design, and sketch the resulting ECL system. Detailed treatment of specific aspects of the system are found elsewhere.',21407,'AFIPS \'71 (Fall) Proceedings of the November 16-18, 1971, fall joint computer conference',5),(1167962,NULL,'1980',NULL,'UCSD Pascal™: a portable software environment for small computers','The UCSD Pascal System is a complete program development and execution environment for small computers. Its facilities include text editors and file management utilities, as well as compilers (for Pascal, in particular), assemblers, and a linkage editor. The system is highly portable; versions have been implemented on almost twenty minicomputers and microprocessors. (A concise description of system facilities is provided in an appendix.)',21417,'AFIPS \'80 Proceedings of the May 19-22, 1980, national computer conference',5),(1272407,NULL,'1971',NULL,'Mathematical models for automatic line detection','The goal of the work here reported is to Investigate algorithms which find the real straight edges In scenes consisting of prismatic solids, taking Into account smooth variations In Intensity over faces, blurring of edges, and noise. To this end we give In thts paper a model of the appearance to an optical Input device of such scenes; and describe a sub-optimal statistical decision procedure, based on the model, for the Identification of a line within a narrow band on the field of view, given an array of Intensities from within the band.',117705,'IJCAI\'71 Proceedings of the 2nd international joint conference on Artificial intelligence',2),(1272414,NULL,'1971',NULL,'Recognition of polyhedrons with a range finder','A recognition procedure with a range finder presented in this paper is developed for the eye of the intelligent robot studied in the Electrotechnical Laboratory. The range finder employs a vertical slit projecter which projects a. light beam on the objects. While the beam is moved in a field of view, the picture at each instant is picked up by a TV camera. The distance to each point can be obtained by means of trigonometrical calculation. The recognition procedure using thus obtained in formation is free from the effects of the arrangement and shadow of objects.',117705,'IJCAI\'71 Proceedings of the 2nd international joint conference on Artificial intelligence',2),(1272415,NULL,'1971',NULL,'Aspects of the detection of scene congruence','The problem of matching two scenes, one contained in the other, arises in many practical picture processing tasks including stereocompilation, classification of photographic data, and map matching for navigation and guidance. Because the images are not exact replicas, but rather noisy and perhaps geometrically distorted versions of the original scenes, it is necessary to develop a body of theory capable of providing answers to questions concerning effects of various types of error, and means for minimizing the effects of such errors. This paper presents a \"decision space\" approach to the problem for dealing with small amounts of noise and distortion, and a promising new technique for dealing with \"rubber sheet\" distortion.',117705,'IJCAI\'71 Proceedings of the 2nd international joint conference on Artificial intelligence',2),(1272798,NULL,'1981',NULL,'Depth from edge and intensity based stereo','The past few years have seen a growing interest in the application\" of three-dimensional image processing. With the increasing demand for 3-D spatial information for tasks of passive navigation [7,12], automatic surveillance [9], aerial cartography [10,13], and inspection in industrial automation, the importance of effective stereo analysis has been made quite clear. A particular challenge is to provide reliable and accurate depth data for input to object or terrain modelling systems (such as [5]. This paper describes an algorithm for such stereo sensing It uses an edge-based line-by-line stereo correlation scheme, and appears to be fast, robust, and parallel implementable. The processing consists of extracting edge descriptions for a stereo pair of images, linking these edges to their nearest neighbors to obtain the edge connectivity structure, correlating the edge descriptions on the basis of local edge properties, then cooperatively removmg those edge correspondences determined to be in error - those which violate the connectivity structure of the two images. A further correlation process, using a technique similar to that used for the edges, is applied to the image intensity values over intervals defined by the previous correlation The result of the processing is a full image array disparity map of the scene viewed.',117713,'IJCAI\'81 Proceedings of the 7th international joint conference on Artificial intelligence - Volume 2',2),(1274106,NULL,'1969',NULL,'The Stanford hand-eye project','There is a large continuing project at Stanford Artificial Intelligence Laboratory aimed towards the development of a system capable of interesting perceptual-motor behavior. This paper presents a brief outline of the currently active efforts and suggests references for more detailed information. A more thorough discussion of the effort to organize a visual perception system is presented.',117704,'IJCAI\'69 Proceedings of the 1st international joint conference on Artificial intelligence',5),(1274299,NULL,'1973',NULL,'A parser for a speech understanding system','This paper describes a parsing system specifically designed for spoken rather than written input. The parser is part of a project in progress at Stanford Research Institute to develop a computer system for understanding speech. The approach described uses as much heuristic knowledge as possible in order to minimize the demands on acoustic analysis.',117706,'IJCAI\'73 Proceedings of the 3rd international joint conference on Artificial intelligence',5),(1274341,NULL,'1973',NULL,'Detection of homogeneous regions by structural analysis','A method is proposed for partitioning input scenes into regions in which selected picture properties are regarded as uniform. In this procedure, usual statistical methods of inspecting average gray levels are not used, but the uniformity of complex picture properties such as shapes, sizes or arrangements of subpatterns in the pictures is examined by a structural analysis. The analyzing process is controlled by a supervisor which gives the order of changing the depth of interest in each picture property, and successive partitioning of the pictures is made toward an interpretable goal partition.',117706,'IJCAI\'73 Proceedings of the 3rd international joint conference on Artificial intelligence',2),(1286547,NULL,'1976',NULL,'On Magnetic Bubble Logic Circuits','This paper is concerned with the realization of logic functions by using two-input magnetic bubble logic elements. A magnetic bubble logic element is the multiple-output logic element whose number of ``1\'\' \'s of the output is equal to that of corresponding input, and fanout of each output terminal of the element is restricted to one. In order to realize some functions, it is necessary to use the generators which correspond to constant-supplying elements. First, the number of generators which are necessary and sufficient to realize an arbitrary functions is obtained for a given set of elements. In particular, it is shown that an arbitrary function can be realized by using IB elements and at most two generators. Since the IB element is a universal element in the above sense and is considered to be rather easily realized by magnetic bubble interactions, the IB logic circuits are mainly discussed. The IB minimum circuit defined here is a circuit which consists of minimum number of generators and minimum number of IB elements. In the last half of this paper, it is shown that the minimum circuits of most functions have the characteristic circuit structure called ``1-4 form.\'\'',117351,'IEEE Transactions on Computers',1),(1286598,NULL,'1973',NULL,'Arithmetic Algorithms for Error-Coded Operands','A set of arithmetic algorithms is described for operands that are encoded in the ``AN\'\' error-detecting code with the low-cost check modulus A = 2a - 1. The set includes addition additive inverse (complementation), multiplication, division, roundoff, and two auxiliary algorithms: ``multiply by 2a - 1,\'\' and ``divide by 2a - 1.\'\' The design of a serial radix-16 processor is presented in which these algorithms are implemented for the low-cost AN code with A = 15. This processor has been constructed for the Jet Propulsion Laboratory STAR computer. The adaptation of ``two\'s complement\'\' arithmetic for an inverse-residue code is also described.',117351,'IEEE Transactions on Computers',8),(1286607,NULL,'1973',NULL,'A Note on Conditional-Sum Addition for Base - 2 Systems','Conditional-sum addition in a -2 base system and its comparison with normal binary conditional-sum addition is discussed. It is found that approximately 2.0 to 2.5 times as much hardware is required for this high-speed addition method in the negative binary system as compared to the positive binary system.',117351,'IEEE Transactions on Computers',8),(1286618,NULL,'1973',NULL,'The Analysis of Radiographic Images','The problem considered is the development of a method of scene analysis that employs a descriptive approach in the analysis of pictures. In an effort to facilitate implementation of the algorithm to a wide range of picture classes, a description of the scene is read in as data in the form of a tree structure that guides the search for objects from the largest to the smallest. The algorithm has the following features: the analysis of the scene is top-down; feature extraction and pattern recognition are combined in a reinforcing system; the algorithm contains a description of the class of scenes to be analyzed; region enumeration techniques are used in primitive object identification; principal parameters in the program are self-adjusting; and the concept of ``field of vision\'\' is used to locate the boundaries of objects. The algorithm has been implemented and tested on posteroanterior (PA) chest radiograms, anteroposterior (AP) knee radiograms, and lateral brain scans. The analysis of PA chest radiograms is used to describe the implementation of the program. To date, the algorithm has been tested only on radiographic images; however, the method should also be applicable to other image classes.',117351,'IEEE Transactions on Computers',2),(1286636,NULL,'1972',NULL,'A Segmentation Technique for Waveform Classification','This note describes the determination of waveform segments which contain the information necessary for classification. The method is successful in discriminating between the vibration record of internal combustion engines before and after repair.',117351,'IEEE Transactions on Computers',2),(1286637,NULL,'1972',NULL,'A Clustering Heuristic for Line-Drawing Analysis','Certain arrangements of local features in a scene tend to group together and to be seen as units. It is suggested that, in some instances, this phenomenon might be interpretable as a process of cluster detection in a graph-structured space derived from the scene. This idea is illustrated using a class of line-drawing ``scenes\'\' that contain only horizontal and vertical line segments.',117351,'IEEE Transactions on Computers',2),(1286641,NULL,'1972',NULL,'An Iterative Array for Multiplication of Signed Binary Numbers','A simple method for the implementation of Booth\'s algorithm for multiplication of signed binary numbers has been presented. It has been shown that for large word lengths, a significant economy has been achieved compared to Majithia and Kitai\'s method.',117351,'IEEE Transactions on Computers',8),(1289415,NULL,'2005',NULL,'Parameterized compilability','Compilability is a measure of how effectively compilation (or preprocessing) can be applied to knowledge bases specified in a particular knowledge representation formalism; the aim of compilation is to allow for efficient, on-line query processing. A theory of compilability has been established for organizing knowledge representation formalisms according to a scheme of \"compilability classes\", and bears strong analogies to the classical theory of complexity, which permits the organization of computational problems according to complexity classes. We develop a novel theory of compilability, called parameterized compilability, which incorporates the notion of parameterization as used in parameterized complexity and permits for refined analysis of compilability.',117697,'IJCAI\'05 Proceedings of the 19th international joint conference on Artificial intelligence',2),(1305147,NULL,'1960',NULL,'Toward mechanical mathematics','Results are reported here of a rather successful attempt at proving all theorems, totalling near 400, of Principia Mathematica which are strictly in the realm of logic, viz., the restricted predicate calculus with equality. A number of other problems of the same type are discussed. It is suggested that the time is ripe for a new branch of applied logic which may be called \"inferential\" analysis, which treats proofs as numerical analysis does calculations. This discipline seems capable, in the not too remote future, of leading to machine proofs of difficult new theorems. An easier preparatory task is to use machines to formalize proofs of known theorems. This line of work may also lead to mechanical checks of new mathematical results comparable to the debugging of a program.',114531,'IBM Journal of Research and Development',2),(1305736,NULL,'1959',NULL,'Some studies in machine learning using the game of checkers','Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.',114531,'IBM Journal of Research and Development',2),(1305794,NULL,'1962',NULL,'Minimization over Boolean graphs','This paper presents a systematic procedure for thed esign of gate-type combinational switching circuits without diredted loops. Each such circuit (Boolean graph) is in correspondence with a sequence of decompositions of the Boolean function which it realizes. A general approach to functional decomposition i s given and, in terms oaf convenient positional representation, efficient tests for the detection of decompositions are derived. These results are employed in the development of an alphabetic search procedure for determining minimum-cost Boolean graphs which satisfy any given design specifications.',114531,'IBM Journal of Research and Development',1),(1306114,NULL,'1964',NULL,'Solid logic technology: versatile, high-performance microelectronics','A new microelectronics packaging technique, called Solid logic Technology (SLT), utilizes silicon planar glass-encapsulated transistors and diodes, and graphic arts techniques for producing high-quality, passive components having tight tolerances. The result is a process permitting the low-cost realization of a variety of versatile, high-performance circuit modules. The salient features of SLT are described: the unique form of the semiconductor devices, the module fabrication process, and some performance results. In addition, insight is provided to the range of components that may be fabricated with this technology, i.e., inductors, capacitors and high-power transistors. Examples are shown of specific high-speed, high-density and complex circuit packages.',114531,'IBM Journal of Research and Development',10),(1306424,NULL,'1967',NULL,'Some studies in machine learning using the game of checkers. II: recent progress','A new signature tablet echnique is described together with an improved book learning procedure which is thougthot be much superior to the linear polynomial method described earlier. Full use is made of the so called \"alpha-beta\" pruning and several forms of forward pruning to restrict the spread of the move tree and to permit the programt o look ahead to a much greater depth than it otherwise could do. While still unable to outplay checker masters, the program\'s playing ability has been greatly improved.',114531,'IBM Journal of Research and Development',2),(1307017,NULL,'1969',NULL,'Simulating operating systems','A simulator is discussed that provides a language and a structure specifically designed for modeling computer systems to evaluate their performance. The simulator prorides general equipment models, and the authors discuss their experience in developing a general submodel of a multiprogramming operating system. The user assembles a system from the equipment models, specifies parameters to allow simulation of his operating system functions, and provides models of his application programs.',114750,'IBM Systems Journal',5),(1307144,NULL,'1969',NULL,'Associative holographic memories','Recently Longuet-Higgins modeled a temporal analogue of the property of holograms that allows a complete image to be constructed from only a portion of the hologram. In the present paper a more general analogue is discussed and two two-step transformations that imitate the recording-reconstruction sequence in holography are presented. The first transformation models the recall of an entire sequence from a fragment while the second is more like human memory in that it provides recall of only the part of the sequence that follows the keying fragment. Both models require only the three operations: shift, multiplication and addition.',114531,'IBM Journal of Research and Development',2),(1307882,NULL,'1975',NULL,'High-speed dynamic programmable logic array chip','This paper describes the circuit design of a programmable logic array chip using four-phase dynamic circuits, operating at a nominal cycle time of 230 nanoseconds. Bootstrap circuit techniques are used to obtain high function and performance by satisfying some special requirements of PLA designs. These include a simple means for two-bit partitioning of the data inputs, a noninverting buffer circuit between precharged arrays, and a fast, compact on-chip driver for heavily loaded arrays. Multiphase clocking enables the use of master/slave type JK flip-flops with minimum circuitry and power dissipation. A polarity hold function is provided at the outputs to allow interfacing the dynamic design to static output circuits.',114531,'IBM Journal of Research and Development',10),(1308231,NULL,'1981',NULL,'The evolution of the MVS operating system','The mechanization of computer operations and the extension of hardware functions are seen as the basic purposes of an operating system. An operating system must fulfill those purposes while providing stability and continuity to its users. Starting with the data processing environment of twenty-five years ago, this paper describes the forces that led to the development of the OS/360 system design and then traces the evolution which led to today\'s MVS system.',114531,'IBM Journal of Research and Development',5),(1308275,NULL,'1982',NULL,'Model for transient and permanent error-detection and fault-isolation coverage','As computer technologies advance to achieve higher performance and density, intermittent failures become more dominant than solid failures, with the result that the effectiveness of any diagnostic procedure which relies on reproducing failures is greatly reduced. This problem is solved at the system level by a new strategy of dynamic error detection and fault isolation based on error checking and analysis of captured information. The model developed in this paper allows the system designer to project the dynamic error-detection and fault-isolation coverages of the system as a function of the failure rates of components and the types and placement of error checkers, which has resulted in significant improvements to both detection and isolation in the IBM 3081 Processor Unit. The model has also resulted in new probabilistic isolation strategies based on the likelihood of failures. Our experiences with this model on several IBM products, including the 3081, show good correlation between the model and practical experiments.',114531,'IBM Journal of Research and Development',6),(1308367,NULL,'1983',NULL,'Random-pattern coverage enhancement and diagnosis for LSSD logic self-test','Embedded linear feedback shift registers can be used for logic component self-test. The issue of test coverage is addressed by circuit modification, where necessary, of random-pattern-resistant fault nodes. Also given is a procedure that supports net-level diagnosis for structured logic in the presence of random test-pattern generation and signature analysis.',114531,'IBM Journal of Research and Development',11),(1316973,NULL,'1962',NULL,'Synthesis of combinational logic using three-input majority gates','This paper examines the problem of synthesizing a switching function using only majority gates. It is assumed that the switching function is given in truth-table form where any \"don\'t care\" input combinations have simply been omitted from the table. First, a theorem is derived which indicates those complemented variables and constants which must be added as additional columns of the truth-table in order that the function be realizable with only majority gates. Likewise, the same theorem permits unnecessary input variables to be eliminated. Next, the table is \"unitized\" by complementing those rows where F is to be O. Finally, a reduction theorem often permits many of the rows of this \"unitized\" table to be eliminated. Various synthesis procedures are described based on this reduced table. Two \"canonical\" realizations are shown which follow immediately from the table. A more comprehensive procedure for three-input gates is then described. Extensions of the methods to majority gates with more than three inputs and to multiple-output functions are discussed briefly. Examples are included.',98777,'FOCS \'62 Proceedings of the 3rd Annual Symposium on Switching Circuit Theory and Logical Design (SWCT 1962)',1),(1316999,NULL,'1967',NULL,'A cellular structure for sequential networks','A cellular structure (\"transfer logic\") suitable for implementing sequential networks, is described in this paper. A method is given for obtaining, for a given flow table, a transfer logic assignment employing the minimal number of state variables. The method is based on the properties of partitions of the set of internal states.',98779,'FOCS \'67 Proceedings of the 8th Annual Symposium on Switching and Automata Theory (SWAT 1967)',1),(1373608,NULL,'1974',NULL,'Approximation algorithms for combinatorial problems','Simple, polynomial-time, heuristic algorithms for finding approximate solutions to various polynomial complete optimization problems are analyzed with respect to their worst case behavior, measured by the ratio of the worst solution value that can be chosen by the algorithm to the optimal value. For certain problems, such as a simple form of the kanpsack problem and an optimization problem based on satisfiability testing, there are algorithms for which this ratio is bounded by a constant, independent of the problem size. For a number of set covering problems, simple algorithms yield worst case ratios which can grow with the log of the problem size. And for the problem of finding the maximum clique in a graph, no algorithm has been found for which the ratio does not grow at least as fast as n^@e, where n is the problem size and @e0 depends on the algorithm.',135053,'Journal of Computer and System Sciences',9),(1373609,NULL,'1974',NULL,'An observation on time-storage trade off','There are two main results proved here. The first states that a certain set SP of strings (those coding \'\'solvable path systems\'\') has tape complexity (log n)^2 iff every set in (i.e., of deterministic polynomial time complexity) has tape complexity (log n)^2. The second result gives evidence that SP does not have tape complexity (log n)^k for any k.',135053,'Journal of Computer and System Sciences',9),(1502033,NULL,'1996',NULL,'Connectivity and sparse wavelength conversion in wavelength-routing networks','We study the effects of topological connectivity and wavelength conversion in circuit-switched all-optical/ wavelength-routing networks. An approximate blocking analysis of such networks is performed. We first propose an improved framework for the analysis of networks with arbitrary topology. We introduce a simple model for networks with a variable number of converters and analyze the effect of wavelength converter density on blocking probability. We then apply this framework to two sparse network topologies, the ring and the mesh-torus, and obtain the blocking performance. The results show that, in most cases, only a fraction of the network nodes need to be equipped with wavelength conversion capability for good performance. Finally, the tradeoff between physical connectivity, wavelength conversion, and the number of available wavelengths is studied through networks with random topologies.',121078,'INFOCOM\'96 Proceedings of the Fifteenth annual joint conference of the IEEE computer and communications societies conference on The conference on computer communications - Volume 1',7),(1512875,NULL,'1984',NULL,'Cascading Transmission Gates to Enhance Multiplier Performance','A high-speed Wallace-tree type combinational multiplier chip has been fabricated as a technology demonstration device using 1.5 驴m NMOS processing. Multiply rates well in excess of 40 mHz have been obtained from laboratory devices. Extensive use of cascaded MOS transmission-gate (or steering) logic resulted in worthwhile improvements in power, average gate delay, and device topology.',117351,'IEEE Transactions on Computers',10),(1512888,NULL,'1984',NULL,'A New PLA Design for Universal Testability','A new design of universally testable PLA\'s is presented in which all multiple faults can be detected by a universal test set which is independent of the function being realized by the PLA. The proposed design has the following properties. 1) It can be tested with function-independent test patterns; hence, no test pattern generation is required. 2) The amount of extra hardware is significantly decreased compared to the previous designs of universally testable PLA\'s. 3) Very high fault coverage is achieved, i. e., all single and multiple stuck faults, crosspoint faults, and adjacent line bridging faults are detected. 4) It is appropriate for built-in testing approaches. 5) It can be applied to the high-density PLA\'s using array folding techniques.',117351,'IEEE Transactions on Computers',11),(1512895,NULL,'1984',NULL,'A Classification of Cube-Connected Networks with a Simple Control Scheme','The correspondence presents three classes of cube-connected networks with individual stage control based on a group theoretic modeling of interconnection networks. It is shown that these classes of networks retain nonisomorphic groups of permutations. Although the permutations realizable by such networks are rather limited in number, the simplicity of their control scheme makes them attractive. Moreover, the interconnection power of these networks can be enhanced by simulating the networks which belong to one class by any member of that class.',117351,'IEEE Transactions on Computers',7),(1546453,NULL,'2000',NULL,'Early-release fair scheduling','We present a variant of Pfair scheduling, which we call early-release fair (ERfair) scheduling. Like conventional Pfair scheduling, ERfair scheduling algorithms can be applied to optimally schedule periodic tasks on a multiprocessor system in polynomial time. However, ERfair scheduling differs from Pfair scheduling in that it is work conserving. As a result, average job response times may be much lower under ERfair scheduling than under Pfair scheduling, particularly in lightly-loaded systems. In addition, runtime costs are lower under ERfair scheduling.',90883,'Euromicro-RTS\'00 Proceedings of the 12th Euromicro conference on Real-time systems',0),(1720760,NULL,'2005',NULL,'Opportunistic data dissemination in mobile peer-to-peer networks','In this paper we examine the dissemination of availability reports about resources in mobile peer-to-peer networks, where moving objects communicate with each other via short-range wireless transmission. Each disseminated report represents an observed spatial-temporal event, and the relevance of the report to a moving object decays as the age of the reported resource and the distance from its location increase. We propose an opportunistic approach, in which an object propagates the reports it carries (namely the information that it has about these resources) to encountered objects and obtains new reports in exchange. Least relevant reports are discarded after each exchange so as to limit the communication data volume of future exchanges. Our theoretical and experimental analysis indicates that the opportunistic dissemination algorithm automatically limits the global distribution of a report to a bounded spatial area and to the duration for which it is of interest. We propose two variants of the opportunistic dissemination algorithm and compare them with the traditional client-server architecture in terms of data accuracy. The proposed system has the potential to create a completely new information marketplace.',223615,'SSTD\'05 Proceedings of the 9th international conference on Advances in Spatial and Temporal Databases',3),(1784280,NULL,'1998',NULL,'Identification of incompletely specified multiple-valued Kleenean functions','This paper focuses on incompletely specified multiple-valued Kleenean functions (1991). It is easy to verify that they do not have functional completeness in the class of all functions on the unit interval. Therefore, not all incompletely specified functions on the unit interval are incompletely specified multiple-valued Kleenean functions. In this paper, we will clarify a necessary and sufficient condition for an incompletely specified function to be an incompletely specified multiple-valued Kleenean function. Further, we show an algorithm which derives one of the logic formulas representing the incompletely specified multiple-valued Kleenean function. In considering the application of multiple-valued Kleenean functions, we will show an example which suggests the possibility that input-output data can be described abstractly in terms of multiple-valued Kleenean functions',117446,'IEEE Transactions on Systems, Man, and Cybernetics, Part A: Systems and Humans',1),(1808917,NULL,'2006',NULL,'A new paradigm for public key identification','The present paper investigates the possibility of designing zero-knowledge identification schemes based on hard problems from coding theory. Zero-knowledge proofs were introduced by Goldwasser, Micali, and Rackoff (1985). Their practical significance was soon demonstrated in the work of Fiat and Shamir [1986], who turned zero-knowledge proofs of quadratic residuosity into efficient means of establishing user identities. In the present paper, we propose a new identification scheme, based on error-correcting codes, which is zero-knowledge and seems of practical value. Furthermore, we describe several variants, including one which has an identity-based character. The security of our schemes depends on the hardness of finding a word of given syndrome and prescribed (small) weight with respect to some randomly generated binary linear error-correcting code. This is, of course, not the first attempt to design a cryptographic scheme using tools from coding theory. The difference is that identification protocols do not follow the public key paradigm based on trap-door functions and described in the seminal Diffie-Hellman paper [1976]. Rather, they only require one-way functions, which opens the way to using, in a rather direct manner, simple combinatorial problems of the kind provided by coding theory. The resulting schemes compare favorably to their number-theoretic analogs',117389,'IEEE Transactions on Information Theory - Part 1',2),(1843320,NULL,'1984',NULL,'The fuzzy geometry of image subsets','In pattern recognition we often want to measure geometric properties of regions in an image, but these regions are not always \'crisply\' defined; it is sometimes more appropriate to regard them as fuzzy subsets of the image. Many of the basic geometric properties of and relationships among regions can be generalized to fuzzy sets; these include connectedness, adjacency and surroundedness, starshapedness and convexity, area and perimeter, extent and diameter. This paper summarizes past work on such fuzzy geometric concepts, and also includes some new results.',180147,'Pattern Recognition Letters',2),(1845754,NULL,'2001',NULL,'A simple efficient approximation scheme for the restricted shortest path problem','In this short paper we give a very simple fully polynomial approximation scheme for the restricted shortest path problem. The complexity of this @e-approximation scheme is O(|E|n(loglogn+1/@e)), which improves Hassin\'s original result (Math. Oper. Res. 17 (1) (1992) 36) by a factor of n. Furthermore, this complexity bound is valid for any graph, regardless of the cost values. This generalizes Hassin\'s results which apply only to acyclic graphs. Our algorithm is based on Hassin\'s original result with two improvements. First we modify Hassin\'s result and achieve time complexity of O(|E|n(loglog(UB/LB)+1/@e)), where UB and LB are upper and lower bounds for the problem. This modified version can be applied to general graphs with any cost values. Then we combine it with our second contribution, which shows how to find an upper and a lower bound such that UB/LB=',174576,'Operations Research Letters',9),(1862520,NULL,'1995',NULL,'Decision-based neural networks with signal/image classification applications','Supervised learning networks based on a decision-based formulation are explored. More specifically, a decision-based neural network (DBNN) is proposed, which combines the perceptron-like learning rule and hierarchical nonlinear network structure. The decision-based mutual training can be applied to both static and temporal pattern recognition problems. For static pattern recognition, two hierarchical structures are proposed: hidden-node and subcluster structures. The relationships between DBNN\'s and other models (linear perceptron, piecewise-linear perceptron, LVQ, and PNN) are discussed. As to temporal DBNN\'s, model-based discriminant functions may be chosen to compensate possible temporal variations, such as waveform warping and alignments. Typical examples include DTW distance, prediction error, or likelihood functions. For classification applications, DBNN\'s are very effective in computation time and performance. This is confirmed by simulations conducted for several applications, including texture classification, OCR, and ECG analysis',117404,'IEEE Transactions on Neural Networks',2),(1862987,NULL,'1997',NULL,'Performance modeling for mobile telephone networks','In a mobile telephone network, users may move around the service area during conversations, which can significantly affect the efficiency of radio resource (i.e., radio channels) allocation in the network. The author describes a simple analytic model to study the effect of user mobility on the performance of a mobile telephone network. Throughout the derivation of the model, the intuition behind the equations is provided to explain how user behavior affects network performance. This model can be used to study different handoff schemes with single and mixed types of users with different mobility patterns',117261,'IEEE Network: The Magazine of Global Internetworking',3),(1863073,NULL,'1998',NULL,'Traffic shaping, bandwidth allocation, and quality assessment for MPEG video distribution over broadband networks','This article provides an overview of residential video delivery systems and presents the applications, benefits, and challenges of using VBR MPEG video encoding in broadband video distribution networks. The network resources required to transmit stored variable-rate MPEG can be reduced by properly analyzing and smoothing the video stream before transmission. A scheduling technique is presented which selects a traffic contract for a pre-encoded MPEG video stream with the criteria of minimizing network resources and maintaining video quality. Several effective bandwidth metrics are discussed and used to model the potential savings in network resources for the shaped streams',117261,'IEEE Network: The Magazine of Global Internetworking',3);
/*!40000 ALTER TABLE `fg_m_article` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2018-04-06 20:14:20
